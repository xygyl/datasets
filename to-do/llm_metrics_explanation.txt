Precision, Recall, and F1 Score in LLM Performance Evaluation

1. Precision
Definition: Of all the instances the model labeled as positive (e.g., “vulnerable”), what fraction were actually positive?
Formula:
  Precision = True Positives / (True Positives + False Positives)
Interpretation: High precision means few false alarms—when the model flags a case, it is usually correct.

2. Recall
Definition: Of all the actual positive instances in the dataset, what fraction did the model correctly identify?
Formula:
  Recall = True Positives / (True Positives + False Negatives)
Interpretation: High recall means few misses—the model finds most of the real positives.

3. F1 Score
Definition: The harmonic mean of precision and recall, balancing both metrics.
Formula:
  F1 = 2 × (Precision × Recall) / (Precision + Recall)
Interpretation: A high F1 score indicates the model is both accurate when it predicts positive and comprehensive in finding positives.

Example Scenario:
- Total code snippets: 100
- True positives (real vulnerabilities): 20
- Model flags as positive: 30 snippets
  - True Positives = 15
  - False Positives = 15
  - False Negatives = 5

Calculations:
- Precision = 15 / (15 + 15) = 0.50
- Recall = 15 / (15 + 5) = 0.75
- F1 Score = 2 × (0.50 × 0.75) / (0.50 + 0.75) ≈ 0.60

This means the model catches 75% of real vulnerabilities but only half of its alerts are correct, yielding an overall F1 of 0.60.
