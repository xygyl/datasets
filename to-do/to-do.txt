OVERALL
	what prompts they used
	methodology
	why did one methodology give a better result than another

checker-bug-detection-and-repair-in-deep-learning-libraries
	why did few-shot give a lower F1 score?
comparison-of-static-application-security-testing-tools-and-large-language-models-for-repo-level-vulnerability-detection
	download datasets detailed in 2.2
detecting-code-smells-using-chatgpt_initial-insights
	why does adding a few extra key words make the prompt go from generic to detailed?	
effective-vulnerable-function-identification-based-on-cve-description-empowered-by-large-language-models
	what makes this paper unique? what is their approach to prompting the llm?
	look in section 2
	notate txt better
# enhancing-static-analysis-for-practical-bug-detection_an-llm-integrated-approach
	SKIP
exploring-chatgpts-capabilities-on-vulnerability-management
	get dataset
	get specific prompts
	what kind of prompts are they using?
	which prompt is the best? get conclusions
	testing looks boring; only two models (3.5, 4). why is that?
exploring-the-influence-of-prompts-in-llms-for-security-related-tasks
	understand the full prompt methodology (tt and ft)
	what do the tables mean? (what is going on in table 1)
gptscan_detecting-logic-vulnerabilities-in-smart-contracts-by-combining-gpt-with-program-analysis
	
interleaving-static-analysis-and-llm-prompting
llmdfa_analyzing-dataflow-in-code-with-large-language-models
llms-cannot-reliably-identify-and-reason-about-security-vulnerabilities_a-comprehensive-evaluation-framework-and-benchmarks
multi-role-consensus-through-llms-discussions-for-vulnerability-detection
prompt-enhanced-software-vulnerability-detection-using-chatgpt
sanitizing-large-language-models-in-bug-detection-with-data-flow
skipanalyzer_a-tool-for-static-code-analysis-with-large-language-models
to-err-is-machine_vulnerability-detection-challenges-llm-reasoning
