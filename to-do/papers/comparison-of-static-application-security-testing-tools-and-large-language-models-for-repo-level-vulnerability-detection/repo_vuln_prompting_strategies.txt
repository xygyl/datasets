Prompting Strategies for Repo-level Vulnerability Detection

Location: Section 2.5.1 – Prompt-based Methods 

1. Prompt Template Components:
   a. Task Description: A natural language instruction for the vulnerability detection task:
      “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No” 
   b. Formatted Input: The target code snippet is wrapped between markers:
      // Code Start
      <code>
      // Code End 
   c. Prediction Marker: A token to signal the model where to produce its answer:
      // Detection 

2. Zero-shot Prompting:
   - Use the base prompt template with no examples.
   - Model directly classifies the input as “Yes” or “No” without any labeled instances.

3. Chain-of-Thought (CoT) Prompting :
   - Same as zero-shot, but append “Let’s think step by step” immediately after the task description.
   - Encourages the model to produce intermediate reasoning before the final decision.

4. Few-shot Prompting:
   - Provide two example pairs of code snippets and their correct Yes/No labels:
     • One vulnerable function example
     • One clean function example
   - Examples are concatenated to the prompt before the target snippet to guide the model’s format and decision-making.


Results of Prompting Strategies:

Zero-shot Prompting:
- Achieved high vulnerability detection ratios (Scenario 1 up to ~87.5%, Scenario 2 up to ~50.0% for Java; up to ~80.0%/70.0% for C; up to ~80.0%/60.0% for Python).
- Marked function ratios were high (15%–70% across languages), indicating many false positives.
- Conclusion: Strong detection ability but low precision in locating true vulnerabilities.

Chain-of-Thought Prompting:
- Delivered modest boosts in some cases (e.g., CodeLlama improved Python Scenario 2 detection to ~64.3%), with detection ranging up to ~90%/70% in best models.
- Marked ratios remained elevated (20%–75%), showing limited reduction in false positives.
- Conclusion: Better reasoning trace, marginal gains over zero‑shot, but precision trade‑offs persisted.

Few-shot Prompting:
- Provided the best overall balance: examples guided format, yielding top aggregated performance (e.g., StarCoder2 reached an effective combined score equivalent to ~73% F1 on Python).
- Detection ratios reached ~70%/60% in best cases across languages, though marked ratios stayed high (35%–77%).
- Conclusion: Few-shot gave the strongest end-to-end results, but high marked ratios indicate further work needed to reduce false positives.
