{
  "filename": "comparison-of-static-application-security-testing-tools-and-large-language-models-for-repo-level-vulnerability-detection.pdf",
  "analysis": "Certainly! Here’s a Q&A-style summary addressing your questions with explicit references to the content in the provided research text:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**  \nThe paper aims to **comparatively evaluate the effectiveness of Static Application Security Testing (SAST) tools and Large Language Models (LLMs)** for *repo-level* vulnerability detection—scanning entire code repositories to find security vulnerabilities. It investigates which approaches are most effective, their pros and cons, and how they might be combined for better results.\n\n**Uniqueness:**  \n- It is the **first** work to compare SAST tools and LLMs for *repo-level* vulnerability detection across three major programming languages: Java, C, and Python.\n- It introduces the *repo-level* vulnerability detection task for LLMs (whereas most prior work focused on function-level).\n- Constructs a **new repo-level Python vulnerability dataset** and curates new Java and C sets suitable for this purpose.\n- Evaluates 15 SAST tools and 12 LLMs using multiple adaptation strategies.\n- Proposes and analyzes ensemble techniques combining multiple SAST tools and LLMs.\n\n*See: Abstract, Introduction, Contributions (bulleted list).*\n\n---\n\n### 2. What dataset did they use?\n\n**Datasets:**  \nThe paper used three datasets, each representing a popular language:\n\n- **Java:** Dataset from Li et al. [49], containing real-world vulnerabilities labeled at function-level. The actual repository sources were curated by the authors from GitHub.\n  - 50 projects, 87 CVEs, 3,986,848 functions, 0.008% vulnerable\n- **C:** Dataset from Lipp et al. [51], also with function-level labels and actual vulnerable repositories collected from GitHub.\n  - 6 projects, 85 CVEs, 1,181,556 functions, 0.009% vulnerable\n- **Python:** **Newly constructed by the authors**\n  - 78 projects, 98 CVEs, 312,045 functions, 0.18% vulnerable\n\nEach dataset includes: CVE ID, CWE type, complete source code, and function-level vulnerability locations.\n\n*See: Section 2.2 Vulnerability Datasets, Table 1.*\n\n---\n\n### 3. What LLM models were used?\n\n**Models Used:**  \nTwelve open-source LLMs, covering two groups: lightweight (<1B parameters) and large (≥1B).\n\n- **Lightweight:**\n  - CodeBERT (0.13B)\n  - GraphCodeBERT (0.13B)\n  - CodeT5 (0.22B)\n  - UniXcoder (0.13B)\n\n- **Large:**\n  - CodeLlama (7B)\n  - StarCoder (7B)\n  - StarCoder2 (7B)\n  - DeepSeek-Coder (6.7B)\n  - CodeQwen (7B)\n  - Mistral (7B)\n  - Llama3 (8B)\n  - Phi3 (3.8B)\n\n*Also briefly compare with ChatGPT (gpt-3.5-turbo-0125) for reference.*\n\n*See: Section 2.4 Studied LLMs, Table 2.*\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n**Prompting Strategies:**  \nFour main adaptation techniques for LLMs:\n\n1. **Zero-shot Prompting:**  \n   Standard prompt with a task description; no labeled examples provided.\n\n2. **Chain-of-thought (CoT) Prompting:**  \n   Adds “Let’s think step by step” to encourage multi-step reasoning in the output.\n\n3. **Few-shot Prompting:**  \n   Includes a couple of labeled input-output examples after the prompt to guide the model.\n\n4. **Fine-tuning:**\n   - *Full Fine-tuning* for light models (<1B): All model parameters are updated on a balanced training set.\n   - *Parameter-Efficient Fine-Tuning (PEFT)* (e.g., LoRA) for larger models: Only a subset of model parameters is updated, reducing computation.\n\n*See: Section 2.5 LLM Adaptation Techniques, Section 2.5.1 Prompt-based Methods.*\n\n---\n\n### 5. Where are the prompt strategies explained?\n\n- **Prompt-based adaptation strategies are explained in Section 2.5.1.**\n- The actual *prompt format* includes:  \n  1. Task Description (“If the following code snippet has any vulnerabilities, output Yes; otherwise, output No”),  \n  2. Formatted Input (code between markers),  \n  3. Prediction Marker (“//Detection”).\n\nDetails for each adaptation are also in Table 2.\n\n---\n\n### 6. What are the results of the prompting strategies, and how successful or not were they?\n\n**Summary of Results:**\n\n- **Fine-tuning (especially for large LLMs)** - achieves the best tradeoff:\n  - Highest vulnerability detection rates (up to 100%)\n  - Marks fewer functions as vulnerable (i.e., lower false positives) than prompted-only runs\n- **Prompting-only methods (zero-shot, CoT, few-shot):**\n  - Generally achieve *lower* detection rates and *higher* false positive rates compared to fine-tuned LLMs.\n\n**Detailed Observations:**\n\n- *CodeBERT, CodeT5, UniXcoder, etc., when fine-tuned, reached 88–100% detection, but with high marked ratios (over 20–40%).*\n- *Zero-shot, CoT, and few-shot prompting for large LLMs like Llama3, StarCoder, etc., got 0–80% detection but marked >30% of code as vulnerable.*\n- Ensembling several LLMs (requiring majority agreement) reduces the marked (false positive) rate **by 40–75%**, but may drop detection slightly.\n- SAST tools had low detection rates (<44%) but extremely low false positive rates (<5%).\n\n**Takeaway:**  \n**Fine-tuned LLMs are the most successful among LLM approaches, with prompting alone much less effective.**\n\n*See: Table 3, Sections 4.1, 4.2, and 4.3.*\n\n---\n\n### 7. Why did one strategy give better results than another?\n\n**Reasoning:**\n- **Fine-tuning prevails** because it *explicitly adapts* an LLM’s weights with labeled vulnerability data, learning to distinguish subtle cues in code that correspond to real vulnerabilities.\n- Prompting-only methods rely on the LLM’s general knowledge and “in-context” training, without specific adaptation — this is insufficient for this detection task, especially due to data and label imbalance (very few vulnerable functions vs. many clean ones).\n- “Chain-of-thought” and “few-shot” prompting help, but not nearly as much as actually training the model.\n\n*See: Section 4.1.3 Comparisons Among LLMs; Section 2.5.1/2.5.2 for rationale.*\n\n---\n\n### 8. All specifics of the researchers' prompt strategies.\n\n**Prompt Structure (for all LLM variants):**\n- **Task description:** “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”\n- **Code delimited by:** “//CodeStart” and “//CodeEnd” markers.\n- **Output marker:** “//Detection” to explicitly mark where output should begin.\n\n**Adaptation/Prompting Methods:**\n- **Zero-shot:** Just the prompt above, provided for each function; no example output.\n- **Chain-of-thought:** Adds “Let’s think step by step” to the task description, encouraging the model to generate intermediate reasoning before prediction.\n- **Few-shot:** Adds two example code-label pairs to the prompt (one vulnerable, one clean) before the target function.\n- **Fine-tuning (for light models):** Actual model training with equal numbers of vulnerable and clean functions—full backpropagation.\n- **PEFT (for large models):** LoRA (Low-Rank Adaptation); only updates part of the model for efficiency; uses the same balanced data strategy.\n\n*Strategy for repo-level task:*  \n- Each repository is split into functions.\n- Each function is independently predicted for vulnerability.\n- Aggregated per-repo for calculating metrics.\n\n*See: Section 2.1.3 (Prompt Strategy for Repository-Level), Section 2.5.1/2.5.2 (Prompt details), Experimental Setup.*\n\n---\n\n**If you need this as a table or a more compressed summary, let me know!**"
}