[
  {
    "filename": "large-language-models-are-zero-shot-reasoners.pdf",
    "analysis": "Sure! Here is a detailed breakdown to answer your questions based on the provided text from \"Large Language Models are Zero-Shot Reasoners\" by Kojima et al.\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Point/Purpose:**  \n- The paper investigates whether Large Language Models (LLMs) can perform complex reasoning tasks in a *zero-shot* manner—not given any task-specific examples—by simply adjusting the prompt, rather than using the standard few-shot or fine-tuned approaches.\n  \n**What it explores:**  \n- The core focus is to test if a simple, task-agnostic prompt such as “Let’s think step by step” can elicit stepwise, “chain-of-thought” reasoning from LLMs on a variety of difficult reasoning benchmarks (arithmetic, symbolic, commonsense, logic), closing much of the gap between zero-shot and more elaborate few-shot prompt-based reasoning, with NO examples.\n\n**Uniqueness:**  \n- Prior work (e.g., Chain-of-Thought prompting by Wei et al., 2022) showed major improvements in reasoning when LLMs were given few-shot examples *with* stepwise reasoning.\n- This paper’s unique claim: **A single, fixed, simple prompt (“Let’s think step by step”) enables strong zero-shot reasoning performance, across diverse tasks, with no task-specific engineering or examples.**  \n- It systematically benchmarks zero-shot-CoT (Zero-shot Chain-of-Thought) against standard zero-shot, few-shot, and few-shot-CoT on many LLMs and datasets.\n\n---\n\n**2. What dataset did they use?**\n\nThey use 12 datasets spanning four categories:\n- **Arithmetic Reasoning:**  \n  - SingleEq  \n  - AddSub  \n  - MultiArith  \n  - AQUA-RAT  \n  - GSM8K  \n  - SVAMP  \n- **Commonsense Reasoning:**  \n  - CommonsenseQA  \n  - StrategyQA  \n- **Symbolic Reasoning:**  \n  - LastLetterConcatenation (self-created)  \n  - CoinFlip (self-created)  \n- **Other Logical Reasoning Tasks:**  \n  - DateUnderstanding (from BIG-bench)  \n  - Tracking Shuffled Objects (from BIG-bench)  \n\nDetails and sources of their versions are in Appendix A.2/Tables 7.\n\n---\n\n**3. What LLM models were used?**\n\nA wide variety:\n- **OpenAI GPT-3 (\"Original\") API**: sizes 0.3B (ada), 1.3B (babbage), 6.7B (curie), 175B (davinci)\n- **OpenAI InstructGPT**: text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002 (size ordering same as GPT-3)\n- **PaLM**: (Google) 8B, 62B, 540B\n- Other LLMs for scaling studies:  \n  - GPT-2 (1.5B)  \n  - GPT-Neo (2.7B)  \n  - GPT-J (6B)  \n  - T0 (11B)  \n  - OPT (13B)\n  \nSee Appendix A.3/Table 8 for full details.\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\nMain prompt-based reasoning strategies analyzed:\n- **Standard Zero-Shot Prompting:**  \n  Just appending \"The answer is\" or similar to the question, expecting a direct answer.\n- **Few-shot Prompting:**  \n  Providing a few solved examples in the prompt to condition the model.\n- **Few-shot Chain-of-Thought (Few-shot-CoT):**  \n  Like above, but including step-by-step reasoning *and* the answer in each example.\n- **Zero-shot Chain-of-Thought (Zero-shot-CoT -- Theirs):**  \n  The *main innovation*:  \n  - NO examples at all — only a special trigger phrase like \"Let's think step by step.\" before the answer.\n  - (Optionally) A second prompt to extract the answer from the generated reasoning (two-stage prompting).\n\nAlso, they explore variants like:\n- Using different trigger templates (see Table 4), e.g. \"First,\", \"Let’s solve this problem by splitting into steps\", etc.\n\n---\n\n**5. Where are the prompt strategies explained?**\n\n- Main explanation:  \n  **Section 3** (\"Zero-shot Chain of Thought\") -- especially 3.1 on two-stage prompting.  \n  - Figure 2 illustrates the two-step process.\n\n- Examples:  \n  **Figure 1** (with subfigures a–d) gives concrete side-by-side examples of standard/few-shot vs. CoT vs. Zero-shot-CoT.\n\n- Prompts and answer extraction triggers:  \n  **Appendix A.5** (Tables 9 and 10) summarizes the various answer extraction prompts used.\n\n- Alternative trigger sentences and template variants:  \n  **Table 4** and Appendix B/Table 13 show the effect of different prompt phrasings.\n\n---\n\n**6. What are the results of the prompting strategies, and how successful were they?**\n\n**Summary:**\n- **Zero-shot-CoT** (their method) strongly improves over standard Zero-shot (no reasoning prompt) on tasks requiring reasoning:\n  - *MultiArith*: accuracy jumps from 17.7% → 78.7%\n  - *GSM8K*: 10.4% → 40.7%\n- Comparable (sometimes better) gains reported for PaLM 540B:  \n   e.g. GSM8K: Zero-shot 12.5%, Zero-shot-CoT 43.0%, Zero-shot-CoT+self-consistency 70.1%\n- On \"system-1\" style direct tasks (SingleEq, AddSub), Zero-shot-CoT is \"on par\" or slightly better than zero-shot because they don’t need multistep reasoning.\n- For commonsense tasks (CommonsenseQA), Zero-shot-CoT does *not* necessarily improve the answer accuracy much, but the stepwise reasoning it generates is often logically coherent and reveals improved intermediate understanding, even if not always giving the right answer.\n- **Few-shot-CoT** is still the strongest, but needs hand-crafting and is sensitive to example format and match.\n- Table 2 gives detailed comparisons.\n\n---\n\n**7. Why did one strategy give better results than another?**\n\n- **Zero-shot-CoT beats Standard Zero-shot** because the \"Let’s think step by step\" (and similar) prompt encourages the model to generate an explicit reasoning process, which helps it avoid mistakes caused by skipping steps or overlooking constraints—something LLMs seem to be capable of but need to be nudged into doing.\n- **Standard Zero-shot** prompt encourages a guess or shortcut (surface heuristics instead of genuine logical steps), so accuracy is poor on multi-step tasks.\n- **Few-shot approaches** provide even more structured guidance—but are less flexible, require task-specific hand-work, are less robust to mismatches in input type, and may not generalize.\n- **Effectiveness scales with model size**: small models do not benefit much, but >100B parameter models (like text-davinci-002, PaLM 540B) show major Zero-shot-CoT gains. This matches intuition: models need to be sufficiently capable to \"know what it means to think step by step.\"\n- **Prompt content is key**: If the prompt is misleading, irrelevant, or gives the wrong reasoning expectation, performance drops to baseline (Table 4—see accuracy for \"It's a beautiful day\" vs \"Let’s think step by step.\").\n\n---\n\n**8. All specifics of the researchers' prompt strategies.**\n\n**Prompt Structure:**\n- **Reasoning Extraction Prompt:**  \n  - Template: \"Q: [QUESTION] A: Let's think step by step.\"\n- **Answer Extraction Prompt:**  \n  - After generating step-by-step reasoning, prompt the LLM again to extract the final answer with:\n    - Math: \"Therefore, the answer (arabic numerals) is\"\n    - Multiple-choice: \"Therefore, among A through E, the answer is\"\n    - Yes/No: \"Therefore, the answer (Yes or No) is\"\n    - (See Tables 9 & 10 for mappings per dataset/task)\n- **Answer Cleansing:**  \n  - Rules to pick the first number, first capital letter, or \"yes/no\" (see Appendix A.6).\n\n**Template Variants:**  \n- In Table 4, they test 16 trigger templates, including:\n  - instructive: \"Let's think step by step.\", \"First,\", \"Let's solve this problem by splitting it into steps.\", etc.\n  - misleading/irrelevant: \"Don't think. Just feel.\", \"By the way, I found a good restaurant nearby.\", etc.\n  - Results indicate only instructive, reasoning-oriented triggers boost performance.\n\n**Two-Stage (Pipeline) Prompting:**  \n- **First stage:** Generate reasoning chain.\n- **Second stage:** Extract answer from the reasoning text.\n- Both stages use greedy decoding (temperature 0).\n\n**Experiments Control:**  \n- All methods/variants use the same decoding, answer extraction, and evaluation methodology for comparability.\n- Tested on both \"instruction-tuned\" LLMs (like text-davinci-002) and base models.\n\n**Self-Consistency:**  \n- For some experiments (esp. with PaLM), they also test \"self-consistency\" (sampling multiple CoTs and majority voting).\n\n---\n\n**Extra: Key Figures and Tables**\n- **Figure 1:** Examples of all four prompt types with outputs from actual LLMs.\n- **Table 1:** Head-to-head accuracy comparison for Zero-shot and Zero-shot-CoT across datasets.\n- **Table 2:** Full comparison with Few-shot(-CoT), fine-tuned baselines, etc.\n- **Table 4:** Shows effect of alternate prompt phrasings.\n- **Appendix (A.5, A.6):** Precise answer extraction prompts and parsing (answer cleansing) rules.\n\n---\n\n**In summary:**  \n- This paper's main claim is that a *simple, universal, trigger prompt*—for example, \"Let’s think step by step.\"—enables LLMs to perform *much* better on challenging, multi-step reasoning tasks, in *zero-shot settings*, compared to naïve zero-shot approaches.\n- Their results establish this technique as a new, strong, minimal baseline, highlighting that LLMs’ reasoning capability is more *latent* and “untapped” than previously realized, provided you prompt for it explicitly."
  },
  {
    "filename": "interleaving-static-analysis-and-llm-prompting.pdf",
    "analysis": "Certainly! Here is a detailed answer to each of your questions about the paper \"Interleaving Static Analysis and LLM Prompting\":\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Point:** The paper introduces a new approach that combines static program analysis with Large Language Model (LLM) prompting, interleaving calls between a static analyzer (EESI) and an LLM during the process of inferring *error specifications* in C system code. Error specifications define what values functions return when errors occur, which is important for program understanding and bug detection.\n\n**Exploration:** It explores how program analysis can be improved by using LLMs to provide information when static analysis alone is insufficient, especially for scenarios involving third-party functions or incomplete program facts.\n\n**Uniqueness:** The main novelty is the *interleaving*—static analysis and LLM querying aren't simply run separately then combined; rather, intermediate results from analysis are used to construct LLM prompts and vice versa, creating a feedback loop. The LLM is only called when the static analyzer would otherwise fail or be imprecise. Unlike prior work, LLM results are fed back into the analyzer, improving its further analysis.\n\n---\n\n**2. What dataset did they use?**\n\nThey used six real-world open source C programs that cover a range of error-handling idioms:\n- **Apache HTTPD**\n- **LittleFS**\n- **MbedTLS**\n- **Netdata**\n- **Pidgin OTRv4**\n- **zlib**\n\nDetails like lines of code and domain knowledge provided are shown in Tables 1 and 2 of the paper. Initial domain knowledge (such as initial error specifications for certain functions, error codes, success codes, and error-only functions) is also supplied per benchmark.\n\n---\n\n**3. What LLM models were used?**\n\n- The experiments used **OpenAI's GPT-4** as the LLM queried in their workflow ([Section 5, Implementation Details] in the paper).\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\nThere are two main prompting strategies:\n  - **queryLLMThirdParty**: Used when dealing with *third-party functions* (whose code is not accessible to static analysis). The prompt includes all currently learned error specifications in the program as context, and asks the LLM about the specific third-party function.\n  - **queryLLMAnalysis**: Used when static analysis for a function is *inconclusive* (result is ⊥, unknown). The prompt includes the current error specifications of *called functions* (within the analyzed function), plus other context, and asks the LLM to infer the error specification for this particular function.\n\nThe prompts themselves are carefully constructed and include:\n  - **CommonContext**: Explanation of the error specification inference problem, the abstract domain used by EESI, idioms/rules about error codes, and chain-of-thought examples to guide the LLM.\n  - **FunctionContext**: Relevant error specifications from the call graph or program facts (either all known error specs for third-party, or only those called inside the function for regular analysis).\n  - **Question**: Either the function name (third-party) or the function's source code (analysis), and a request for the error specification.\n\nAdditionally, chain-of-thought prompting, self-consistency (multiple runs), and example formatting are employed to stabilize LLM output.\n\n---\n\n**5. Where are the prompt strategies explained?**\n\nPrompt construction is detailed in **Section 4.1 (Building Prompts)** of the paper. The distinction between `queryLLMThirdParty` and `queryLLMAnalysis`, and their use in the proposed algorithm, is explained in **Section 4.2 (Error Specification Inference)** and in the pseudocode (Algorithm 1).\n\n---\n\n**6. What are the results of the prompting strategies and how successful or not were they?**\n\nSee Table 4 and Figure 4.\n\n- **queryLLMThirdParty** increased recall by about 29% on average over baseline EESI, especially benefiting benchmarks heavily reliant on third-party code (e.g., Netdata, Pidgin OTRv4), with similar precision to EESI.\n- **queryLLMAnalysis** gave a much higher recall increase (average +59.88%) and recall (70.26%) over EESI, with a modest drop or maintenance in precision.\n- **Combined/Interleaving approach** (using both strategies) achieved the best results:\n    - **Recall**: rose from EESI's 52.55% to 77.83% (average)\n    - **F1-score**: improved from 0.612 to 0.804\n    - **Precision**: remained high (slight decrease from 86.67% to 85.12%)\n\nSuccess varied across benchmarks, most notably improving error specification inference in projects where static analysis had limited knowledge (e.g., due to third-party code).\n\n---\n\n**7. Why did one strategy give better results than another?**\n\n- **queryLLMThirdParty** helps mostly when missing knowledge is due to third-party or library functions, since only the names and previously inferred error specs can be used. It cannot reason about internal function logic, so the gain is limited to inferring specs for \"black box\" APIs.\n- **queryLLMAnalysis** is more impactful because it enables the LLM to directly analyze the source code of functions where static analysis failed, and it supplies context/specs for any called functions. The LLM can thus reason over code and context, supplying previously missing insights that static analysis struggled with.\n- **Combined interleaving** works best—using the right method at the right time and feeding LLM results back into static analysis, so earlier discoveries enhance later ones.\n\n---\n\n**8. All specifics of the researcher's prompt strategies.**\n\n**Prompt Structure for Each Query (see Section 4.1):**\n- **CommonContext** (for all prompts):\n    - General explanation of error specification inference\n    - Description of the \"abstract domain\" (the mathematical model EESI uses)\n    - List of current error codes, success codes, error-only functions (domain knowledge)\n    - List of idioms/rules about how errors/specs generally work (from domain knowledge)\n    - Multiple chain-of-thought examples: function definitions, their error specifications, and an explanation for clarity. This both demonstrates the expected answer format and provides LLM with examples to reason by analogy.\n\n- **FunctionContext**:\n    - For **queryLLMThirdParty**: include all known error specifications from the program facts, as context, to help the LLM infer the likely error spec for a given third-party function name.\n    - For **queryLLMAnalysis**: include only the error specs of functions called within the function under analysis, to help with local reasoning.\n\n- **Question**:\n    - For **queryLLMThirdParty**: \"What is the error specification for function XYZ?\"\n    - For **queryLLMAnalysis**: Provide full function source code, and \"What is the error specification for this function?\"\n\n**Additional details and stabilization strategies:**\n- LLM is called *only if* static analysis is inconclusive (⊥) or the function is third-party.\n- Every LLM response is parsed to check format; if the output doesn't fit expectations, it is discarded (marked as ⊥/unknown).\n- To reduce hallucinations:\n    - They query the LLM multiple times and use self-consistency to select the most common result.\n    - Ensure that LLM’s output matches the chain-of-thought reasoning it provides.\n    - Don't accept error specs that (a) include known success values or (b) are improper supersets of the function’s return range, to avoid semantic contradictions.\n- Outputs from LLM are fed back as new program facts for static analysis to use in subsequent inference; this creates an iterative improvement.\n\n**Algorithmic Workflow:**\n- Bottom-up over the program's call graph.\n- For each function:\n    - If third-party: queryLLMThirdParty\n    - Else: Run EESI analysis; if inconclusive, queryLLMAnalysis\n- Update learned error specs after each function.\n\nThese strategies allow the approach to leverage the best of both worlds: static analysis's confidence in some scenarios, and LLM's flexible reasoning in the presence of incomplete information.\n\n---\n\n**Summary Table**\n\n|                    | EESI Only         | LLM ThirdParty      | LLM Analysis           | Combined Interleaved Approach   |\n|--------------------|-------------------|---------------------|------------------------|---------------------------------|\n| Precision (avg)    | 86.7%             | ~85%                | ~84.7%                 | 85.1%                           |\n| Recall (avg)       | 52.5%             | 62.2%               | 70.3%                  | 77.8%                           |\n| F1-score (avg)     | 0.612             | 0.68                | 0.74                   | 0.804                           |\n\nThe combined, interleaved approach is most successful, balancing high recall and high precision, and thus the best F1, for error specification inference in C code.\n\n---\n\nIf you want expanded details on any subpoint, just ask!"
  },
  {
    "filename": "exploring-the-influence-of-prompts-in-llms-for-security-related-tasks.pdf",
    "analysis": "Certainly! Here is an organized and detailed breakdown addressing each of your questions about the paper **\"Exploring the Influence of Prompts in LLMs for Security-Related Tasks\"**:\n\n---\n\n## 1. What is the point of the paper? What is it trying to explore? What makes it unique?\n\n**Point and exploration:**  \nThe paper systematically investigates how the construction of prompts—specifically their structure and content—affects the performance of large language models (LLMs) on security-related tasks. While prompt engineering for general tasks has been widely studied, its impact on specialized domains like security (where tasks involve code, natural language, and security-specific terms) is underexplored. The study aims to:\n\n- Identify which types of prompt structures (e.g. zero-shot, one-shot, few-shot) are effective for security tasks.\n- Determine which kinds of prompt content (e.g. role definition, emotional cues, GPT-generated content) enhance LLM performance in these tasks.\n- Explore if combining multiple prompt engineering strategies further improves results.\n\n**Uniqueness:**  \n- First systematic, quantitative comparison of diverse prompt structures and contents for LLMs in security-related contexts.\n- Focuses on three practical, real-world security tasks.\n- Highlights that best-performing prompt strategies vary by task—there’s no \"one-size-fits-all.\"\n- Provides reproducible results and code/datasets.\n\n---\n\n## 2. What dataset did they use?\n\nThey consider three benchmark datasets, each for a specific security-related task:\n\n- **Security Patch Detection (SPD):** [PatchDB](https://github.com/Wayne-Bai/Prompt-Affection) – 12,000 labeled security patches & 24,000 non-security patches from sources like the NVD and GitHub.\n- **Vulnerable Function Detection (VFD):** [Devign](https://github.com/microsoft/Devign) – 48,687 C/C++ functions labeled as vulnerable or not, with data from projects like Linux, QEMU, Wireshark, and FFmpeg.\n- **Stable Patch Classification (SPC):** [PatchNet](https://github.com/hvdang/patchnet), with 82,403 Linux kernel patches labeled as stable or not.\n- **Sampling for experiments:** 2,000 samples per task (1,000 positive and 1,000 negative).\n\n---\n\n## 3. What LLM models were used?\n\n- **GPT-3.5-Turbo** (via the OpenAI API) was used exclusively for all experiments. Chosen for availability, capability, stable API throughput, and cost-effectiveness.\n- Note: No experiments with GPT-4 or open LLMs due to budget, but methodology and code are open-sourced for future research.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\n### **Prompt Structure (Framework)**\n- **Zero-shot:** Only a task description; no examples.\n- **One-shot:** Task description + one example (either positive or negative class).\n- **Few-shot:** Task description + two examples—can be two positives, two negatives, or one of each (in either order).\n  - *Variants named*: 1-shot-t, 1-shot-f, few-shot-tt, few-shot-ff, few-shot-tf, few-shot-ft.\n\n### **Prompt Content**\nNine types (can be applied to any structure):\n- **Basic:** Plain task description and query.\n- **Role:** Sets a specific role (“You are an AI expert in patch analysis…”).\n- **Act As-User / Act As-System:** Specifies the persona in either the user's or the system’s message (e.g., “Act as security expert…”).\n- **GPT-generated:** Prompt written by GPT-3.5 itself given task description.\n- **Emotion:** Motivational or psychological cues, including:\n    - **Encourage**: Positive reinforcement (“you’re the best...”)\n    - **Threaten**: Negative consequences for failure (“…otherwise you are the worst…”)\n    - **Reward**: Promise of positive reward for good output (“If you do well, you get a tip”)\n    - **Punish**: Threat of punishment for failure (“If badly, you will be fined”).\n\n- **Combined:** Some experiments combine structure and content (e.g., role + GPT-generated).\n\n---\n\n## 5. Where are the prompt strategies explained?\n\n- **Prompt structures:** Section II-B (“Prompt Structure”).\n- **Prompt contents:** Section II-C (“Prompt Content”).\n- **Table I** in Section II-C shows concrete prompt content examples for the SPD task.\n- **Tables II, III, IV** display results for combinations of content and structure for each task.\n- **Section II-D** explains the experimental methodology and performance metrics.\n\n---\n\n## 6. What are the results of the prompting strategies, and how successful or not were they?\n\n**General findings:**\n\n- **Prompt structure and content both substantially affect LLM performance.** The difference between worst- and best-performing strategies for a task can be 13–30% in accuracy.\n- **Task dependence:** The most effective prompt structure/content varies by task.\n\n**Detailed successes:**\n\n- **Security Patch Detection (SPD):**  \n  - **Best structure:** few-shot-ff (two negative examples) yields highest accuracy, recall, and F1.\n  - **Best content:** \"Punish\" content variant yields the best overall performance in most structures; \"GPT-generated\" gives best precision.\n- **Vulnerable Function Detection (VFD):**\n  - Structures make less impact; sometimes zero-shot or one-shot-t is best depending on the metric.\n  - Precision is best with 1-shot-t; recall with 0-shot. Adding more examples may sometimes confuse the LLM.\n- **Stable Patch Classification (SPC):**\n  - Structures: few-shot-ff helps for recall and F1; 1-shot-f helps for precision.\n  - Content: Changing content generally has little effect for this task except for GPT-generated, which often makes things worse.\n\n- **Combining contents:**  \n  - Rarely helps further; only about 10% of cases show improvement when combining, and often combinations perform worse.\n\n**See Tables II-IV for all numeric results.**\n\n---\n\n## 7. Why did one strategy give better results than another?\n\n- **Data bias:** For some tasks (like SPC), the LLM is biased towards the majority (non-security) class, so seeing more negative examples (few-shot-ff) helps it recognize positives.\n- **Task nature:** For VFD, vulnerabilities may be highly diverse, so few demonstrations can’t capture generalizable patterns; here, zero-shot works best.\n- **Prompt content:** Psychological or role-based content can act as \"priming\" for the LLM, nudging it to take tasks more seriously or mimic expert behavior, but the effect depends strongly on task relevance.\n- **Prompt order and class:** The sequence of examples (positive/negative) and class shown can orient the model to pay attention to certain features or biases.\n- **GPT-generated prompts:** Sometimes too verbose or generic, and may introduce unnecessary details that reduce accuracy.\n\n---\n\n## 8. All specifics of the researcher's prompt strategies.\n\n### **Prompt structures:**\n- **Zero-shot:** Just the task description.\n- **1-shot-t:** Task description + one positive-class example.\n- **1-shot-f:** Task description + one negative-class example.\n- **few-shot-tt:** Two positive examples.\n- **few-shot-ff:** Two negative examples.\n- **few-shot-tf/ft:** One positive and one negative, order varied.\n\n### **Prompt contents:**\n- **Basic:** Plain instruction.\n- **Role:** \"You are Frederick, an AI expert in patch analysis. Your task is to decide whether...\"\n- **Act As-System:** System message as role declaration.\n- **Act As-User:** User message as role declaration.\n- **GPT-generated:** Generated by ChatGPT itself with elaborate task breakdown.\n- **Emotion:** Four variants, all include a role and:\n    - **Encourage:** \"Remember, you’re the best...\"\n    - **Threaten:** “…otherwise you are the worst.”\n    - **Reward:** “If you perform well, I will tip you.”\n    - **Punish:** “If you perform badly, you will be fined.”\n\n#### **Example from Table I for SPD:**\n\n- *Basic:*\n  - System: \"You are a helpful assistant. (DEFAULT)\"\n  - User: \"Decide whether a patch is a security patch (SRP) or non-security patch (NSP).\"\n- *Role variant:*\n  - System: \"You are Frederick, an AI expert in patch analysis. Your task is to decide...\"\n- *GPT-generated:*\n  - System: \"You are a helpful assistant.\"\n  - User: Multi-sentence, self-reasoning version describing how to examine patches.\n\n#### **Combined content:**  \nExperiments tested, for example:\n- (Role + GPT-generated) in 0-shot context\n- (Emotion + GPT-generated) in 0-shot context\n\n### **Sampling strategy:**  \n- Each structure/content combination is run three times per task to compute average results and standard error.\n\n### **Metrics:**  \n- Accuracy, recall, precision, F1.\n\n### **Statistical validation:**  \n- Standard error used to determine significant differences (twice the SE threshold).\n\n---\n\n# TLDR Table\n\n| Q# | Answer (summary) |\n|---|------------------|\n| 1 | Paper investigates how prompt structure & content affect LLMs on security tasks; unique as first systematic, quantitative study in this domain. |\n| 2 | Three datasets: PatchDB (SPD, ~36k), Devign (VFD, ~49k), PatchNet (SPC, ~82k); 2,000-sample balanced subsets per task. |\n| 3 | GPT-3.5-Turbo (OpenAI API) only. |\n| 4 | Structures: zero-shot, 1-shot, few-shot (+ 6 variants). Contents: basic, role, Act As (system/user), GPT-generated, and 4 emotion-based types. |\n| 5 | Section II, Tables I-IV. |\n| 6 | No universal best; few-shot-ff is best for SPD & SPC, 0-shot/1-shot for VFD. Content and structure can alter accuracy by up to 30%. Punish/Role/GPT-generated sometimes help. |\n| 7 | Depends on data imbalance, task type, psychological priming, and prompt complexity. Structure/content can prime LLM differently. |\n| 8 | See Tables I-IV: all structure/content variants, example prompts, emotion/role specifics, and combination experiments. |\n\n---\n\nIf you need any of these sections expanded further (e.g. specific numbers from results, or side-by-side prompt examples), just ask!"
  },
  {
    "filename": "checker-bug-detection-and-repair-in-deep-learning-libraries.pdf",
    "analysis": "Certainly! Based on the paper text you provided, here is a detailed answer to each question, with references to the relevant sections. (Direct quotes are in “...”. Where relevant, I include justifications and specifics.)\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Point & Exploration**\n- The paper focuses on **\"checker bugs\"** in Deep Learning (DL) libraries (PyTorch and TensorFlow). Checker bugs are defects in input validation & error-checking code—such as missing or incorrect condition checks.\n- These bugs can cause \"silent failures, incorrect results, or unexpected program behavior in DL applications\", threatening the reliability of AI systems built on these libraries.\n- While checker bugs in conventional software (e.g., C/C++ programs) have been studied, the characteristics of such bugs in DL libraries are underexplored.\n\n**Uniqueness**\n- It is the **first comprehensive analysis** (\"the first comprehensive study\") specifically for checker bugs in **DL libraries**.\n- Creates a **benchmark dataset** of 527 real checker bugs, analyzed by root causes, symptoms, and fixing patterns.\n- **Shows that checker bugs in DL libraries are more diverse (root causes and symptoms) than in traditional software**, justifying the need for new taxonomy/classification.\n- Proposes and evaluates **TensorGuard**, the first (to their knowledge) LLM+RAG-based tool for detecting & repairing DL checker bugs, using prompt engineering.\n\n---\n\n**2. What dataset did they use?**\n\nThey used *three* kinds of data:\n\n**A. For taxonomy & analysis:**  \n- All checker bug-related commits in PyTorch and TensorFlow GW repos between Sept 2016 and Dec 2023.\n    - Used 102 keywords to select relevant commits (Section II.A.2–4).\n    - Manually labeled 527 checker bugs (221 in PyTorch, 306 in TF), constructing the main \"benchmark dataset\".\n\n**B. For RAG database (external knowledge for TensorGuard):**\n- All commits (not just checker-bug ones) from PyTorch & TensorFlow (2016–2023).\n    - “61,453 commits for PyTorch and 150,352 for TensorFlow, parsed into 391,571 and 920,108 code changes” → total of 1.3 million code changes for RAG.\n\n**C. For TensorGuard Evaluation (testing on fresh bugs):**\n- All commits in PyTorch & TensorFlow from Jan 2024 to July 2024 filtered by heuristics, yielding a test set of 92 buggy and 135 clean checker-related changes.\n\n**D. For generalization:**  \n- Collected 87 checker-related commits (493 changes) from JAX (another DL library) for additional detection/repair experiments.\n\n---\n\n**3. What LLM models were used?**\n\n- **Main model: [OpenAI] ChatGPT (GPT-3.5-turbo)**\n    - Used as backbone for all stages of their multi-agent (detection, analysis, repair) pipeline in TensorGuard.\n    - \"We experiment with ChatGPT, i.e., GPT-3.5-turbo, due to its demonstrated effectiveness in software engineering tasks…\" (Section I Introduction and III.A).\n- **For baseline:**  \n  - **AutoCodeRover** (baseline repair tool) was run with [OpenAI] GPT-4o-mini, selected due to its 128K context window, matching the baseline tool’s requirements (Section III.D).\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\nThree main prompting strategies were discussed and evaluated for the LLM agents:\n\n1. **Chain of Thought (CoT):**\n   - Multi-step prompts. Instruct the model to analyze step-by-step: message comprehension, code change review, issue identification, impact analysis, decision.\n2. **Zero-Shot:**\n   - The LLM is prompted with the task description and the input, but no examples and minimal intermediate steps.\n3. **Few-Shot:**\n   - Model is given the prompt, PLUS two worked examples (extracted from their dataset) illustrating detection/categorization, to prime the model on expectations.\n\nAll are applied to the *checker bug detection* agent. CoT was also used to guide the repair agent (\"think step by step and generate a patch…\").\n\n---\n\n**5. Where are the prompt strategies explained?**\n\nThe prompt strategies are explained in detail in **Section III.A.2: Prompt Engineering for Different Agents**.\n\n- For each LLM agent (checker bug detection, root cause analysis, patch generation), they give the actual prompt templates in *Tables V–IX*.\n    - **Table V:** CoT prompt (detection agent)\n    - **Table VI:** Zero-shot prompt (detection agent)\n    - **Table VII:** Few-shot prompt (detection agent)\n    - **Table VIII:** Root cause analysis agent prompt\n    - **Table IX:** Patch generation agent prompt\n\nSee specifically the explanations above and below these tables in the paper text.\n\n---\n\n**6. What the results of the prompting strategies are and how successful or not were they?**\n\nCompare Table XI. The takeaways:\n\n**Main findings:**\n- **CoT:** Highest recall (finds nearly all real bugs), but suffers from low precision (many false positives).\n    - E.g., PyTorch: 100% recall, 50.75% precision. TensorFlow: 89.03% recall, 30.05% precision.\n- **Zero-Shot:** More balanced. Good recall and higher precision than CoT. \"A balanced performance between precision and recall\".\n    - PyTorch: 79.34% recall, 66.47% precision.\n    - TensorFlow: 63.2% recall, 57.5% precision.\n    - Avg F1 highest of the three.\n- **Few-Shot:** Highest precision, but lowest recall (misses more true bugs, but few false alarms).\n    - PyTorch: 69.3% precision, 37.0% recall.\n    - TensorFlow: 55.6% precision, 51.0% recall.\n\nIn *patch generation* (repair task):\n- TensorGuard generated 90 patches, 10 correct → 11.1% accuracy.\n- AutoCodeRover: 32 patches, 3 correct → 9.3% accuracy.\n- So, TensorGuard outperforms the baseline, but repair remains a very challenging task.\n\n---\n\n**7. Why did one strategy give better results than another?**\n\n- **CoT** does best in **recall** (finds almost all bugs), which is desirable in safety-critical/robustness contexts. The step-wise reasoning \"helps the checker bug detection agent break down the decision-making process... and catch more potential issues\". But this can overgeneralize, leading to more false positives (i.e., lower precision).\n- **Zero-shot** is more concise, finds a middle ground—since it doesn’t encourage step-by-step thinking, it is less likely to \"hallucinate\" extra problems.\n- **Few-shot** primes the LLM to be conservative, copying the style of the labeled examples; thus, it flags fewer commits, favoring precision over recall (fewer false positives, but misses some correct bugs).\n\nIn sum:  \n- **CoT = maximize recall (catch all possible bugs; tolerate more false alarms).**\n- **Zero-shot = tradeoff (general use).**\n- **Few-shot = maximize precision (minimize false alarms; risk missing some bugs).**\n\n---\n\n**8. All specifics of the researcher's prompt strategies.**\n\nDetailed per agent (see Section III.A.2 and Tables V–IX):\n\n**A. Checker Bug Detection Agent:**\n- **CoT prompt (Table V):**\n    - 6 explicit steps: \"understand commit message\", \"review code change\", \"identify potential issues\", \"analyze the impact\", \"make a decision\", \"output conclusion\".\n    - Model decides \"YES\" or \"NO\".\n- **Zero-shot prompt (Table VI):**\n    - Direct task description: Given commit message and change—YES/NO.\n- **Few-shot (Table VII):**\n    - Gives two complete labeled bug examples before the test case (examples drawn randomly from curated checker bug dataset).\n\n**B. Root Cause Analysis Agent:**\n- Single prompt (Table VIII).  \n    - Input: commit message.  \n    - Output: “Describe the root cause of the bug”.\n\n**C. Patch Generation Agent:**\n- (Table IX): Prompt includes:\n    - Two code change examples (before/after).\n    - A “bug explanation” (from root cause analysis agent).\n    - “Retrieved context” (relevant code from RAG DB).\n    - Target code snippet.\n    - Instructions: “think step by step and generate a patch”, with explicit instruction to ignore code indentation.\n    - Output should give reasoning steps + the generated patch.\n\n- **RAG retrieval:** Input is root cause → output is similar code change (from vector DB of 1.3M code changes). Helps patch agent see relevant historical patterns.\n\n**D. Agent execution:**\n- For each new commit, the agents are called in sequence: detection → root cause analysis → code retrieval from RAG → patch generation.\n\n---\n\n**In summary:**  \nThe paper uniquely addresses checker bugs in DL libraries, providing empirical insight and taxonomy, and systematically evaluating LLM-driven detection and repair with prompt strategies (CoT, zero-shot, few-shot), showing tradeoffs in recall/precision, and outperforming a baseline on patch generation—while also extensively documenting their prompt methodologies for full reproducibility."
  },
  {
    "filename": "prompt-enhanced-software-vulnerability-detection-using-chatgpt.pdf",
    "analysis": "Certainly! Here is a structured answer to your questions, based directly on the paper text:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point and Purpose:**  \nThe paper investigates whether and how Large Language Models (LLMs), specifically ChatGPT-4, can be enhanced to automatically detect software vulnerabilities. It aims to go beyond previous, naive uses of LLMs (simple Q&A) by introducing carefully constructed prompt engineering—including auxiliary (structural/sequential) information and chain-of-thought reasoning—to see if such designs improve detection performance.\n\n**Uniqueness:**  \n- It is the first work to systematically design and compare various prompt strategies tailored for vulnerability detection tasks, rather than generic prompting.\n- It directly integrates classic auxiliary code information (API call sequences and Data Flow Graphs, or DFGs) as context in prompts.\n- The study also leverages ChatGPT’s multi-round/dialogue (\"chain-of-thought\") abilities to potentially mimic the stepwise reasoning humans use to verify software vulnerabilities.\n\n---\n\n### 2. What dataset did they use?\n\nThey use two datasets:\n- **Java dataset:** Collected from the Software Assurance Reference Dataset (SARD), published by NIST. Contains 1,171 vulnerable and 917 non-vulnerable Java functions, focusing on the top-50 vulnerability types.\n- **C/C++ dataset:** Derived from the National Vulnerability Database (NVD), focusing on .cor.cpp files associated with vulnerabilities. Contains 1,015 vulnerable and 922 non-vulnerable functions.\n\n---\n\n### 3. What LLM models were used?\n\n- **ChatGPT-4**: The main focus; experiments and all prompt engineering strategies use ChatGPT-4.\n- **Baselines for comparison:** CFGNN (a graph neural network model for bugs) and Bugram (an n-gram language model for code).\n  - But *ChatGPT-4 is the only LLM model* tested in prompt engineering and main experiments.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nSeveral prompting strategies are explored:\n- **Basic Prompting**: A straightforward question—\"Is the following program buggy? Yes/No\".\n- **Role-based Prompting:** Asks ChatGPT to act as a \"vulnerability detection system\" (adds system role-context).\n- **Reverse-question Prompting:** Swaps the word \"buggy\" for \"correct\" to test for keyword bias in answers.\n- **Auxiliary Information Prompting:** Adds code-level info to the prompt:\n    - **API Call Sequences**: A sequence of external calls made by code.\n    - **Data Flow Graphs (DFG):** Paths of variable value assignment and use.\n- **Chain-of-Thought Prompting:** Multi-round (multi-step) dialogue in which the model first summarizes the code's intent/functionality, then is asked if the code is buggy, possibly with auxiliary information.\n- **Prompt Positioning:** Testing whether auxiliary info works better before or after the code body in the prompt.\n- **Combining Code Summaries:** Incorporating LLM-generated natural-language summaries of code as context for the vulnerability detection question.\n\n---\n\n### 5. Where are the prompt strategies explained?\n\n- **Section 4 (Prompt Design):** Main explanations; see subsections:\n    - 4.1: Basic Prompting\n    - 4.2: Prompting with Auxiliary Information (including API call, DFG; 4.2.1, 4.2.2)\n    - 4.3: Chain-of-Thought Prompting\n- **Section 5.5 (Impact of Position):** Details on placing auxiliary info before/after code in the prompt.\n\nPrompt templates are explicitly listed (pseudocode), showing what is put in each prompt.\n\n---\n\n### 6. What are the results of the prompting strategies and how successful or not were they?\n\n**Main Results:**\n- **ChatGPT-4 outperforms classic baselines** (CFGNN, Bugram) on both Java and C/C++ datasets when using tailored prompts.\n- **Basic Prompting**: Gives high recall on vulnerable samples (ChatGPT tends to predict \"vulnerable\" often), but is biased by prompt keywords.\n- **Role-Based Prompting**: Marginal improvement over basic; more so in Java than C/C++.\n- **Auxiliary Information (API call/DFG):**\n    - Improves accuracy in some cases, especially when incorporating API calls for Java.\n    - The effectiveness is programming-language-specific: API call info helps Java, DFG helps C/C++.\n    - Order matters: API calls work best before code; DFG works best after code in the prompt.\n- **Chain-of-Thought Prompting**:\n    - Greatly improves detection for C/C++, but can harm performance on Java.\n    - Confirms that stepwise reasoning is useful, but language-specific.\n- **Code summaries**: Sometimes help (C/C++), sometimes not (Java).\n- **Vulnerability-type Results**:\n    - Certain boundary/grammar/logical vulnerabilities (e.g., off-by-one, logic bombs, always-false-expressions) are detected with near-100% accuracy.\n    - Others (e.g., missing salt, weak hash) are poorly detected; these types require more contextual or domain-specific knowledge.\n\n---\n\n### 7. Why did one strategy give better results than another?\n\n- **API Call auxiliary info:** Java code often relies on clear library/API patterns, so providing that info aligns with how Java vulnerabilities often occur and are fixed. In C/C++, vulnerabilities are more about data/state manipulation (buffer overflows, etc.), so DFG is more useful.\n- **Prompt positioning:** Placing info in the spot that best primes the LLM’s attention helps—leading context is often more strongly attended to.\n- **Chain-of-Thought:** Helps C/C++ analysis (which is more \"reasoning-based\" for vulnerabilities), but for Java, the extra step may introduce confusion or irrelevant distractions.\n- **Role Prompting:** Frames the task more explicitly for the LLM, directing its reply style (especially for languages/contexts where the model is less sure).\n\n---\n\n### 8. All specifics of the researchers’ prompt strategies:\n\n#### Basic Prompt:\n- \"**Is the following program buggy? Please answer Yes or No. [CODE]**\"\n\n#### Role-based Basic Prompt:\n- \"**I want you to act as a vulnerability detection system. My first request is 'Is the following program buggy?' Please answer Yes or No. [CODE]**\"\n\n#### Reverse-question Prompt (Test for Keyword Bias):\n- **\"Is the following program correct?\"** (substitutes \"buggy\" with \"correct\")\n\n#### Prompting with Auxiliary Information:\n- **API Call Sequence Prompt:**\n    - Format: \"**I want you to act as a vulnerability detection system. I will provide you with the original program and the API call sequence, and you will act upon them. [APIdescription]. Is the following program buggy? [CODE]**\"\n    - [APIdescription]: Natural language list of API/function calls extracted from code.\n- **Data Flow Graph Prompt:**\n    - Format: \"**I want you to act as a vulnerability detection system. I will provide you with the original program and the data flow information, and you will act upon them. Is the following program buggy? [CODE]. [DFdescription]**\"\n    - [DFdescription]: Natural language explanation of variable assignments/flows.\n\n#### Chain-of-Thought Prompting:\n- **Step 1:** \"**Please describe the intent of the given code. [CODE]**\"\n- **Step 2:** \"**I want you to act as a vulnerability detection system. Is the above program buggy? Please answer Yes or No.**\"\n- *Also tested with API/DFG auxiliary info added in Step 2*.\n\n#### Prompt Positioning:\n- [APIdescription] and [DFdescription] tested both before/after [CODE] in the prompt.\n\n#### Prompt Naming:\n- P_b: Basic\n- P_r-b: Role-based\n- P_r-r-b: Reverse-question role-based\n- P_r-a-b: Role-based, API before code\n- P_r-b-d: Role-based, DFG after code\n- ... and their ordering variants (see Table 5 in text)\n\n#### Example APIdescription Construction:\n- \"**The program first calls KerberosPrincipal.new, then calls String.toCharArray, then calls KerberosKey.new, then calls KerberosKey.toString, and finally calls IO.writeLine.**\"\n\n#### Example DFdescription Construction:\n- \"**The data value of the variable data at the 17th token comes from the variable data at the 11th token or the variable data at the 14th token...etc.**\"\n\n#### Human Evaluation and Further Testing:\n- Generated answers to prompts were rated for quality by human annotators for code explanation and vulnerability explanation tasks.\n\n---\n\nIf you need the tables extracted or more example prompts, I can supply that too! Let me know if you want those."
  },
  {
    "filename": "multi-role-consensus-through-llms-discussions-for-vulnerability-detection.pdf",
    "analysis": "Certainly! Here’s a breakdown answering your questions based on the content of the provided paper **\"Multi-role Consensus through LLMs Discussions for Vulnerability Detection\"**:\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point and Exploration:**  \nThe paper aims to **improve software vulnerability detection using large language models (LLMs)** by simulating a real-world code review process, where not just one, but multiple roles (e.g., developers and testers) are involved. Most prior work only uses a single-role (tester) perspective, potentially missing context or deeper understanding.\n\n**Uniqueness:**  \n- Introduces a **multi-role approach**: LLMs act as both \"tester\" and \"developer,\" engaging in discussion to reach consensus.\n- This simulates an actual team-based code review, integrating multiple viewpoints into the automation via LLM \"dialogues,\" rather than relying solely on a single QA/tester prompt.\n- Shows notable improvements in precision, recall, and F1 over single-role approaches.\n\n---\n\n## 2. What dataset did they use?\n\n**Dataset:**  \n- A C/C++ dataset referenced from [6]: *SySeVR: A framework for using deep learning to detect software vulnerabilities*  \n- Contains both vulnerable and non-vulnerable code segments, with samples grouped by four vulnerability categories:\n  - Function call (FC)\n  - Arithmetic expression (AE)\n  - Array usage (AU)\n  - Pointer usage (PU)\n- Three groups with different ratios of vulnerable/non-vulnerable samples (see Table I).\n\n---\n\n## 3. What LLM models were used?\n\n- **gpt-3.5-turbo-0125** (OpenAI)  \n  All LLM experiments were performed using this single model instance for fairness.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\nTwo main **prompting strategies** are evaluated:\n\n1. **Basic Prompt:**  \n   - Directly asks the LLM whether a given code segment contains a particular type of vulnerability, with no additional reasoning steps.\n\n2. **Chain-of-Thought (CoT) Prompt:**  \n   - Encourages the LLM to think step-by-step (reasoning through the problem), e.g., \"explain your reasoning for whether or not a vulnerability exists.\"\n\nBoth strategies are applied in both the single-role setup (just the tester) and the multi-role dialogue (tester–developer discussion).\n\n---\n\n## 5. Where are the prompt strategies explained?\n\n- Prompt strategies are explained in **Section 3.1 (Experiment Settings)**:  \n  > “Inspired by [3], the following two types of prompts are used to test these two approaches. The basic prompt directly asks the LLM… Chain-of-thought (CoT), i.e., step-by-step, is also used…”\n\n---\n\n## 6. What are the results of the prompting strategies and how successful or not were they?\n\n- **Overall Results:**  \n  - The **multi-role approach** (with LLMs simulating tester and developer discussions) outperforms the single-role approach (tester only).\n  - Across all metrics, the multi-role method achieved:  \n    - **13.48% higher precision**\n    - **18.25% higher recall**\n    - **16.13% higher F1 score**  \n  - Both basic and CoT prompting are improved by using the multi-role consensus method.\n- **Cost:**  \n  - Multi-role doubles token usage (484% increase in tokens), i.e., higher computational cost.\n- **Improvement Details:**  \n  - Improvements especially strong when more vulnerable samples are present in the test set.\n\n---\n\n## 7. Why did one strategy give better results than another?\n\n**Why Multi-role Is Better:**  \n- Discussion between different simulated roles enables cross-examination:  \n  - The *developer* can provide code-intent clarifications that a tester alone might miss.\n  - This back-and-forth leads to more correct judgments by challenging and refining each other's reasoning.\n- The example in the paper shows the tester’s conclusion being corrected after considering the developer’s technical input.\n\n**Why Chain-of-Thought helps:**  \n- CoT prompting encourages reasoned, stepwise analysis, reducing mistakes due to superficial reading of code.\n\n**Overall:**  \n- Multi-role + CoT is most effective because it combines stepwise reasoning with multiplicity of perspectives, closely mimicking real-life code review dynamics.\n\n---\n\n## 8. All specifics of the researcher's prompt strategies\n\nHere’s a detailed account:\n\n### Strategy 1: **Single-role Approach**\n- **Roles:** Only \"tester\"\n- **Prompt Types:**\n  - **Basic:**  \n    E.g., *\"Does the following code segment contain a vulnerability of type FC, AE, AU, or PU? (Yes/No)\"*\n  - **Chain-of-Thought (CoT):**  \n    E.g., *\"Please think step by step: analyze whether the following code segment contains a vulnerability in FC, AE, AU, or PU. Explain your reasoning.\"*\n\n### Strategy 2: **Multi-role Consensus Approach**\n- **Roles:** \"Tester\" and \"Developer\", both simulated by LLMs\n- **Stages:**\n  1. **Initialization:**  \n     - Tester receives prompt (as above) with code segment and provides a binary decision (1=vulnerable, 0=not) plus reasoning.\n  2. **Discussion Stage:**\n     - Tester and Developer engage in iterative dialogue:\n        - Each presents arguments, critiques, or alternative perspective.\n        - Each round: “pose query - deduce response - relay insight.”\n     - Maximum rounds: Pre-set (5); each response is capped at 120 tokens (to stay concise and focused).\n  3. **Conclusion Stage:**  \n     - When consensus (agreement or max rounds reached), the most recent \"tester\" judgment is recorded as final.\n- **Prompt Type Within Multi-role:**  \n  Both \"basic\" and \"CoT\" styles were used in these discussions.\n\n### Constraints\n- Each prompt round capped at 120 tokens, maximum 5 dialogue turns.\n\n---\n\n**References for prompts:**  \n- Section 2 (\"MULTI-ROLE APPROACH\") describes the staged process and constraints.\n- Section 3.1 describes prompt variants and experiment settings.\n\n---\n\n### Summary Table of Prompt Strategies\n\n| Strategy      | Roles Involved   | Prompt Style   | Discussion?   | Notes                    |\n|---------------|------------------|---------------|--------------|--------------------------|\n| Single-role   | Tester           | Basic, CoT    | No           | Baseline                 |\n| Multi-role    | Tester & Developer | Basic, CoT    | Yes (up to 5 rounds) | Simulates real review |\n\n---\n\n**In conclusion:**  \nThe paper shows that by moving from a single, tester-focused query to a dynamic, multi-role dialogue – especially when paired with Chain-of-Thought reasoning – LLMs can automatically detect vulnerabilities in code far more effectively and realistically, albeit at higher compute cost."
  },
  {
    "filename": "gptscan_detecting-logic-vulnerabilities-in-smart-contracts-by-combining-gpt-with-program-analysis.pdf",
    "analysis": "I'll provide structured, concise answers addressing your 8 questions about the paper \"GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis\". Each answer includes specific citations from the text you supplied.\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n#### Point & Exploration\n- The paper presents **GPTScan**, a novel tool to detect business logic vulnerabilities in smart contracts by **combining large language models (GPT-3.5) with static program analysis**.\n- It's motivated by findings that **~80% of real security bugs in DeFi smart contracts cannot be detected by traditional analysis tools**, as these focus only on simple, fixed patterns (like reentrancy or integer overflow) and not on project-specific business logic issues.\n- The main exploration is: \"Can GPT models improve logic vulnerability detection in smart contracts, if combined with static analysis?\"\n\n#### Uniqueness\n- Unlike pure GPT-based approaches (which lead to high false positives and require advanced reasoning abilities), GPTScan **breaks down vulnerability types into code-level \"scenarios\" and \"properties\"** that GPT can match more reliably.\n- It **uses GPT as a code understanding tool** — not as a sole oracle — and **cross-checks results with static analysis for validation**, greatly reducing false positives (by about 2/3 compared to GPT-only approaches).\n- GPTScan introduces the **\"mimic-in-the-background\" prompting strategy** to reduce output randomness.\n\n**Key quotes:**\n> \"We propose GPTScan, the first tool combining GPT with static analysis for smart contract logic vulnerability detection. Instead of relying solely on GPT... we utilize GPT as a versatile code understanding tool.\" (Abstract)\n\n> \"GPTScan... breaks down each logic vulnerability type into scenarios and properties... then validated by static confirmation.\" (Abstract)\n\n---\n\n### 2. What dataset did they use?\n**Three real-world datasets:**\n- **Top200**: 303 (top market cap, assumed well-audited) contract projects, 555 Solidity files, 134K LOC, 0 known logic vulnerabilities (for false positive checking).\n- **Web3Bugs**: 72 contract projects (collected from the Web3Bugs dataset, mostly Code4rena-audited), 2,573 Solidity files, 320K LOC, 48 known logical vulnerabilities.\n- **DefiHacks**: 13 projects, 29 files, 18K LOC, 14 known vulnerabilities (all real exploited contracts).\n- **Total**: 388 projects, 3,157 Solidity files, 472K LOC, 62 ground-truth logic vulnerabilities.\n\n**Reference: Table 2**\n> \"Together, these datasets comprise around 400 contract projects, 3K Solidity files, 472K lines of code, and include 62 ground-truth logic vulnerabilities.”\n\n---\n\n### 3. What LLM models were used?\n- **GPT-3.5-turbo** was used for all experiments and in deployment due to its cost-effectiveness (~20x cheaper than GPT-4) and sufficient context size.\n- Brief testing with GPT-4 (**no notable gains, much higher cost**), and discussion of possible future benchmarking with other models (Bard, Claude, LLaMA) when accessible.\n\n**Reference:**\n> \"We implemented GPTScan with the widely used GPT-3.5-turbo model... more cost-effective... than the advanced GPT-4 model.\"\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n#### Main strategies:\n- **Scenario and Property Matching**: For each vulnerability, create precise prompts where the LLM must answer YES/NO to targeted scenario/property questions (e.g., \"Does this function transfer tokens without re-checking approval?\").\n- **Mimic-in-the-background prompting**: LLM is told (in the prompt) to \"internally mimic answering the same query five times and return the most frequent answer,\" to mitigate output randomness.\n- **Variable and Statement Recognition Prompts**: Instructional prompts to extract critical variables/statements (e.g., \"Which variable stores the total supply? Name it and describe.\").\n\n**Prompt template excerpt** (see §4.2 and Figure 4 in paper):\n- *System: You are a smart contract auditor. You will be asked... answer in the background five times and provide the most frequently appearing answer. ... adhere to output format.*\n- *ScenarioMatching, PropertyMatching templates provided, instructing YES/NO answers only, in strict JSON format.*\n\n---\n\n### 5. Where are the prompt strategies explained?\n- **§4.2: GPT-based Scenario and Property Matching**: Explains scenario/property abstraction, YES/NO queries, and prompt templates (Figure 4).\n- **Section titled \"PromptTemplate\"** and **Figure 4**: Gives exact example of the prompt used for scenario/property matching, and the reasoning for format.\n- **\"Minimizing the impact of GPT output randomness\"**: Describes \"mimic-in-the-background\" prompt strategy.\n\n---\n\n### 6. What are the results of the prompting strategies and how successful or not were they?\n- **Effectiveness**:\n    - **GPTScan (with enhanced prompting and static confirmation):**\n        - **Top200 (no bugs):** 4.39% false positive rate.\n        - **DefiHacks:** 90.91% precision, 71.43% recall.\n        - **Web3Bugs:** 57.14% precision, 83.33% recall.\n    - Prompting yielded much **lower false positives and higher recall** than prior GPT-only tools (e.g., 4.14% precision, 43.84% recall, 7.57% F1 for David et al. [34]).\n\n- **Static confirmation step using variable/statement recognition extracted by prompt templates reduced false positives by about 66%**.\n\n**Key quote:**\n> \"GPTScan achieves high precision (over 90%) for token contracts and acceptable precision (57.14%) for large projects... Static confirmation helps GPTScan reduce two-thirds of false positives.”\n\n---\n\n### 7. Why did one strategy give better results than another?\n- **Problem with pure GPT prompting (e.g., just high-level vulnerability description)**: This leads to ambiguity, GPT hallucinations, and very high false positives because GPT struggles to link complex code-level semantics to vague descriptions.\n- **GPTScan’s advantage**: By **breaking vulnerabilities into concrete, programmable scenario/property queries** and demanding strict YES/NO, GPT answers are less ambiguous, and static analysis cross-checks filter hallucinations and context errors.\n- **\"Mimic-in-the-background\"** lowers the rate of unlucky random answers, as LLM output for the same prompt can vary.\n\n> “Existing GPT-based works typically feed GPT with high-level vulnerability descriptions... which demands advanced reasoning... challenging for GPT to directly interpret code-level semantics.”\n\n> “Static confirmation effectively filtered out 65.84% of the false positive cases ... This finding underscores the importance of combining GPT with static analysis.”\n\n---\n\n### 8. All specifics of the researcher's prompt strategies.\n#### a. Scenario and Property Matching (see §4.2, Table 1, Figure 4)\n- For each vulnerability, define:\n    - **Scenario**: High-level code situation (e.g., \"deposit/mint/add the liquidity pool/amount/share ... the total share is set to the number of first deposit when supply is 0\").\n    - **Property**: Specific code attributes/operations (e.g., \"no clear/reset of the approval when the transfer finishes\").\n- Prompt structure:\n    - **System prompt** directs LLM to mimic 5 answers and provide the most frequent.\n    - **User prompt**: All scenario/property questions. LLM must output answers as JSON, with YES/NO per property.\n\n#### b. Variable/Statement Recognition Prompt (see §4.4, Figure 5)\n- After a function matches a vulnerability scenario:\n    - **Prompt**: “In this function, which variable or function holds the total supply/liquidity AND is used by the conditional branch to determine supply/liquidity is 0? Please answer in a section starting with ‘VariableB:’...”\n    - Output format: Strict JSON list of variable names and brief descriptions, to be parsed by code.\n\n#### c. Strict Output Enforcement\n- Prompts instruct the LLM **not to give explanations**, only formatted YES/NO outputs or variable name lists to ease downstream parsing.\n\n#### d. “Mimic-in-the-background”\n- System prompt tells the LLM to \"mimic answering in the background five times; provide the most common output,\" prompting more stable, reliable responses.\n\n#### e. Temperature Setting\n- GPT temperature parameter set to **0 for determinism**; minimal output randomness.\n\n---\n\n### Summary Table\n\n| Prompt Strategy           | Where Used          | What it Does                                           | Implementation Detail                                 |\n|---------------------------|---------------------|--------------------------------------------------------|-------------------------------------------------------|\n| Scenario/Property Prompt  | §4.2, Figure 4      | YES/NO, per vulnerability definition                   | System prompt + single/batched user prompt (JSON out) |\n| Variable Recognition      | §4.4, Figure 5      | Extract relevant var names/statements for static check  | JSON output, description included                     |\n| Mimic-in-the-background   | Throughout          | Repeats answer 5 times, returns most frequent          | System prompt instructs, helps with GPT randomness    |\n| Strict Output/Parsing     | All prompting       | Enforces answer format for postprocessing              | Direct in-user/system prompt                          |\n| Temperature=0             | All GPT use         | Ensures output determinism                             | Model config parameter                                |\n\n---\n\n## TL;DR of the Paper's Key Methods and Results\nGPTScan uses GPT-3.5 as a code-understanding assistant by asking YES/NO scenario/property questions about candidate functions (after static filtering). If a potential vulnerability exists, further GPT prompts extract the relevant variables/statements, which are then formally confirmed/denied by static analysis tools. This multi-step, strict prompting and validation, plus \"mimic-in-the-background\", yields a system that is much more precise and scalable than pure GPT-based (description-only) or pure static approaches, finding bugs missed by human auditors and doing so fast and cheaply."
  },
  {
    "filename": "llmdfa_analyzing-dataflow-in-code-with-large-language-models.pdf",
    "analysis": "Certainly! Here’s a comprehensive breakdown of the LLMDFA paper based on your questions and the supplied text.\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n### Point and Exploration\n- **Problem:** Classical dataflow analysis (DFA) techniques for code rely on compiled intermediate representations (IR), which makes them inapplicable to incomplete or uncompilable code, and require significant expertise to customize for new bug types.\n- **Goal:** The paper presents **LLMDFA**, a framework that applies large language models (LLMs) for *compilation-free, customizable dataflow analysis* directly on source code, focusing on code bug detection.\n- **What it explores:** It investigates whether LLM-powered workflows can circumvent traditional DFA's limitations (compilation requirement, inflexibility/customization hurdles, and user expertise) and if decomposition into smaller reasoning tasks plus LLM prompt strategies can achieve performant, reliable DFA.\n\n### Uniqueness\n- **First to leverage LLMs as code-tool synthesizers for dataflow analysis**: Synthesis includes code to parse code ASTs and scripts that query SMT-solvers for path feasibility.\n- **Fully compilation-free** (works on incomplete code).\n-**Customizable** to new bug types via natural language descriptions (users don't need expert-level knowledge).\n- **Methodologically novel**: Combines prompt engineering (few-shot, chain-of-thought) with automated tool synthesis (scripts for extraction, validation) to combat LLM hallucinations, addressing known LLM limitations in reliability and robust reasoning.\n\n---\n\n## 2. What dataset did they use?\n\n- **Synthetic Dataset:** [Juliet Test Suite](https://samate.nist.gov/SRD/testsuite.php) ([17]), a widely-used set of synthetic C/C++ and Java programs with seeded bug patterns (divide-by-zero, XSS, command injection).\n    - For this paper: Java (main), C/C++ and JavaScript used for additional evaluation.\n    - **Coverage:** 1,850 DBZ, 666 XSS, 444 OSCI bugs (evaluation removes comments and obfuscates code to avoid ground truth leakage).\n- **Real-world Dataset:** [TaintBench](https://jianguo-liu.github.io/TaintBench/) ([19]), 39 real-world Android malware apps with ground-truth source-sink annotations for taint analysis (i.e., bug-inducing flows).\n- **Extra (for transferability):**\n    - SecBench.js for JavaScript (command injection/taint path/code injection validation).\n    - Juliet Test Suite (C/C++ variant).\n\n---\n\n## 3. What LLM models were used?\n\nFour state-of-the-art LLMs were utilized:\n- **GPT-3.5-turbo-0125** (“gpt-3.5” in the text)\n- **GPT-4-turbo-preview** (“gpt-4”)\n- **Gemini-1.0-pro** (“gemini-1.0”)\n- **Claude-3-opus** (“claude-3”)\n  \nThese cover both OpenAI, Google, and Anthropic's model families to compare performance and generality.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\nThe paper introduces *multiple* prompt engineering strategies at different analysis phases to mitigate LLM hallucinations and improve accuracy:\n\n### a. **Code Synthesis as Prompt Target**\n- Instead of asking LLMs to *directly* extract sources/sinks or validate path feasibility (which leads to hallucinations), they are prompted to *generate scripts* (e.g., using parsing libraries and SMT solvers). The output is code, not just text answers.\n\n### b. **Few-Shot Chain-of-Thought (CoT) Prompting**\n- Used for *dataflow summarization* within individual functions.\n- Prompts include a series of example code snippets and detailed reasoning/explanations, helping the LLM to model correct dataflow patterns step by step—mirroring human reasoning and giving semantic grounding.\n- Each prompt expects a Yes/No answer plus a stepwise explanation.\n\n### c. **Iterative Fixing Loops**\n- When generator scripts (for extraction or validation) are erroneous, error messages and outputs are looped back into the prompt context, asking LLMs to “fix” the code until a passing script is synthesized.\n\n### d. **Skeleton-based Prompting**\n- For path feasibility validation, a partial Python script (skeleton) is included in the prompt; the LLM must fill in the specifics based on a provided path condition.\n\n---\n\n## 5. Where are the prompt strategies explained?\n\n- **Main Paper, Section 3 \"Method\".**\n    - Phase explanations from 3.1 to 3.3 cover the design and utilization of these strategies.\n- **Appendix A.2 (\"Prompt of LLMDFA and Baseline\")**\n    - A.2.1: Principle.\n    - A.2.2: Actual prompt templates (shown in Figures 9, 10, 11).\n    - A.2.4: Details for end-to-end baseline prompts.\n- **Examples** of the actual prompts and code snippets included for each phase (see Figure 9–12 for concrete cases).\n\n---\n\n## 6. What are the results of the prompting strategies? How successful or not were they?\n\n### Quantitative Results\n- **Phase (Table 1): For all 4 LLMs, and all bug types (DBZ, XSS, OSCI):**\n    - **Source/sink extraction:** 100% precision and recall (by code generation, no hallucinated sources/sinks).\n    - **Dataflow summarization:** High precision/recall (e.g., up to 97% F1 with GPT-4).\n    - **Path feasibility validation:** High precision/recall in most cases; some drop in precision for DBZ if path conditions are complex.\n    - **Overall detection F1:** Ranges from 0.70 (hardest, gemini-1.0, DBZ) to 0.99 (easiest, claude-3, OSCI), with averages well above traditional baselines.\n\n### Qualitative Findings\n- **Ablation studies (Figure 8, Table 1, Appendix A.3/A.4):**\n    - Stripping out CoT (NoCoT), code synthesis (NoSynExt, NoSynVal) causes significant drops in recall and/or precision, demonstrating the impact of the prompt strategies.\n    - Direct, naive prompting fails due to hallucinations or inability to process complex dataflow patterns.\n- **Comparison to Baselines:**\n    - Far surpasses classical DFA tools (CodeFuseQuery, Pinpoint) in recall/F1 (esp. on incomplete code), while matching/approaching their precision.\n    - Far outperforms LLM “end-to-end” (non-decomposed) prompting pipelines, which are brittle and hallucinate much more often in both sources/sinks and dataflow facts.\n\n---\n\n## 7. Why did one strategy give better results than another?\n\n- **Decomposition:** Breaking DFA into three sub-problems means each LLM query targets a simple and focused task (source/sink extraction, single-function summarization, or path validation) instead of monolithic reasoning, reducing room for error/hallucination.\n- **Code Synthesis:** Letting LLMs generate parsing queries or validation scripts delegates sensitive reasoning to *deterministic, verifiable expert tools* (AST parsers, SMT solvers), rather than relying on the LLM’s own internal reasoning.\n- **Few-shot CoT:** Aligns LLM’s reasoning closely with actual dataflow semantics and exposes it to common/edge patterns, mitigating LLM \"guesswork\" and reinforcing the correct logic flow.\n- **Iterative Fixing:** Quickly recovers from LLM mistakes by integrating real-world error feedback, which grounds the LLM’s output and leads to convergence on valid code and results.\n- **Contrast:** End-to-end reasoning or omission of these strategies puts the burden on the LLM for holistic semantics (across complex code) and error-prone details (e.g., parsing, constraint processing), leading to more hallucinated or incomplete answers.\n\n---\n\n## 8. All specifics of the researcher's prompt strategies\n\nBelow is a detailed synthesis, integrating the examples and explanations from the paper and appendix:\n\n### a. **Phase 1: Source/Sink Extraction**\n- **Prompt Goal:** Synthesize a script that, given the program ASTs, extracts sources and sinks according to the bug specification.\n- **Structure:**\n    - Role/Situation: “You are a good programmer familiar with ASTs.”\n    - Inputs: Specification of source/sink forms (**Spec**), examples of source/sink-labeled programs and their ASTs (**Example Programs + ASTs**), and a code skeleton for traversing ASTs.\n    - Prompt: “Write a Python script to extract sources/sinks on AST. Refer to the provided AST structures and code skeleton.”\n    - Fix loop: “Here’s the buggy script from the last round and the error message. Missed/correct/incorrect sources/sinks are X/Y/Z. Please fix and return a new script.”\n- **Templates provided in Figure 9 and Appendix A.2.2.**\n- **Behavior:** This process is a one-time per bug type. The resulting extractor is then re-used for all subsequent programs in the same class.\n\n### b. **Phase 2: Dataflow Summarization (Few-Shot CoT)**\n- **Prompt Goal:** Determine, between two variables in a function, whether a dataflow fact exists (value dependence/equality, depending on bug type).\n- **Structure:**\n    - Role: “You are a good Java programmer experienced in semantics.”\n    - Description: State the rules for equality or dataflow, with several explicit rules (e.g., assignment means same value, unmodified variable means value preserved, etc.).\n    - **Few-shot examples:** Each includes a code snippet, user question (“Does [VAR1] at line [L1] have the same value as [VAR2] at line [L2]?”), and system answer (Yes/No plus exact step-by-step explanation covering assignments, uses, overwrites, etc.).\n    - Query: Function code, variables/lines.\n    - Template in Figure 10, A.2.2.\n- **Behavior:** By asking the LLM to reason step by step and explain each chain, hallucinations and missed edges are greatly reduced.\n\n### c. **Phase 3: Path Feasibility Validation**\n- **Prompt Goal:** Synthesize a Python script (using Z3 SMT solver) that encodes the path conditions for a given dataflow path and determines if it’s feasible.\n- **Structure:**\n    - Role: “You are a programmer familiar with Z3 python binding.”\n    - Inputs: Path info (sequence of statements and branch conditions), variable assignments, skeleton script with place-holders.\n    - Prompt: Complete/write the script, given the path and the skeleton.\n    - Fixing: If the generated script fails, the error message is wrapped back into the prompt and the LLM is asked to repair and retry. Capped to 3 rounds; fallback to direct reasoning if synthesis is not successful.\n    - Template in Figure 11, A.2.2.\n- **Behavior:** One script per path needs to be synthesized (not one-time per bug type as in source/sink extraction).\n\n### d. **End-to-End Baseline Prompting**\n- For comparison, the authors also tried prompting LLMs “end-to-end”—giving all code, sample questions for all kinds of sources/sinks, and asking for bug facts to assess the limitations of “monolithic” prompting.\n- Examples of these prompts and their failures are documented in Appendix A.2.4 and A.4.1.\n\n### e. **Ablation Studies**\n- **NoSynExt:** Directly prompts LLMs for source/sink lines, no extractor synthesis.\n- **NoCoT:** Summarizes dataflow facts with simple prompts/descriptions, not few-shot chain-of-thought.\n- **NoSynVal:** Directly asks LLM whether path is feasible, rather than via synthesized scripts for Z3.\n\n---\n\n# **Summary Table**\n\n| Phase                | Prompting Strategy         | Reason for Success                                       |\n|----------------------|---------------------------|----------------------------------------------------------|\n| Source/Sink Extract. | LLM-synthesized code over ASTs; error-feedback loop | Shifts task to deterministic code+tool, escapes LLM hallucinations; easily customized|\n| Dataflow Summarization | Few-shot CoT, explicit reasoning, stepwise explanations | Trains LLM on common patterns and exposes logic, mitigates error/hallucinations |\n| Path Feasibility     | LLM-synthesized Python (Z3), iterative error fixing | Lets expert tool solve constraint, prevents semantic hallucination, corrects script |\n| Baseline             | End-to-end few-shot prompting | Fails due to insufficient context, prompt overload, no modularization        |\n\n---\n\n**Conclusion:**  \nLLMDFA's prompting strategies, combining code synthesis, iterative repair, and few-shot chain-of-thought, offer a **paradigm shift** in program analysis with LLMs. They outperform both classical static analysis and naive LLM prompting by aligning tasks with LLM strengths, carefully curbing weakness (hallucinations) via decomposition and delegation to expert tools.\n\n**Let me know if you'd like code examples, further diagram explanations, or more details from any section!**"
  },
  {
    "filename": "detecting-code-smells-using-chatgpt_initial-insights.pdf",
    "analysis": "Certainly! Here’s a detailed breakdown of the paper “Detecting Code Smells using ChatGPT: Initial Insights” based directly on your questions and the provided text.\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**\n- The paper investigates the use of ChatGPT (specifically GPT-3.5 Turbo) to automatically detect code smells in Java projects. \n- Code smells are patterns in code that signal problems in design or implementation (“bad smells”), such as Blob, Data Class, Feature Envy, and Long Method.\n- The research compares how well ChatGPT can identify code smells under two different prompting strategies: a generic prompt (doesn't specify which smells to look for) and a specific (detailed) prompt (explicitly lists the smells).\n\n**Unique Aspects:**\n- This is one of the first studies to systematically evaluate LLMs for *code smell detection* (as opposed to more common tasks like code generation, summarization, or bug fixing).\n- It analyzes how prompt engineering—varying in specificity—impacts LLM performance for this software engineering task.\n- It employs a curated, industry-relevant dataset with severity annotations, providing a nuanced view on model effectiveness.\n\n---\n\n## 2. What dataset did they use?\n\n**Dataset:**\n- The study uses the **MLCQ dataset** ([9] in References).\n  - Contains 14,739 labeled instances of four code smells: Blob, Data Class, Feature Envy, Long Method.\n  - Each instance is annotated with a severity level (None, Minor, Major, Critical).\n  - For this study, only instances with severity Minor, Major, or Critical are used (None are discarded), totaling 3,291 instances: 974 Blob, 1,057 Data Class, 454 Feature Envy, 806 Long Method.\n\n---\n\n## 3. What LLM models were used?\n\n- **ChatGPT version 3.5-Turbo** (GPT-3.5-Turbo) was used in all experiments.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\nTwo main prompting strategies:\n\n### 1. **Generic Prompt (\"Prompt #1\")**\n- Asks ChatGPT to check if the provided Java code contains any code smells without specifying which types.\n- Simulates a base scenario to see if the model can detect different types of smells *autonomously* (i.e., open-ended detection).\n\n### 2. **Detailed/Specific Prompt (\"Prompt #2\")**\n- Lists explicitly the target code smells (Blob, Data Class, Feature Envy, Long Method).\n- Asks ChatGPT to identify which of these occur in the code, but not to describe them.\n- This tests if explicit task instructions improve detection accuracy.\n\n---\n\n## 5. Where are the prompt strategies explained?\n\n- **Section 2.2 Prompts Definition** (and Figures 1 and 2 in the paper) detail the prompts.\n- Also summarised in **Section 3 Research Questions** and **Section 4 Results**.\n- Full texts of both prompt templates are given (verbatim) in the paper.\n\n---\n\n## 6. What are the results of the prompting strategies and how successful or not were they?\n\n**Summary: Prompt #2 (detailed) outperforms Prompt #1 (generic).**\n\n### Prompt #1 (Generic)\n- Generally poor performance.\n- Best F-measure: Long Method (0.41)\n- For Blob, ChatGPT failed to correctly identify any instance.\n- F-measure for Data Class as low as 0.11; Feature Envy 0.23.\n\n### Prompt #2 (Detailed)\n- Modest but significant improvement, especially for Data Class (F-measure jumps to 0.59, a 0.48 increase!).\n- For Blob, some improvement (F-measure 0.19).\n- Improvement is much less pronounced for Feature Envy; in some cases, performance slightly dropped.\n- For overall performance, the statistical test (McNemar’s) showed that ChatGPT is **2.54 times more likely to give a correct answer with Prompt #2**; absolute success rate increased by **16%**.\n\n### By Severity\n- ChatGPT is best at detecting *Critical* smells (F-measure up to 0.52 with Prompt #2) and worst at Minor.\n- Data Class and Long Method smells are detected more accurately than Blob or Feature Envy.\n\n---\n\n## 7. Why did one strategy give better results than another?\n\n- **Providing explicit, targeted information directs the model’s attention to relevant features in the code**, overcoming its general uncertainty in the open-ended scenario.\n- Prompt #2 \"focuses\" ChatGPT on concrete categories, increasing recall and precision, particularly for code smells that manifest via clear textual/code patterns (as in Data Class).\n- When unspecified (Prompt #1), ChatGPT tends to generalize and may overlook subtler or less prototypical cases.\n- Some smells, like Feature Envy, remain challenging regardless of prompt, likely due to their context-dependency or complexity (needing inter-class information not provided in isolated code chunks).\n- The biggest benefit is for \"simpler\" smells or those that can be recognized via prominent features (like fields in Data Classes).\n\n---\n\n## 8. All specifics of the researcher's prompt strategies\n\n**Prompt #1: Generic Prompt (see Figure 1)**\n- Instruction: \"I need to check if the Java code below contains code smells (aka bad smells). Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.\"\n- Answer syntax: “YES I found bad smells. The bad smells are: (list, e.g., 1.Long method, 2.Feature envy…)” or “NO, I did not find any bad smell.”\n- [Followed by] The Java source code with the smell.\n\n**Purpose:** Test if ChatGPT can autonomously identify any of the four code smells without being told which ones to look for.\n\n---\n\n**Prompt #2: Detailed Prompt (see Figure 2)**\n- Instruction: \"The list below presents common code smells (aka bad smells). I need to check if the Java code provided at the end of the input contains at least one of them. \\n *Blob \\n *DataClass \\n *FeatureEnvy \\n *LongMethod\"\n- Then the same answer instructions as above.\n- [Followed by] The Java source code.\n\n**Purpose:** See if explicitly mentioning the target smells improves detection performance.\n\n---\n\n**Prompt Development:**  \n- Both prompts were iteratively refined after pilot tests to ensure clarity and consistency.\n- The answer format was constrained for easier automatic parsing of results.\n\n**Processing:**  \n- ChatGPT answers were parsed to extract identified smells; if output deviated from the expected format, manual correction happened (~27 instances).\n- Both prompts were sent to ChatGPT via automated Python scripts.\n\n---\n\n## In summary/at a glance:\n\n**- The paper tests ChatGPT (GPT-3.5-Turbo) for automatically detecting code smells in Java, focusing on how prompt specificity affects performance.**\n**- Dataset: MLCQ, four smells (Blob, Data Class, Feature Envy, Long Method), 3 severity levels, 3,291 instances.**\n**- Two prompt strategies: generic (no smell names) and detailed (lists the smells).**\n**- ChatGPT is much more accurate (up to 2.54x) with detailed prompts, especially for Data Class smells.**\n**- Smell/Severity matters: best performance for critical Data Class and Long Method, poorest for Feature Envy and Blob regardless of strategy.**\n**- Prompt specificity helps, but not a panacea for more complex/conceptual smells.**\n**- Prompts are fully laid out in Section 2.2 with all required formatting.**\n\n---\n\n**If you want further breakdowns by table or metric, or want the literal text of the prompts as used in the study, let me know!**"
  },
  {
    "filename": "llms-cannot-reliably-identify-and-reason-about-security-vulnerabilities_a-comprehensive-evaluation-framework-and-benchmarks.pdf",
    "analysis": "Certainly! Here’s a clear, point-by-point breakdown of the requested information based on the supplied text of the paper **\"LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities (Yet?): A Comprehensive Evaluation, Framework, and Benchmarks\"**.\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n### **Point and Exploration**\n- **Core Question**: Can Large Language Models (LLMs) be reliably used as security assistants to identify and reason about software vulnerabilities?\n- **Exploration**: The paper systematically investigates the capabilities, consistency, reasoning, and robustness of modern LLMs in the context of vulnerability detection—far beyond a simple binary “vulnerable/not vulnerable” judgment.\n- **Gap Addressed**: Prior work often used outdated or synthetic datasets, restricted to binary evaluations, or addressed only code generation/repair. There was no comprehensive, standardized, fully automated framework for evaluating LLMs on vulnerability detection (including reasoning and robustness).\n- **Unique Contributions**:\n  - Introduction of **SecLLMHolmes**, a fully automated, general, and extensible evaluation framework.\n  - Evaluation of 8 state-of-the-art LLMs across **8 key dimensions** (determinism, robustness, reasoning, real-world scenarios, etc.)\n  - Extensive prompt engineering analysis: 17 prompting techniques tested.\n  - Carefully constructed benchmark dataset (details below).\n  - Public release of framework and datasets for the community.\n\n---\n\n## 2. What dataset did they use?\n\n### **Dataset Details**\n- **Total: 228 code scenarios**, across two languages (C, Python) and various vulnerability classes.\n- **Components**:\n  - **48 Hand-crafted scenarios** (covering 8 critical MITRE CWEs; 3 difficulty levels each; vulnerable/patched pairs; mix of C/Python)\n  - **30 Real-world CVE scenarios**: Both vulnerable and patched versions of 15 real CVEs from 4 open-source projects, all published and fixed after LLM training cutoff (2023) to avoid contamination.\n  - **150 Code augmentations**:\n    - **Trivial**: e.g., adding whitespace, changing variable/function names, adding unused code.\n    - **Non-trivial**: e.g., swapping in dangerous/sanitizing library functions, masking safe/unsafe code via preprocessor macros.\n- **Additional for Reasoning**: 48 scenarios annotated with expert-generated ground-truth reasoning (100-word explanations).\n\n---\n\n## 3. What LLM models were used?\n\n### **Tested LLMs**\n- **OpenAI GPT-4** (Remote, 1.76T params, cutoff 09/2021)\n- **OpenAI GPT-3.5-turbo-16k** (Remote, 175B params, cutoff 09/2021)\n- **Google PaLM2** ('codechat-bison@001' & 'chat-bison@001', Remote, 340B params, cutoff mid-2021)\n- **Meta Llama2** ('codellama-7b', 'codellama-13b', 'codellama-34b', Local, 7B/13B/34B, cutoff 01/2022)\n- **StarCoder+ ('starchat-beta')** (Locally hosted, 15.5B, cutoff 09/2022)\n- All models are chat-oriented unless noted.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\n### **Prompting Categories**\nThe paper systematically explores prompting techniques along two axes:\n\n**A. Information Provided**\n- **Zero-Shot (ZS)**: No prior examples, just the raw question or task.\n- **Few-Shot (FS)**: Prompt includes a small set of examples (with patches and explanations).\n\n**B. Persona**\n- **Task-Oriented (TO)**: LLM given an explicit question/command (\"Is there an out-of-bounds write in this code?\")\n- **Role-Oriented (RO)**: LLM assigned a role (\"You are a security expert...\") via system prompt, instructing it to behave like an expert, think step-by-step, etc.\n\n**C. Prompt Template Types**\n- **Standard (S)**: Directly ask the question.\n- **Step-by-Step/Reasoning-based (R = chain-of-thought)**: Instruct LLMs to lay out reasoning step-by-step (e.g., include \"Let's think step by step\")\n    - R prompts also emulate multi-step processes used by human security experts (overview, sub-component analysis, detailed root cause).\n- **Definition-based (D)**: Prepend vulnerability definition (from MITRE CWE) to the input.\n\n### **Concrete Prompt Examples (Table 3 in text)**\n- S1: ZS-TO, direct question about a specific CWE.\n- S2: ZS-RO, direct question, but LLM assigned \"helpful assistant.\"\n- S3–S4: ZS-RO, different \"roles\" or omission of explicit questions.\n- S5/S6: FS-TO/FS-RO, with few-shot examples included.\n- R1–R6: Chain-of-thought reasoning (stepwise, role-based, multi-round).\n- D1–D5: Add CWE definition to prompt (with/without few-shots, with/without defining LLM role).\n\n**Total: 17 prompting templates.**\n\n---\n\n## 5. Where are the prompt strategies explained?\n\n- **Definition and breakdown**: Section 3.3 (Prompt Templates; Table 3).\n- **Examples**: Figure 1 (Chat input format), Table 3 (all prompt types with descriptions).\n- **Discussion on best prompt per setting/model**: Section 4.3 (Diversity of Prompts), Table 11.\n\n---\n\n## 6. What are the results of the prompting strategies? How successful or not were they?\n\n### **Key Findings**\n- **No Universal Best Prompt**: Performance varies greatly across LLM, vulnerability type, prompt, and code complexity.\n\n#### **General performance:**\n- **Top accuracy achieved:** GPT-4, using best prompts, achieves up to **~89.5%** on hand-crafted scenarios.\n- **High False Positives:** All models, including GPT-4, frequently flag already-patched code as vulnerable.\n- **Non-deterministic output:** Most models gave inconsistent answers across runs, unless temperature was forced to 0.0.\n- **Step-by-step ⟶ some improvement:** Role-based, stepwise or chain-of-thought (multi-step emulating human expert) prompts improved both accuracy and reasoning (reasoning scores up, especially for GPT-family).\n- **Definition helps some models:** Adding vulnerability definition increased accuracy/reasoning in some LLMs (especially GPT-4, codellama; less for PaLM2).\n- **Few-shot > Zero-shot:** Few-shot and role-oriented prompts outperform zero-shot for almost all LLMs.\n\n#### **But:**\n- All LLMs are fragile to small perturbations (see next sections).\n- None robust to code augmentations; e.g., GPT-4 gave wrong answers for 17% of cases with trivial/non-trivial modifications.\n- Performance on real-world CVEs: Very poor—LLMs often missed true vulnerabilities and/or flagged non-vulnerable, patched code.\n\n---\n\n## 7. Why did one strategy give better results than another?\n\n### **Analysis from Results**\n- **Role-Oriented & Few-Shot outperforms**: Assigning a vivid expert persona (e.g., ‘security expert’), *and/or* showing detailed, stepwise examples, \"grounds\" the LLM in the right task and reduces hallucination.\n- **Chain-of-thought aids reasoning**: Step-by-step reasoning encourages longer, more detailed analysis, which can catch more subtle bugs, and reflects workflows of expert humans (shown in their qualitative human pilot study).\n- **Definitional context**: Including definitions helps LLMs clarify what is being asked, reducing confusion or misinterpretation.\n- **Model biases**: Some LLMs were more sensitive to such context—e.g., GPT-4 was improved by vulnerability definitions, while PaLM2 models often did not explain their decisions even with prompts.\n- **Zero-shot/task-only prompts**: Tended to underperform—models \"guessed\" based on training data, not specific reasoning or domain expertise.\n\n#### **In sum:** Prompting strategies that guided LLMs to \"think like a security expert\" (role + few-shot + multi-step/chained reasoning) consistently produced more accurate *and* better-explained answers. Other prompt styles were subject to more hallucination, overflagging, or shallow pattern-matching.\n\n---\n\n## 8. All specifics of the researcher's prompt strategies\n\n### **Prompting Matrix/Details (from Table 3 & Section 3.3)**\nEach prompt combines:\n- **Shot level (zero/few)**\n- **Orientation (task/role)**\n- **Reasoning style (standard/stepwise/definition-based)**\nResulting in 17 specific prompt templates. Examples for clarity:\n\n#### **Standard / Task-Oriented**\n- *S1:* Zero-shot, task-oriented – Just code + specific question (\"Does this code have [CWE-name]?\")\n- *S4:* Zero-shot, role-oriented – LLM told it’s a security expert; code; *no explicit question* (tests if role identity alone focuses model).\n\n#### **Few-Shot / Task-Oriented**\n- *S5:* (FS-TO) – Question about a CWE + an example (vulnerable code, patch, and reasoning).\n- *S6:* (FS-RO) – As above, but LLM told it is a \"security expert\".\n\n#### **Step-by-step / Chain-of-thought**\n- *R1:* Zero-shot, task, but preface: \"Let’s think step by step.\"\n- *R2:* Zero-shot, role: LLM told to think as “security expert”, use multi-step reasoning.\n- *R3:* Multi-round: start with code, then probe sub-components one-by-one like an expert.\n\n#### **Definition-based**\n- *D1:* Zero-shot, task-oriented: Prepend MITRE CWE definition to prompt, then ask question.\n- *D3/D4:* Combine definition, few-shot, and/or role assignment.\n\n**Complete List of Prompt IDs & Types:**\n- S1–S6: Standard, various combinations of shot/orientation/role/few-shot.\n- R1–R6: Step-by-step, chain-of-thought, with/without examples, role assigned or not, multi-round reasoning.\n- D1–D5: Add official vulnerability definition, various combinations with other categories.\n\n### **Implementation in experiment:**\n- For test runs, each code scenario was paired with **all 17 prompt templates**—and for all LLMs. They then tracked for every code/prompt/model the answer, and if reasoning was present, and if it was correct (compared to ground truth explanations).\n- Measured accuracy, reasoning, faithfulness of reasoning, false positives, and output consistency.\n\n---\n\n## **Summary Table**\n\n| Aspect     | Details/Source |\n|------------|-------------------|\n| Dataset    | 228 scenarios: 48 hand-crafted, 30 real CVEs, 150 augmented |\n| LLMs       | GPT-4, GPT-3.5-16k, PaLM2 (chat-bison, codechat-bison), Codellama (7b/13b/34b), StarCoder+ |\n| Prompt Types | 17 templates – combinations of zero/few-shot, task/role, standard/step-by-step/definition |\n| Prompt Strategies Explained | Table 3, Section 3.3 (full details and types) |\n| Results    | Few-shot + role + chain-of-thought = best performance; all fragile to code changes; all with high FPR in real world |\n| Why One Better| Role/few-shot/step-by-step prompts give expert context, more reliable, reduce hallucination|\n| All Prompt Details | Table 3; system prompt construction; step-by-step templates; example multi-round conversations (Section 3.3) |\n\n---\n\nIf you need excerpts or further illustration on a specific prompt type (e.g., \"What exactly did the D3 prompt look like?\"), let me know!"
  },
  {
    "filename": "comparison-of-static-application-security-testing-tools-and-large-language-models-for-repo-level-vulnerability-detection.pdf",
    "analysis": "Certainly! Here’s a Q&A-style summary addressing your questions with explicit references to the content in the provided research text:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**  \nThe paper aims to **comparatively evaluate the effectiveness of Static Application Security Testing (SAST) tools and Large Language Models (LLMs)** for *repo-level* vulnerability detection—scanning entire code repositories to find security vulnerabilities. It investigates which approaches are most effective, their pros and cons, and how they might be combined for better results.\n\n**Uniqueness:**  \n- It is the **first** work to compare SAST tools and LLMs for *repo-level* vulnerability detection across three major programming languages: Java, C, and Python.\n- It introduces the *repo-level* vulnerability detection task for LLMs (whereas most prior work focused on function-level).\n- Constructs a **new repo-level Python vulnerability dataset** and curates new Java and C sets suitable for this purpose.\n- Evaluates 15 SAST tools and 12 LLMs using multiple adaptation strategies.\n- Proposes and analyzes ensemble techniques combining multiple SAST tools and LLMs.\n\n*See: Abstract, Introduction, Contributions (bulleted list).*\n\n---\n\n### 2. What dataset did they use?\n\n**Datasets:**  \nThe paper used three datasets, each representing a popular language:\n\n- **Java:** Dataset from Li et al. [49], containing real-world vulnerabilities labeled at function-level. The actual repository sources were curated by the authors from GitHub.\n  - 50 projects, 87 CVEs, 3,986,848 functions, 0.008% vulnerable\n- **C:** Dataset from Lipp et al. [51], also with function-level labels and actual vulnerable repositories collected from GitHub.\n  - 6 projects, 85 CVEs, 1,181,556 functions, 0.009% vulnerable\n- **Python:** **Newly constructed by the authors**\n  - 78 projects, 98 CVEs, 312,045 functions, 0.18% vulnerable\n\nEach dataset includes: CVE ID, CWE type, complete source code, and function-level vulnerability locations.\n\n*See: Section 2.2 Vulnerability Datasets, Table 1.*\n\n---\n\n### 3. What LLM models were used?\n\n**Models Used:**  \nTwelve open-source LLMs, covering two groups: lightweight (<1B parameters) and large (≥1B).\n\n- **Lightweight:**\n  - CodeBERT (0.13B)\n  - GraphCodeBERT (0.13B)\n  - CodeT5 (0.22B)\n  - UniXcoder (0.13B)\n\n- **Large:**\n  - CodeLlama (7B)\n  - StarCoder (7B)\n  - StarCoder2 (7B)\n  - DeepSeek-Coder (6.7B)\n  - CodeQwen (7B)\n  - Mistral (7B)\n  - Llama3 (8B)\n  - Phi3 (3.8B)\n\n*Also briefly compare with ChatGPT (gpt-3.5-turbo-0125) for reference.*\n\n*See: Section 2.4 Studied LLMs, Table 2.*\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n**Prompting Strategies:**  \nFour main adaptation techniques for LLMs:\n\n1. **Zero-shot Prompting:**  \n   Standard prompt with a task description; no labeled examples provided.\n\n2. **Chain-of-thought (CoT) Prompting:**  \n   Adds “Let’s think step by step” to encourage multi-step reasoning in the output.\n\n3. **Few-shot Prompting:**  \n   Includes a couple of labeled input-output examples after the prompt to guide the model.\n\n4. **Fine-tuning:**\n   - *Full Fine-tuning* for light models (<1B): All model parameters are updated on a balanced training set.\n   - *Parameter-Efficient Fine-Tuning (PEFT)* (e.g., LoRA) for larger models: Only a subset of model parameters is updated, reducing computation.\n\n*See: Section 2.5 LLM Adaptation Techniques, Section 2.5.1 Prompt-based Methods.*\n\n---\n\n### 5. Where are the prompt strategies explained?\n\n- **Prompt-based adaptation strategies are explained in Section 2.5.1.**\n- The actual *prompt format* includes:  \n  1. Task Description (“If the following code snippet has any vulnerabilities, output Yes; otherwise, output No”),  \n  2. Formatted Input (code between markers),  \n  3. Prediction Marker (“//Detection”).\n\nDetails for each adaptation are also in Table 2.\n\n---\n\n### 6. What are the results of the prompting strategies, and how successful or not were they?\n\n**Summary of Results:**\n\n- **Fine-tuning (especially for large LLMs)** - achieves the best tradeoff:\n  - Highest vulnerability detection rates (up to 100%)\n  - Marks fewer functions as vulnerable (i.e., lower false positives) than prompted-only runs\n- **Prompting-only methods (zero-shot, CoT, few-shot):**\n  - Generally achieve *lower* detection rates and *higher* false positive rates compared to fine-tuned LLMs.\n\n**Detailed Observations:**\n\n- *CodeBERT, CodeT5, UniXcoder, etc., when fine-tuned, reached 88–100% detection, but with high marked ratios (over 20–40%).*\n- *Zero-shot, CoT, and few-shot prompting for large LLMs like Llama3, StarCoder, etc., got 0–80% detection but marked >30% of code as vulnerable.*\n- Ensembling several LLMs (requiring majority agreement) reduces the marked (false positive) rate **by 40–75%**, but may drop detection slightly.\n- SAST tools had low detection rates (<44%) but extremely low false positive rates (<5%).\n\n**Takeaway:**  \n**Fine-tuned LLMs are the most successful among LLM approaches, with prompting alone much less effective.**\n\n*See: Table 3, Sections 4.1, 4.2, and 4.3.*\n\n---\n\n### 7. Why did one strategy give better results than another?\n\n**Reasoning:**\n- **Fine-tuning prevails** because it *explicitly adapts* an LLM’s weights with labeled vulnerability data, learning to distinguish subtle cues in code that correspond to real vulnerabilities.\n- Prompting-only methods rely on the LLM’s general knowledge and “in-context” training, without specific adaptation — this is insufficient for this detection task, especially due to data and label imbalance (very few vulnerable functions vs. many clean ones).\n- “Chain-of-thought” and “few-shot” prompting help, but not nearly as much as actually training the model.\n\n*See: Section 4.1.3 Comparisons Among LLMs; Section 2.5.1/2.5.2 for rationale.*\n\n---\n\n### 8. All specifics of the researchers' prompt strategies.\n\n**Prompt Structure (for all LLM variants):**\n- **Task description:** “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”\n- **Code delimited by:** “//CodeStart” and “//CodeEnd” markers.\n- **Output marker:** “//Detection” to explicitly mark where output should begin.\n\n**Adaptation/Prompting Methods:**\n- **Zero-shot:** Just the prompt above, provided for each function; no example output.\n- **Chain-of-thought:** Adds “Let’s think step by step” to the task description, encouraging the model to generate intermediate reasoning before prediction.\n- **Few-shot:** Adds two example code-label pairs to the prompt (one vulnerable, one clean) before the target function.\n- **Fine-tuning (for light models):** Actual model training with equal numbers of vulnerable and clean functions—full backpropagation.\n- **PEFT (for large models):** LoRA (Low-Rank Adaptation); only updates part of the model for efficiency; uses the same balanced data strategy.\n\n*Strategy for repo-level task:*  \n- Each repository is split into functions.\n- Each function is independently predicted for vulnerability.\n- Aggregated per-repo for calculating metrics.\n\n*See: Section 2.1.3 (Prompt Strategy for Repository-Level), Section 2.5.1/2.5.2 (Prompt details), Experimental Setup.*\n\n---\n\n**If you need this as a table or a more compressed summary, let me know!**"
  },
  {
    "filename": "sanitizing-large-language-models-in-bug-detection-with-data-flow.pdf",
    "analysis": "Certainly! Below I break down the key aspects of the \"Sanitizing Large Language Models in Bug Detection with Data-Flow\" paper as requested.\n\n---\n\n### 1. **What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Goal:**  \nThe paper tackles the problem of hallucinations (i.e. false positives) produced by Large Language Models (LLMs) when using them for automated code bug detection. Specifically, it seeks reliable LLM-driven bug detection even in incomplete code, where classic static analysis tools are not viable.\n\n**Explored Problem:**  \n- LLMs, when prompted to find bugs in code (using chain-of-thought or few-shot learning), often hallucinate: they report incorrect bugs that don't exist due to their limited grounding in actual code semantics.\n- Standard bug detection tools require compilation and expert configuration; LLMs allow non-expert customization but are unreliable due to hallucinations.\n\n**Unique Contributions:**\n- Introduces **LLMSAN**, a framework that:\n    - Forces LLMs to emit **data-flow paths** as concrete bug \"proofs\" (i.e., explicit sequences showing where buggy values originate and propagate in code).\n    - Proposes a multi-part **sanitization pipeline** to systematically validate (and potentially reject) these bug \"proofs\" using both deterministic program parsing and LLM-based reasoning.\n- Separates bug detection from code compilation and deep program analysis, allowing customization and use with incomplete code.\n- Shows, through experiments, that LLMSAN can increase precision (correctness) *dramatically* with little loss in recall.\n\n---\n\n### 2. **What dataset did they use?**\n\nThey used two datasets:\n- **Juliet Test Suite**: A well-known synthetic bug benchmark covering five Common Weakness Enumeration (CWE) bug types (Absolute Path Traversal, XSS, OS Command Injection, Divide-by-Zero, Null Pointer Dereference), each with 100 programs randomly chosen. Juliet provides ground truth for evaluation.\n- **TaintBench**: A benchmark of 39 real-world Android malware applications, annotated with 203 taint flows (potential information leaks). This represents real, complex software.\n\n---\n\n### 3. **What LLM models were used?**\n\nThe following LLMs were tested as the bug-detection engine powering the framework:\n- **gpt-3.5-turbo-0125** (\"gpt-3.5\")\n- **gpt-4-turbo-preview** (\"gpt-4\")\n- **gemini-1.0-pro** (\"gemini-1.0\")\n- **claude-3-haiku** (\"claude-3\")\n\n---\n\n### 4. **What are the prompting strategies discussed in this paper?**\n\n**Prompting strategy (core idea):**  \nLLMs are prompted—not just to label a line as buggy—but to emit an explicit **data-flow path** that justifies any bug report. Each data-flow path is a sequence of (value, line-number) pairs, tracing propagation from a buggy value's origin to its use at a dangerous location.\n\n**Two main levels of prompting:**\n1. **Few-shot Chain-of-Thought (CoT) Prompting** to extract data-flow paths:  \n   Prompts give example buggy programs, natural language explanations, and explicit data-flow paths, then ask the LLM to follow this reasoning style.\n2. **Sanitizer-Driven Prompts** to validate each segment of a given data-flow path, e.g., \"Can value X at line Y be zero?\", or \"Can the value at line A reach line B?\". These are focused on local code snippets for manageability and precision.\n\n---\n\n### 5. **Where are the prompt strategies explained?**\n\nPrompting strategies are mainly detailed in **Section 4 (\"LLM Sanitization for Bug Detection\")** of the paper.  \nKey figures and templates:\n- **Figure 4:** Example of data-flow-based bug explanations for few-shot CoT prompting.\n- **Figure 5:** Prompt template for the functionality sanitizer (to check if a value could have a specific property, such as being zero).\n- **Figure 6:** Prompt template for the reachability sanitizer (to check if a value can propagate from one line to another within the code).\n\n---\n\n### 6. **What are the results of the prompting strategies, and how successful were they?**\n\n**Main findings:**\n- **Raw (few-shot CoT) prompting**, without sanitization, yields high recall but very low precision (many false positives).\n- **With sanitization (LLMSAN):**  \n   - **Precision** improves dramatically. For instance, on bug detection across all types with gpt-4, precision rises from 69.04% (FSCoT baseline) to **91.03%** with LLMSAN.\n   - **Recall** remains comparably high (74.00%), only slightly reduced versus unconstrained CoT prompting.\n   - On TaintBench (real-world apps), precision rises from 28.68% (CodeFuseQuery) to **44.04%** (gpt-4, LLMSAN), a gain of 15.36%, while recall (true positives found) is similar or slightly improved.\n- LLMSAN pruned over 78% (on average) of spurious data-flow paths (i.e., hallucinated bugs) without sacrificing many true bug reports.\n\n**Ablation and baseline comparison:**  \nOther hallucination defenses (e.g., Ask-Check, Self-Consistency, CoT-Check) are far less effective; LLMSAN is consistently the best at identifying and pruning spurious results with minimum loss of true positives.\n\n---\n\n### 7. **Why did one strategy give better results than another?**\n\n**Fundamental reason:**  \n- **LLMSAN** adopts a rigorous, multi-part **validation** step for each LLM-emitted data-flow path. It examines both code syntax and code semantics at a fine granularity, splitting validation into deterministic (parser-based) and LLM-driven (reasoning-based) checks.\n- In contrast:\n  - **Plain CoT** or **FSCoT**: Only asks the model to reason, but trusts its (sometimes hallucinated) output.\n  - **Ask-Check**/**CoT-Check**: Only ask the LLM to evaluate its own outputs (not fully grounded in code semantics), leading to inconsistent and incomplete filtering of hallucinations.\n- LLMSAN re-uses LLMs only where necessary, ensuring each step is (1) as deterministic as possible, and (2) carried out on small, local code snippets—making it more robust and less error-prone.\n- Multiple sanitizer modules (type, functionality, order, reachability) cross-check each other, so a hallucination not caught by one pathway may be caught by another.\n\n---\n\n### 8. **All specifics of the researcher's prompt strategies**\n\n**Summary Table: Sanitizer Prompting Strategies**\n\n| Sanitizer              | Property Checked                 | Method               | Prompting Template (paraphrased)              |\n|------------------------|----------------------------------|----------------------|-----------------------------------------------|\n| **Type Sanitizer**     | Syntactic types consistency      | Parsing-based        | No prompt; checks type deterministically      |\n| **Functionality San.** | Value has dangerous property     | LLM Q&A prompt       | \"Given this code, can value X at line Y be zero? Think step by step\" |\n| **Order Sanitizer**    | Control-flow order consistency   | Parsing-based        | No prompt; checks order deterministically     |\n| **Reachability San.**  | Value propagation is possible    | LLM step-by-step Q&A | \"Given this code, can value at line A propagate to line B? Think step by step\" |\n\n**Prompt Details:**\n\n- **Few-shot CoT for data-flow path emission:**  \n    - Prompt:  \n      ```\n      Here are several examples containing DBZ [or other bug] bugs:  \n      [program+explanation+data-flow path examples]  \n      Please understand how zero values are produced.\n      Task: Analyze the code: [function]. Please check whether [value] at line [linenumber] can be zero. Please think step by step and conclude the answer with Yes or No.\n      ```\n    - LLM is shown examples and then the target code, tasked to emit a data-flow path as \"proof\" of a bug (not just a label/explanation).\n\n- **Functionality San. Prompt (Figure 5):**\n    - Checks: Can this variable/expression at line N actually have the bad/dangerous value (e.g., zero)?\n    - Template:  \n      ```\n      Examples: [examples contain bug type]. Please understand how zero values are produced.  \n      Task: Analyze the code: [function]. Please check whether [value] at line [line number] can be zero. Please think step by step and conclude the answer with Yes or No.\n      ```\n\n- **Reachability Sanitizer Prompt (Figure 6):**\n    - Checks: Can a value at one location in the code reach another location? (This guards against hallucinated data-flows that are syntactically impossible.)\n    - Template:  \n      ```\n      Examples: [examples with data propagation].  \n      Task: Analyze [function1], [function2]. Please check whether [value1] at [linenumber1] can be propagated to [value2] at [linenumber2]. Please think step by step and conclude with Yes or No.\n      ```  \n\n- **Order/type sanitizers:**  \n    - No prompt—these are deterministic, using just code parsing.\n    - Order sanitizer ensures that, for example, value propagation only happens in viable control-flow order (e.g., not \"backwards\" in unreachable branches).\n\n**Why these details?**\n- Using focused prompts on *small* code regions means LLMs are less overwhelmed (less hallucination).\n- Separation into syntactic/semantic checks allows rapid, reliable pruning of impossible bug reports with simple program analysis, and reserves LLM queries only for what's undecidable or requires \"understanding\" of program behavior.\n- This two-layer, property-specific prompting is more reliable than simply telling the LLM to justify all its results or check its own reasoning, since the latter, in programming tasks, is much less reliable.\n\n---\n\n### **Summary Table: Sanitizers, their Purposes, and Prompt Details**\n\n| Sanitizer           | Purpose                               | Verification Method      | Prompt Template?          |\n|---------------------|---------------------------------------|-------------------------|---------------------------|\n| Type Sanitizer      | Syntactic type correctness            | Parsing/deterministic   | None                      |\n| Functionality San.  | Realizes faulty value at origin/use   | LLM, local code prompt  | Yes (Figure 5)            |\n| Order Sanitizer     | Control-flow feasibility              | Parsing/deterministic   | None                      |\n| Reachability San.   | Value reachability between locations  | LLM, code pair prompt   | Yes (Figure 6)            |\n\n---\n\n**In summary:**\n- The paper introduces LLMSAN—a framework for sanitizing LLM-produced bug reports using detailed data-flow paths and multiple, modular program property checkers, combining deterministic and LLM-based prompts on code snippets for precise, hallucination-resistant bug detection. The prompting strategies are property-specific, decomposed, and largely local, making the approach both scalable and robust."
  },
  {
    "filename": "skipanalyzer_a-tool-for-static-code-analysis-with-large-language-models.pdf",
    "analysis": "Below is a structured and detailed breakdown answering your questions about the paper *\"SkipAnalyzer: A Tool for Static Code Analysis with Large Language Models\"*:\n\n---\n\n#### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point/Objective**:\n- The paper introduces *SkipAnalyzer*, a tool that uses Large Language Models (LLMs) – specifically versions of ChatGPT – to perform static code analysis tasks such as bug detection, false-positive warning removal, and bug repair *without human intervention*.\n\n**Exploration**:\n- The main exploration is whether LLMs, via prompt engineering, can match or outperform traditional static analysis and program repair tools (e.g., Infer, VulRepair) in detection and repair of two common bug types: Null Dereference and Resource Leak.\n\n**What makes it unique:**\n- It is the *first* tool to leverage LLMs (such as ChatGPT) for the complete cycle: static bug detection, false-positive filtering, and bug repair.\n- It systematically examines different prompting strategies (zero-shot, one-shot, few-shot) and LLM versions to determine optimal configurations.\n- It publicly releases its labeled dataset (manual validation and fixing).\n- Direct comparison to state-of-the-art, with evidence that LLMs can best traditional detectors and repair tools.\n---\n\n#### 2. What dataset did they use?\n\n- **Data source**: Bugs collected from 10 open-source Java projects (from GitHub, Alibaba, Microsoft, Apache, Google, Community).\n- **Bug selection**: They run the Infer static analyzer to detect two bug types: Null Dereference (222 instances) and Resource Leak (46 instances).\n- **Manual verification**: Three experienced developers reviewed every detected bug to label whether warnings were true/false positives and to create ground truth patches for true bugs.\n- **Total**: 552 verified warnings (across the two bug types), with public release of code, warnings, and patches.\n\n---\n\n#### 3. What LLM models were used?\n\n- **ChatGPT-3.5 Turbo**\n- **ChatGPT-4**\n- Both models were accessed via OpenAI API; for each model, default token limits apply (3.5 Turbo: ~4,097, GPT-4: ~8,192 tokens).\n\n---\n\n#### 4. What are the prompting strategies discussed in this paper?\n\nThree prompting strategies are explored:\n- **Zero-shot:** No examples included in the prompt. The LLM is prompted with the task instruction only.\n- **One-shot:** A single input/output example is included in the prompt along with the task instruction.\n- **Few-shot (K=3):** Three examples (with both true/false positive cases, selected randomly) are given in the prompt along with task instruction.\n- For bug repair, **only zero-shot strategy** is used, because examples are too large to fit the LLM token limit.\n\nIn addition, for all strategies, they include a *request for explanation* (\"chain-of-thought\" reasoning) to have the LLM output not just a decision but the step-by-step reasoning process.\n\n---\n\n#### 5. Where are the prompt strategies explained?\n\n- Main explanation: **Section V.C: Prompting Strategies** (search for \"Prompting Strategies\"). Here, the strategies (zero-shot, one-shot, few-shot) are defined for all components of the pipeline, with rationale for choices and discussion of chain-of-thought explanations.\n- Component-wise details: **Section IV (Approach)**, under each pipeline component (A-C), explaining if/when example inclusion is supported in the prompt.\n- Limits for prompting (tokens): Section V.B and IV.C discuss limitations on using one/few-shot approaches for the patch generation component.\n\n---\n\n#### 6. What are the results of the prompting strategies, and how successful or not were they?\n\n##### For Static Bug Detection\n\n- *Best results* are achieved with **GPT-4 and zero-shot strategy** for both Null Dereference and Resource Leak bugs in their main dataset.\n    - **Null Dereference (GPT-4, zero-shot):**\n        - Accuracy: 68.37%\n        - Precision: 63.76%\n        - Recall: 88.93%\n    - **Resource Leak (GPT-4, zero-shot):**\n        - Accuracy: 76.95%\n        - Precision: 82.73%\n        - Recall: 55.11%\n- Both notably outperform Infer (precision for Null bugs: 50.9%; Resource leak: 39.6%).\n- Other prompt/model combinations (e.g., GPT-3.5, one/few-shot) usually perform worse than GPT-4/zero-shot, but may still outperform the baseline.\n- *Precision improvement* (best over Infer): +12.86% (Null) and +43.13% (Resource Leak)\n\n##### For False-Positive Warning Removal\n\n- Best strategy: Few-shot with GPT-4 often gave the best results for removing false positives.\n    - E.g., For Null Dereference warnings by Infer, **GPT-4/few-shot** increased precision from 65.2% to 93.88% (+28.68%).\n- For Resource Leak, improvement was lower (up to +9.53%)\n\n##### For Bug Repair\n\n- Only zero-shot strategy (no examples).\n    - **GPT-4 logic rate** (i.e., correct logic): 97.3% (Null), 91.77% (Resource Leak)\n    - Syntax correctness: ~99%+\n- GPT-3.5 Turbo slightly worse; traditional baseline (VulRepair) is *much* worse (e.g., 18.39% correct repairs for Null).\n- The LLM does not require retraining/fine-tuning (as opposed to VulRepair).\n\n---\n\n#### 7. Why did one strategy give better results than another?\n\n- **Zero-shot with GPT-4 outperformed others** in most main tasks (detection, repair). Some likely reasons:\n    - **Model scale and instruction-following**: GPT-4 has stronger reasoning and understanding, so it's able to infer the required task from instructions alone.\n    - **Examples limited by token constraints**: In large code tasks, too many in-context examples in one/few-shot may eat up tokens and reduce room for analyzing the actual code.\n    - **Few-shot/one-shot need carefully curated examples**: In domains like code bug detection, unless examples are almost exact matches in code structure, few-shot gains are less pronounced. Random examples may sometimes confuse.\n    - **Zero-shot chain-of-thought**: Explicitly requesting explanations may improve output quality (as supported by literature).\n\n- **For false-positive filtering**, few-shot sometimes helps more: maybe because seeing a range of positive/negative examples lets the LLM distinguish subtle cues about warning accuracy.\n\n---\n\n#### 8. All specifics of the researcher's prompt strategies.\n\n- **For each task (component)**, the prompt includes:\n    - A system instruction for the type of bug (e.g., describe what is a Null Dereference or Resource Leak, patterns to watch out for).\n    - Task: Either \"identify whether the code contains a X bug\", \"is this warning a false positive\", or \"generate a patch for this warning\".\n    - A request to \"explain your decision\".\n    - **Zero-shot**: Only above.\n    - **One-shot/few-shot**: Prepend 1 (one-shot) or 3 (few-shot/K=3) fully formatted example(s), selected randomly, ensuring both true/false positive examples appear to avoid bias.\n    - Inputs: For detection, code snippet only. For false-positive warning removal, code snippet + warning. For bug repair, code snippet + warning.\n    - **Code scope**: To keep within token limits, analysis is performed at the method level (not whole files/classes).\n    - All outputs are structured so they are easy to parse later.\n    - For bug repair: **no one/few-shot**, only zero-shot due to token limits/variable code lengths.\n\n- **Chain-of-thought**: Always asked for step-by-step reasoning.\n\n---\n\n### Summary Table\n\n| Task                        | Prompting Strategies Used  | Model(s)           | Best Result             | Why Best?                                              |\n|-----------------------------|---------------------------|--------------------|-------------------------|--------------------------------------------------------|\n| Bug Detection               | zero, one, few-shot       | GPT-3.5, GPT-4     | GPT-4 zero-shot         | Large model, clear instructions, no token penalties    |\n| False Positive Warning      | zero, one, few-shot       | GPT-3.5, GPT-4     | GPT-4 few-shot (Null)   | Seeing positive/negative helps, GPT-4 better overall   |\n| Bug Repair                  | zero-shot ONLY            | GPT-3.5, GPT-4     | GPT-4 zero-shot         | No room for examples; GPT-4 best at \"single-turn\" tasks|\n\n---\n\n## Conclusion\n\n**SkipAnalyzer** demonstrates that carefully prompt-engineered, advanced LLMs (specifically GPT-4, zero-shot or few-shot) can outperform traditional static analysis and program repair tools, especially for Null Dereference and Resource Leak bugs in Java. Zero-shot chain-of-thought with GPT-4 is the standout configuration for detection and repair, possibly because of model reasoning ability and token constraints with large code samples. For false positive filtering, a few-shot GPT-4 approach is especially effective. The research details prompt content/output structure and the reasoning for design choices, and provides public data and code for replication.\n\nIf you need the *actual prompt templates* or further specifics, let me know!"
  },
  {
    "filename": "to-err-is-machine_vulnerability-detection-challenges-llm-reasoning.pdf",
    "analysis": "Certainly! Here’s a structured, thorough answer addressing each of your questions, making reference to passages, sections, and specifics from the provided paper.\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Main Point & Goal:**  \nThe paper investigates whether state-of-the-art Large Language Models (LLMs) can reason well about program code to detect vulnerabilities—a complex task requiring deep semantic understanding and multi-step reasoning.\n\n**What It Tries to Explore:**  \n- The ability and limits of SOTA LLMs in *detecting vulnerabilities* in source code.\n- How LLMs perform under different prompting strategies.\n- Whether improvements such as bigger models, more/fine-tuned training data, or prompt engineering can meaningfully enhance vulnerability detection.\n- Which aspects of code semantics are hardest for LLMs (e.g., bounds checks, NULL checks, pointer operations).\n\n**Uniqueness:**  \n- It systematically analyzes **14 prominent LLMs** and **7 prompting methods**.\n- It provides *fine-grained error classifications* through manual inspection of 300+ model responses, focusing on where and why LLMs fail at reasoning steps.\n- Uniquely, it tries a variety of domain-specific/multi-step prompting approaches and directly correlates those strategies to error reductions (or not).\n- The paper argues that *scaling models or data alone* doesn’t overcome specific reasoning shortcomings — suggesting the need for paradigm shifts in modeling/training, not just scale.\n\n---\n\n### 2. What dataset did they use?\n\n**Primary Dataset:**  \n- **SVEN** (He & Vechev, 2023)—a high-quality, real-world dataset with:\n  - 772 paired functions (386 vulnerable + 386 corresponding fixed versions)\n  - Drawn from real C/C++ projects\n  - 168 lines average function length\n  - High label accuracy (94%), manually vetted\n\n**Secondary datasets (for certain experiments):**\n- **Big-Vul dataset** (Fan et al., 2020) (used for obtaining CVE descriptions for prompts)\n- **D2A dataset** (Zheng et al., 2021) (used for extracting static analysis explanations)\n- **CWE and SARD** (simple function experiments)\n\n---\n\n### 3. What LLM models were used?\n\n**Fourteen SOTA LLMs:**  \n- **Open-source:**\n  - LLAMA2 (7B, 13B, 34B)\n  - CodeLLAMA (7B, 13B, 34B)\n  - Mistral (7B)\n  - Mixtral (8x7B MoE)\n  - StarCoder (15.5B), StarCoder2 (16B)\n  - StarChat (15.5B), StarChat2 (15.5B)\n  - MagiCoder (7B)\n  - WizardCoder (33B)\n  - DeepSeek-Coder (1.3B, 6.7B, 33B)\n- **Proprietary:**\n  - GPT-3.5-turbo\n  - GPT-4-turbo\n  - Gemini 1.0 Pro\n\n(See Table 4 in Appendix B for full list, parameters, context length.)\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n**Prompting strategies (Section 2 / Appendix A):**\n\n**A. Baseline approaches:**\n1. **Basic zero-shot** – a simple instruction (\"Is the following function buggy? Please answer Yes or No.\")\n2. **Q/A** – framing as a question/answer.\n3. **CWE list** – prompt includes a list of CWEs (bug types) to guide model attention.\n\n**B. In-context learning:**\n4. **Random in-context** – random selection of n-shot example pairs.\n5. **Embedding-based retrieval** – in-context examples similar in embedding space.\n6. **Contrastive pairs** – *paired* in-context examples, showing both vulnerable and patched (fixed) versions to illustrate subtle differences.\n\n**C. Chain-of-Thought (CoT) strategies:**\n7. **CoT-CVE** – With in-context examples providing step-by-step vulnerability reasoning, generated by summarizing or paraphrasing published CVE descriptions (from Big-Vul).\n8. **CoT-StaticAnalysis** – Chain-of-thought in-context examples, where the reasoning is created from static analyzer (Infer) proofs extracted from D2A dataset.\n9. **CoT-Annotations** – Custom, lightweight static analysis annotates code with likely NULL assignments, NULL checks, pointer dereferences, etc., and the prompt asks the model to reason step-by-step using those cues. (Introduced in Section 3.3.)\n\n---\n\n### 5. Where are the prompt strategies explained?\n\n**Explicit Details:**\n- **Section 2:** Prompting strategy overview with main methods, including baseline and new strategies.\n- **Appendix A:** *Vulnerability Detection Prompts*—Full templates, alternative designs, step-by-step explanations for each strategy, and sample prompts.\n- **Section 3.3:** Further details & rationale specifically for CoT-Annotations, including example annotated prompts.\n\n---\n\n### 6. What are the results of the prompting strategies? How successful or not were they?\n\n**Main Findings:**\n- **All strategies only slightly outperformed random guessing** (50% balanced accuracy) — best models/prompts achieve at most ~55%.\n- **Contrastive prompts slightly improved best-case performance** for 8/14 models (but not by much).\n- **Other CoT/domain-knowledge or in-context prompts produced small improvements**, but *none* achieved a major boost.\n- **Figure 2**: Most bars hover just above random baseline; stars indicate best result per model, but rarely exceed BA=55%.\n- **Table 1:** All models performed 50–54% on SVEN, even though they get high scores in non-reasoning code/NL tasks.\n- **CoT-Annotations**: For Null Pointer Dereference (NPD) vulnerabilities specifically, “annotations” reduced bounds/NULL-check recognition errors by 15–70% in three models, but that **still left 23–67% of checks missed**, and *overall vulnerability detection did not improve much* (see Figure 9).\n\n---\n\n### 7. Why did one strategy give better results than another?\n\n**Findings & Interpretations:**\n- **Contrastive prompts** improve performance slightly for some models because presenting paired buggy/fixed functions highlights the minimal, crucial semantic change (a subtlety that random or generic context examples may miss).\n- **CoT-Annotations** (domain knowledge) help models understand specific reasoning steps—e.g., recognizing “this is a null check” or “this pointer could be null”—but the benefit is limited:\n  - **Blocker:** Even if a model gets individual semantic facts correct, **multi-step logical integration and inference is still weak**; errors shift to logical reasoning about variable relations/policies, instead of just statement understanding.\n- **Instructional or in-context strategies** rely on the model’s ability to generalize from patterns, but if the underlying representations or pretraining lack code execution/semantic alignment, no amount of prompt manipulation helps meaningfully.\n\n---\n\n### 8. All specifics of the researchers' prompt strategies\n\nHere’s a table summarizing them (all detailed in Appendix A, with some examples in the main text):\n\n#### A. Basic Prompts\n- **Zero-shot**:  \n  - *System prompt*: \"I want you to act as a vulnerability detection system\"\n  - *NL instruction*: \"Is the following function buggy? Please answer Yes or No.\"\n  - Variants: CWE list format, Q/A format.\n\n#### B. In-context Learning\n- **Random selection (n-shot):**  \n  - Provide several (up to 6, best found empirically) random input/output pairs before the query.\n\n- **Embedding-similar in-context (RAG)**:  \n  - Use CodeBERT embeddings to select in-context examples close to query.\n\n- **Contrastive pairs:**  \n  - For a function, provide both the vulnerable and the patched version in one context (before the query function).\n  - Aim: Highlight how small changes determine vulnerability.\n\n#### C. Chain of Thought (CoT) Prompts\n- **CoT from CVE descriptions**:  \n  - Take bug report text summarizing how and why the code is buggy (e.g., \"The setkey function in crypto/skcipher.c lacks a key-size check, which allows a NULL pointer dereference...\"), paraphrase as step-by-step reasoning, conclude with “Therefore, the example is buggy.”\n  - Use as in-context example response.\n\n- **CoT from Static Analysis**:  \n  - Run static analyzer (Infer); obtain buggy “path” (key statements leading to bug).\n  - Paraphrase those into step-wise NL reasoning (e.g., List lines where buffer is allocated, where index is set, where access occurs).\n  - Use as in-context example.\n\n#### D. Domain Knowledge/Annotation–based CoT (introduced in Section 3.3)\n- **CoT-Annotations**:\n  - Use lightweight, custom static analysis to annotate code in-prompt with locations of NULL assignments, NULL checks, dereferences.\n  - Include detailed instructions guiding the LLM through precise reasoning (\"Find likely null pointers, check where they're dereferenced, filter out protected dereferences, decide if any are vulnerable\").\n  - Specific step-by-step instructions often like:\n    1. Identify where pointers could be NULL.\n    2. Identify dereferences.\n    3. Filter out checked dereferences.\n    4. If any unprotected dereference remains, mark function as vulnerable.\n\n**Empirical adjustments:**\n- Varied number of in-context examples (1–10), found 6 is optimal.\n- Examined different formats for example inclusion (single vs. multiple messages).\n- For non-vulnerable examples, maintain default (not CoT) responses.\n\n**Quality Filtering:**\n- In all in-context/COT datasets, removed duplicates, very long/short code, or noisy labels.\n\n**Notable distinctions:**\n- **CoT-Annotations** (custom lightweight static analysis) vs. **CoT-StaticAnalysis** (heavy, off-the-shelf Infer output):\n  - The former is targeted to what LLMs have most trouble with (bounds/NULL checks).\n\n**Prompt examples provided in Appendix A, Section 3.3, and Figure 8.**\n\n---\n\nIf you would like sample prompts or a tabular breakdown, or if you need a very short summary, let me know!"
  },
  {
    "filename": "exploring-chatgpts-capabilities-on-vulnerability-management.pdf",
    "analysis": "Certainly! Here are detailed answers to each of your specific questions, thoroughly referencing the content of the paper *\"Exploring ChatGPT’s Capabilities on Vulnerability Management\"*.\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n#### Point of the Paper & What It Explores\n\nThe paper conducts **the first large-scale, systematic evaluation of ChatGPT's performance on a comprehensive suite of vulnerability management tasks**. Specifically, it investigates:\n\n- **Can ChatGPT assist software maintainers across diverse tasks in the full vulnerability management process**, which includes not only code analysis, but also complex tasks like bug report summarization, security relevance prediction, patch correctness assessment, etc.?\n- **How do various prompt engineering strategies affect ChatGPT's performance on these tasks?**\n- **What are the current difficulties and bottlenecks of using ChatGPT for these tasks, and what promising future directions can be explored to improve ChatGPT’s efficacy?**\n\n#### What Makes It Unique\n\n- **Broad scope:** Unlike prior work that only focused on narrow tasks (such as vulnerability fixing), this work evaluates ChatGPT across 6 distinct tasks that span the *entire vulnerability management lifecycle* (see Figure 1, Table 1).\n- **Scale:** It uses a very large dataset (over 70,000 samples, 19 million+ tokens).\n- **Prompting analysis:** It systematically compares a diverse set of prompt engineering strategies, documenting their effects on different tasks.\n- **Self-heuristic prompting:** It introduces and evaluates a new, *self-heuristic* prompt strategy—where ChatGPT is leveraged to extract and compress expertise from the data and then use that extracted expertise in further prompting.\n- **Direct SOTA Comparison:** Compares ChatGPT (zero/few-shot) to 11 State-of-the-Art domain-specific methods on their own datasets.\n- **Human Study:** It includes a user study (with professional/intermediate developers) for qualitative evaluation.\n\n---\n\n### 2. What dataset did they use?\n\n**Huge, Multi-Task Dataset:**  \n- **Total size:** 70,346 samples (~19.4 million tokens)\n- **Composition:** Datasets from SOTA baselines for each of the 6 tasks (see Table 1), as follows:\n  - **Bug report summarization:** 33,438 samples\n  - **Security bug report identification:** 22,970 samples\n  - **Vulnerability severity evaluation:** 1,642 samples\n  - **Vulnerability repair:** 12 hand-crafted + 10,601 from LLMset (total small due to task difficulty)\n  - **Patch correctness assessment:** 995, 139, & 208 samples across 3 SOTA datasets\n  - **Stable patch classification:** 10,896 samples\n\n**Source:** For each task, they used the publicly available datasets from prior SOTA approaches, ensuring direct comparability.\n\n---\n\n### 3. What LLM models were used?\n\n- **ChatGPT based on gpt-3.5-turbo-0301:**  \n  - Used for initial template selection phases (because of cost).\n- **ChatGPT based on gpt-4-0314:**  \n  - Used for large-scale testing and best-performance evaluations.\n- **Comparison LLMs (for code repair):**  \n  - Codex and AI21’s Jurassic-1 (as studied in LLMset [37]).\n- **Baselines:** 11 SOTA (mostly deep learning or traditional ML) models specific to each task.\n\n---\n\n### 4. What prompting strategies are discussed in this paper?\n\n**Prompt Strategies Evaluated (see Table 2):**\n1. **0-shot:**  \n   Only a task description and input; no examples.\n2. **1-shot:**  \n   Task description + one demonstration example + input.\n3. **Few-shot:**  \n   Task description + multiple random demonstration examples + input.\n4. **General-info:**  \n   Integrates system-level role instructions, task confirmation dialogue, reinforcement, and positive feedback into prompt.\n5. **Expertise:**  \n   Like general-info + *hand-crafted domain expertise* (summarized from docs/literature—Table 14).\n6. **Self-heuristic:**  \n   Like expertise, but *the expertise is generated by ChatGPT* itself, summarizing knowledge from a set of demonstration examples.\n\n**Special cases:**  \n- For some tasks, extra templates such as code-only or desc-code were designed (e.g., for patch correctness assessment).\n\n---\n\n### 5. Where are the prompting strategies explained?\n\n- **Table 2 (Prompt Templates):** Lists and documents each strategy.\n- **Section 3.3 (“Prompt Design and Implementation”):** Offers methodology and discussion on how each prompt strategy is constructed and their rationale.\n- **Appendix (Table 13):** Provides concrete prompt examples for each strategy across different tasks.\n- **Table 14 (Appendix):** Lists the expertise content (for expertise & self-heuristic prompts) for each task and their sources.\n\n---\n\n### 6. What are the results of the prompting strategies and how successful were they?\n\n**Key Results by Strategy:**\n\n- **0-shot:**  \n   - Suffices for “language-centric” tasks (e.g., bug report summarization): ChatGPT outperformed SOTA using just zero-shot.\n   - Poor for logic-heavy code tasks (vulnerability repair, severity), often yields errors or hallucinations.\n- **1-shot/few-shot:**  \n   - Improves over 0-shot for tasks that benefit from examples (e.g., security bug ID), but not a universal win.\n   - May induce \"anchoring\" on irrelevant demonstration details (can hurt precision).\n- **General-info and Expertise:**  \n   - Adding role-play, clarifications, and manually summarized expertise helps significantly, especially for classification or more complex decision tasks.\n   - However, can sometimes *hurt* in language-centric tasks already well-covered by LLM pretraining.\n- **Self-heuristic:**  \n   - Especially beneficial for hard, ambiguous, or data-starved tasks (e.g., vulnerability severity evaluation).\n   - For instance, precision and recall in vulnerability severity task improved dramatically when using expertise *summarized by ChatGPT itself* from demonstration data.\n\n**Overall Success:**\n- **For some tasks, ChatGPT (with suitable prompting, esp. expertise/self-heuristic) surpassed or matched SOTA (e.g., bug report summarization, patch correctness, vulnerability repair).**\n- **In other, more domain-specific tasks, it still lagged behind SOTA unless guided by domain expertise via the prompt.**\n- **Human study confirmed ChatGPT’s superiority in correctness and readability for summarization tasks.**\n\n---\n\n### 7. Why did one strategy give better results than another?\n\n- **Language-centric Tasks:**  \n   - 0-shot worked well due to ChatGPT’s massive pretraining on generic language tasks (e.g., summarization).\n   - Adding complexity in the prompt could introduce confusion or unnecessary details—less is more.\n- **Logic- or Domain-centric Tasks:**  \n   - Plain 0-shot fails: ChatGPT needs explicit domain structure, rules, or patterns, as it cannot infer complex task mappings from generic language alone.\n   - **Expertise/self-heuristic** prompts work better because they encode the rules or summary knowledge explicitly, removing ambiguity and helping ChatGPT “anchor” its reasoning in the specific domain logic.\n   - Self-heuristic strategy leverages ChatGPT’s strength in abstraction/compression—distilling patterns from many examples, then using that distillation as factual guidance.\n- **Too much or irrelevant info (e.g., ill-chosen demonstration examples) can hurt**: ChatGPT can overfit to superficial correlations in demonstration examples, leading to biased predictions (“anchoring” problem).\n\n---\n\n### 8. All specifics of the researcher's prompt strategies.\n\n**Prompts (see Table 2, Table 12, Table 13, Table 14, and main text):**\n\n- **0-shot:**  \n   - The prompt gives only a task description and the input sample.\n   - Example: “USER: Decide whether a bug report is a security bug report (SBR) or non-security bug report (NBR). Bug report: <bugreport> Category:”\n- **1-shot:**  \n   - Adds a single, randomly selected example (with correct output) before the test input.\n   - Example: “... Bug report: Memory Leak in... Category: SBR ### Bug report: <test_bugreport> Category:”\n- **Few-shot:**  \n   - Adds multiple randomly selected demonstration examples (typically 4), formatted as above.\n- **General-info:**  \n   - Uses the “SYSTEM” role to assign ChatGPT a relevant persona (e.g., “You are Frederick, an AI expert in bug report analysis...”).\n   - Reinforces instructions and simulates a dialog (task confirmation, positive feedback).\n   - May include step-by-step (Zero-CoT) logic prompts: “Think step by step...”\n- **Expertise:**  \n   - Like general-info, but adds domain-specific knowledge (from docs/literature).\n   - Example: “When analyzing the bug report, take into account that bug reports related to memory leak or null pointer problems should be seen as security bug reports.”\n- **Self-heuristic:**  \n   - Provides demonstration samples to ChatGPT, asks it to summarize the generic characteristics/rules (e.g., what defines AV:Network for a vulnerability).\n   - Those ChatGPT-generated summaries are then fed as the 'expertise' portion in the final prompt for inference.\n\n**Additional strategies (for task-specific needs):**\n- **Desc-code/code-only:**  \n   - For tasks where only code or code+description is needed (e.g., patch correctness), special templates are used to prevent ChatGPT from misusing context.\n- **Prompt Implementation Details:**  \n   - All template writing, demonstration example selection, instruction tuning were done on training/probe data only; test data never accessed for prompt engineering.\n   - Probe-test datasets (10% random training samples, up to 1,000 for costly tasks) are used for template selection before final testing on the full test set.\n- **Appendix Tables:**  \n   - Table 12 gives prompt skills for general-info (role, reinforce, confirmation, positive feedback, CoT, etc.)\n   - Table 14 specifies task-relevant expertise (taken from Linux kernel docs, CVSS guidelines, prior SOTA papers).\n   - Table 13 shows example prompts per task and per strategy.\n\n---\n\n## In summary\n\n- **The paper systematically explores ChatGPT’s effectiveness across 6 core vulnerability management tasks,** using a large, multi-task dataset, SOTA model comparisons, and multiple prompt strategies.\n- **Prompt strategies—including 0-shot, 1/4-shot, general-info, expertise, and self-heuristic—are clearly explained, systematically evaluated, and compared.**\n- **Advanced prompt strategies (especially expertise and self-heuristic) are often key to unlocking strong performance on logic-heavy or domain-specific tasks.**\n- **Prompt design specifics, examples, and empirical results (by task and strategy) are thoroughly documented in the main text and appendix.**\n\nIf you want concrete example prompts or implementation details for a specific task or prompting strategy, let me know!"
  },
  {
    "filename": "effective-vulnerable-function-identification-based-on-cve-description-empowered-by-large-language-models.pdf",
    "analysis": "Here is a detailed breakdown of the paper, following your questions:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Objective:**  \nThe paper proposes VFFinder, a new approach to accurately identify *vulnerable functions* (VFs)—the actual functions in open source projects where a known security vulnerability resides. This is crucial for downstream users of OSS, as existing vulnerability databases (like NVD) rarely specify the exact vulnerable function, and most SCA tools only work at the package or version level, producing lots of false positives.\n\n**What it explores:**\n- The effectiveness of using only the *CVE vulnerability description* (widely available for every disclosed vulnerability) to localize VFs, instead of needing vulnerability patches or Proof-of-Concepts, both of which are often unavailable.\n- How *Large Language Models (LLMs)*, with in-context learning, can extract indicator entities (attack vector and root cause) from natural language descriptions to build more accurate function-level queries for localization.\n\n**What makes it unique:**\n- First to use LLMs (with in-context learning and careful prompt engineering/case selection) for VF identification from *just* vulnerability descriptions and (source) code.\n- Avoids the need for patches and PoCs, thus making the approach widely applicable to all vulnerabilities (all CVEs).\n- Designs prompting strategies (demonstration selection, pattern embedding) to maximize LLM extraction performance.\n- Demonstrates practical integration with SCA tools, showing dramatic false positive reductions.\n\n---\n\n### 2. What dataset did they use?\n\nThey constructed 3 main datasets:\n- **A&R Dataset:**  \n  Hand-labeled attack vector and root cause for 642 CVEs (from Java projects), used to train/test their LLM extractor's effectiveness.\n- **Vulnerable Function Dataset (VF dataset):**  \n  77 CVEs across 75 open-source projects, for each the authors (via manual effort, prior work, and exploiting PoCs) established the true vulnerable function. Used as the main testbed for comparing methods and ablation studies.\n- **Cross-project Vulnerability Dataset:**  \n  For impact experiments: 62 CVEs, 61 vulnerable OSS libraries, and 9,829 downstream applications to validate whether VF localization improves SCA false positive reduction.\n\nThe datasets and tool are open-sourced:  \nhttps://github.com/CGCL-codes/VFFinder\n\n---\n\n### 3. What LLM models were used?\n\n**Primary LLM:**  \n- **OpenAI's ChatGPT: gpt-3.5-turbo-0613**  \n  Used via API to perform extraction tasks (attack vector & root cause extraction) with various prompting strategies.\n\n**Semantic Similarity Embedding:**  \n- **angle-llama-7b-nli-20231027** – A sentence embedding model (Llama 2 7b variant) for semantic similarity tasks (mainly used for ranking functions and for one demonstration retrieval baseline).\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThere are several, each designed to improve extraction of indicator entities from textual CVE descriptions:\n\n#### A. **Basic Prompt** (“Direct” Prompt)\n- Task description, a single example, then the test CVE description.\n- The LLM is told: \"Extract attack vector and root cause from the following...\"\n\n#### B. **ICL-enhanced Prompt (In-Context Learning)**\n- The LLM is provided with *multiple* example cases before the test description, leveraging \"in-context learning.\"\n- These demonstration cases are chosen to be structurally similar to the input description.\n\n**There are two main demonstration selection strategies within ICL:**  \n    - **S-ICL (Semantic ICL):** Picks examples via semantic similarity (using angle-llama-7b-nli).\n    - **P-ICL (Pattern ICL):** Picks via *pattern similarity*, a custom method:  \n        - Abstract important entities (vendor, product, version info, vuln type, component, attacker)  \n        - Parse and represent sentences as grammatical patterns  \n        - Use an autoencoder to embed these patterns, then select the most similar patterns for demonstrations in the prompt.\n\n#### C. **Comparison with Unsupervised Extraction (CaVAE):**\n- Baseline method (from prior work) using rule-based/syntax-based extraction; included as a non-LLM baseline.\n\n---\n\n### 5. Where are the prompt strategies explained?\n\nThey are described in Section 3.3 (Motivating Examples), Section 4.1 (ICL-Enhanced Extractor), and Figure 5 (Prompt Format).  \n- **Prompt format:** See Figure 5 in the text for an example.  \n- **Demonstration selection methods and embedding method:** Figure 4 and Section 4.1.2.\n\nTable 1 provides empirical results of the different prompting strategies.\n\n---\n\n### 6. What are the results of the prompting strategies and how successful were they?\n\n**Results (see Table 1):** For attack vector (AV) and root cause (RC) extraction from CVE descriptions:\n\n- **CaVAE** (unsupervised):  \n  - Coverage: 0.07 (both AV, RC)\n  - JSI (Jaccard Similarity Index): 0.04\n\n- **Direct (basic prompt):**  \n  - Coverage: AV 0.47, RC 0.35\n  - JSI: 0.35\n\n- **S-ICL (Semantic ICL):**  \n  - Coverage: AV 0.58, RC 0.55\n  - JSI: 0.54\n\n- **P-ICL (Pattern ICL):**  \n  - Coverage: AV 0.60, RC 0.58\n  - JSI: 0.58\n\n**Interpretation:**  \n- Moving from CaVAE to LLM with no demonstrations nearly increases coverage and JSI an order of magnitude.\n- ICL (multiple demonstrations, similar cases) gives a further strong boost.\n- Pattern-based demonstration selection gives a marginal but consistent improvement over semantic similarity selection.\n\nThe P-ICL strategy covered 60%+ of the indicator entities and provided concise extraction (shorter output), meaning LLMs could extract the most useful core details, and pattern-based case prompting works the best.\n\n---\n\n### 7. Why did one strategy give better results than another?\n\nThe improvements follow this chain:\n- **Direct/Baseline vs. CaVAE:** LLMs are much more flexible and robust than unsupervised rule-based extraction, especially for messy/unstructured text.\n- **ICL vs. basic prompt:** Providing *multiple, highly similar* demonstration examples (\"in-context learning\") helps the model learn the subtle structure and boundaries of entity extraction, reducing noise and redundancy.\n- **Pattern vs. Semantic similarity:** While semantic similarity of cases to the input does help (S-ICL), *pattern* similarity (grammar/structure of the CVE description) matters more for LLM extraction. The model can better align its extraction process to the expected structure if shown demonstrations with similar linguistic/structural features.\n- **Empirical results:** Pattern ICL (P-ICL) consistently provided the highest entity coverage and precision.\n\n---\n\n### 8. All specifics of the researcher's prompt strategies.\n\n**Summarized specifics from the paper:**\n\n#### 1) **Case Pool Construction:**\n- Pool of 642 manually-annotated CVE descriptions (attack vector & root cause) for Java projects.\n- Two annotators, disagreements resolved via discussion or dropped.\n\n#### 2) **Demonstration Retrieval:**\n- Each test CVE description is mapped to the \"most similar\" (K=3) cases from the pool to be used as demonstrations in the prompt.\n- Similarity method options:\n    - Semantic similarity (angle-llama-7b-nli embeddings)\n    - Pattern similarity: extract and replace entities (VPV, vuln type, component, etc) with type markers; construct a parse tree, prune to tokens; embed the token sequence with autoencoder; use cosine similarity between embeddings for ranking.\n\n#### 3) **Prompt Structure (see Figure 5 in paper):**\n- **Instruction:**  \n  - “I want you to act like an expert in vulnerability report analysis. I will give you a description. Your task is to extract the attack vector and root cause from it. Attack vector is the method of triggering the vulnerability. ... if you cannot find the corresponding answer, just answer None.”\n- **Demonstration cases:**  \n  - Three examples (each with a description and its correct extraction).\n- **Query:**  \n  - The actual CVE description to be analyzed.\n\n- Temperature set to 0 for repeatability.\n\n#### 4) **Extraction Level:**\n- Extraction is done *per sentence* of the description.\n\n#### 5) **Preprocessing:**\n- Remove stopwords, split compound and camel case words, apply stemming.\n- Special-case extraction for class names in the description.\n\n#### 6) **Negative Case Handling:**\n- If no suitable attack vector/root cause found, the LLM is instructed to answer “None.”\n\n#### 7) **LLM engine:**  \n- ChatGPT gpt-3.5-turbo-0613 for all intention extraction tasks.\n\n#### 8) **Integration into Retrieval:**\n- Weighted similar matching between extracted entities (indicator entities) and function code entities (see also Algorithm 1), using TF-IDF weighting and scoring by semantic similarity.\n\n#### 9) **Ablation study:**\n- Removing this “ICL-enhanced extractor” reduces localization performance by 10.8% on average (substantial impact, see Figure 7 in paper).\n\n---\n\n**Summary Table for Prompting Strategies**\n\n| Name     | Demonstration Cases | Case Selection           | Prompt Structure    | Main Finding                   |\n|----------|--------------------|--------------------------|---------------------|-------------------------------|\n| CaVAE    | None (not LLM)     | N/A                      | N/A                 | Very poor extraction           |\n| Direct   | 1 (example)        | Static, same for all     | Instruction+1 ex    | LLM does OK, better than CaVAE |\n| S-ICL    | 3                  | Semantic similarity      | Instr+3 ex+query    | Significant boost              |\n| P-ICL    | 3                  | Pattern similarity       | Instr+3 ex+query    | Best overall performance       |\n\n---\n\n**In conclusion:**  \nThe authors created a multi-step, carefully-structured prompt:  \n1. Given a new CVE description, find 3 *pattern-similar* cases from the hand-annotated pool.  \n2. Use these, with the instruction and the new description, as a prompt to the LLM.  \n3. Instruct the LLM to extract the “attack vector” and “root cause.”  \n4. Postprocess the output, match (using semantic similarity and TF-IDF) to function-level code entities for localization.\n\nThis ICL pattern-similarity-enhanced prompting approach enabled much greater extraction quality compared to naive prompting.\n\n---\n\nIf you want any of these answers expanded (e.g., details about semantic retrieval/ranking/scoring after entity extraction), just let me know!"
  }
]