### 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point/Goal:**  
The paper investigates the effectiveness of ChatGPT (specifically GPT-3.5 Turbo) in detecting code smells in Java projects. Code smells are patterns in software code that may indicate deeper problems and opportunities for refactoring, such as “Blob”, “Data Class”, “Feature Envy”, and “Long Method”.  

**What is it trying to explore?**  
- Whether ChatGPT can autonomously detect code smells in Java, both with generic (unspecified) and detailed (specifying the target smells) prompts.
- How the precision, recall, and F-measure of ChatGPT change with different prompt strategies.
- The impact of severity levels of code smells (Minor, Major, Critical) on ChatGPT’s ability.

**What makes it unique:**  
- It is one of the first works to evaluate LLMs for code smell detection—a key software improvement activity—rather than code generation, bug-finding, etc.
- It systematically compares prompt engineering strategies (generic vs. specific).
- The study uses a real-world, industry-relevant dataset (MLCQ).
- Provides statistically rigorous evaluation (McNemar’s test).
- Makes their experiment fully replicable (open Zenodo link).

---

### 2. What dataset did they use?

They used the **MLCQ dataset** [9], which contains 14,739 annotated instances of four code smells (Blob, Data Class, Feature Envy, and Long Method), each classified into four severity levels (None, Minor, Major, Critical). For this study:
- They discarded the “None” severity level (used as negative samples), focusing on 3,291 instances (Minor, Major, Critical).
- Counts: 974 Blob, 1,057 Data Class, 454 Feature Envy, and 806 Long Method across severity levels.

---

### 3. What LLM models were used?

They used **ChatGPT version 3.5-Turbo** (GPT-3.5).
- The paper only refers to this version; no other LLMs are experimentally evaluated.

---

### 4. What are the prompting strategies discussed in this paper?

**Two prompting strategies were used:**
- **Prompt #1 ("Generic Prompt")**: Asks ChatGPT to detect any code smells in the provided Java code, without mentioning which kinds of code smells to look for.
- **Prompt #2 ("Specific/Detailed Prompt")**: Lists the four code smells of interest (Blob, Data Class, Feature Envy, Long Method) and asks ChatGPT to identify any of them in the provided code.

---

### 5. Where are the prompt strategies explained?

- **Section 2.2 ("Prompts Definition")** describes both prompts and the rationale behind them.
- Figures 1 and 2 in the paper show the full text of each prompt as used in the experiments.

---

### 6. What are the results of the prompting strategies and how successful or not were they?

**Prompt #1 (Generic):**
- Generally poor performance. 
- For example, for Data Class: F-measure = 0.11; for Long Method: F-measure = 0.41.
- For Blob, ChatGPT did not identify any correctly (F-measure cannot be computed).

**Prompt #2 (Specific):**
- Markedly better for some smells, especially Data Class (F-measure = 0.59) and moderate improvements for others.
- For instance, Data Class F-measure improved by 0.48 (to 0.59).
  
**Statistical comparison:**
- **McNemar’s test** showed that using the specific prompt (Prompt #2), ChatGPT was 2.54 times more likely to provide a correct outcome (Odds Ratio = 2.54)
- The Absolute Risk Difference was 16%—Prompt #2 yields correct results 16% more often overall.

**Effect of severity:**  
- ChatGPT performed best at detecting code smells of **Critical** severity (e.g., F1 = 0.52 with Prompt #2), less so for Minor (F1 = 0.43).
- For Data Class at Critical severity, F-measure = 0.67 with Prompt #2.

---

### 7. Why did one strategy give better results than another?

- **Prompt #2** was more successful because it provided explicit task description—indicating exactly which code smells to look for—helping the LLM to better focus and understand the task at hand.
- Prompt #1 was too generic, so ChatGPT lacked guidance and often either missed subtle smells or failed to generalize.
- ChatGPT performed especially better at identifying code smells that have clear and easily detectable patterns (such as Data Class) when properly instructed.
- The specificity in Prompt #2 likely helped the model use its training knowledge more directly for the relevant patterns, increasing both precision and recall.

---

### 8. All specifics of the researcher's prompt strategies.

**Prompt #1 (Generic Prompt):**  
- Asks: “I need to check if the Java code below contains code smells (aka bad smells). Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.”
- Instructions: Start with “YES, I found bad smells” or “NO, I did not find any bad smell.”
- For positive findings, must answer: “the bad smells are:” and then a comma-separated list of smells detected such as “1. Long method, 2. Feature envy...”.

**Prompt #2 (Specific/Detailed Prompt):**  
- Lists the four code smells of interest at the start of the prompt:
    - *Blob
    - *DataClass
    - *FeatureEnvy
    - *LongMethod
- Then asks: “Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.”
- Same answer formatting requirements as Prompt #1.

Both prompts were:
- Carefully refined through pilot tests to avoid ambiguity and ensure the model gave answers in a machine-parseable format.
- Designed so that only the level of detail and explicitness about the targeted smells varied (not the overall complexity or length).

**Further Details:**
- Both prompts included the actual Java source code under analysis.
- For both, if the model deviated from the expected output format in 27 cases, manual inspection was done.
- Prompts that exceeded the GPT-3.5 token limit or for which code was unavailable were excluded from the study.

---

## Summary Table

| Question         | Answer (Concise Reference)                                               |
|------------------|--------------------------------------------------------------------------|
| 1. Point/Unique  | Tests ChatGPT’s ability to find code smells; fills a research gap with LLMs for code quality, not just generation |
| 2. Dataset       | MLCQ dataset: 14,739 Java instances, filtered to 3,291 (Minor, Major, Critical) cases of Blob, Data Class, Feature Envy, Long Method |
| 3. LLM models    | ChatGPT 3.5-Turbo                                                        |
| 4. Prompting     | Prompt #1 (generic, unspecific); Prompt #2 (detailed, specifies 4 smells) |
| 5. Where         | Section 2.2, Figures 1 and 2                                             |
| 6. Success       | Prompt #2 much more effective (Odds Ratio=2.54); especially for Data Class, and Critical severity |
| 7. Why better    | Specificity focuses model attention, reduces ambiguity, leverages targeted model knowledge |
| 8. Prompt detail | Both prompts require listing found smells in a strict format; only difference is the (lack of) explicit listing of target smells. Prompts include full Java code. |

---

If you need a quick digest, a diagram, or analysis for a particular code smell or severity level, let me know!
