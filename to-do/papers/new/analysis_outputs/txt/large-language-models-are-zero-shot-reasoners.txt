**1. What is the point of this paper? What is it trying to explore? What makes it unique?**

**Point & Exploration:**  
The main aim of the paper is to show that large language models (LLMs) can perform strong *zero-shot reasoning* by simply prompting them with a phrase like "Let's think step by step," without the need for any task-specific demonstration examples (few-shot learning) or extensive fine-tuning. Specifically, the paper explores whether complex, multi-step reasoning (“system-2 tasks” like math word problems and symbolic logic) is possible in a *zero-shot* setting, simply by using a well-chosen, generic prompt.

**Unique Contribution:**  
- The paper introduces **Zero-shot-CoT (Chain-of-Thought)** prompting: a form of zero-shot prompt that elicits *stepwise* (chain-of-thought) reasoning from LLMs via a simple generic phrase.
- Prior work (notably Wei et al., 2022) showed that multi-step reasoning required *few-shot* chain-of-thought examples. This paper is the first to show that **no examples are needed**—a single “Let’s think step-by-step” can give large boosts in performance across a variety of reasoning tasks.
- The method is **task-agnostic**—the same prompt works across arithmetic, symbolic, logical, and even some commonsense reasoning tasks.

---

**2. What dataset did they use?**

They evaluated their approach on 12 datasets covering four main categories:

- **Arithmetic Reasoning**:
    - SingleEq
    - AddSub
    - MultiArith
    - AQUA-RAT
    - GSM8K
    - SVAMP

- **Commonsense Reasoning**:
    - CommonsenseQA
    - StrategyQA

- **Symbolic Reasoning**:
    - LastLetter Concatenation *(created by the authors)*
    - CoinFlip *(created by the authors)*

- **Other Logical Reasoning (from BIG-bench):**
    - DateUnderstanding
    - TrackingShuffledObjects

For details on dataset size, answer format, and licensing, see Table 7 in Appendix A.2.1.

---

**3. What LLM models were used?**

A wide range of LLMs were evaluated. Main experiments were on:
- **OpenAI InstructGPT-3**: text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002
- **OpenAI GPT-3**: ada, babbage, curie, davinci (175B)
- **PaLM**: 8B, 62B, 540B parameters

Other models (for scaling studies):
- **GPT-2 (1.5B)**
- **GPT-Neo (2.7B)**
- **GPT-J (6B)**
- **T0 (11B)**
- **OPT (13B)**

See Table 8 (Appendix A.3) for details.

---

**4. What are the prompting strategies discussed in this paper?**

Main strategies:
- **Zero-shot prompt (baseline):** Plain task instruction or question (no examples), with a direct answer extraction phrase e.g. “The answer is ...”.
- **Zero-shot-CoT (proposed):** Add an explicit chain-of-thought trigger like “Let’s think step by step.” before the answer.
- **Few-shot prompt:** Show the model a handful of input-output examples.
- **Few-shot-CoT:** Show the model a handful of input + step-by-step explanation + answer examples (from Wei et al., 2022).
- **Self-consistency (PaLM):** Generate multiple reasoning paths by sampling, then answer by majority voting.

**Variants of the chain-of-thought trigger were also studied**:
- “Let’s think about this logically.”
- “Let’s solve this problem by splitting it into steps.”
- “Let’s be realistic and think step by step.” etc. (See Table 4).

---

**5. Where are the prompt strategies explained?**

- **Abstract and Section 1 (Introduction):** Motivates methods, presents Figure 1 (different prompt types).
- **Section 3 (Zero-shot Chain of Thought):** Detailed description of Zero-shot-CoT and the two-stage prompting pipeline (see Figure 2).
- **Appendix A.5:** Lists all answer extraction prompts for each dataset.
- **Table 4:** Lists all tested chain-of-thought triggers/templates.

---

**6. What are the results of the prompting strategies and how successful or not were they?**

- **Zero-shot-CoT** led to massive improvements over standard Zero-shot prompting. For example:
    - On MultiArith: **17.7% → 78.7%**
    - On GSM8K: **10.4% → 40.7%**
    - On PaLM 540B model, similar improvements were reported (12.5% → 43% for GSM8K, Table 2).
- **On tasks not requiring multi-step reasoning** (e.g., SingleEq, AddSub), gains were smaller.
- **On some commonsense tasks**, gains were less pronounced.
- **Few-shot-CoT** generally still outperformed Zero-shot-CoT, but Zero-shot-CoT outperformed simple Few-shot and Zero-shot approaches everywhere.

---

**7. Why did one strategy give better results than another?**

- **Main finding:** Explicitly prompting the LLM to “think step by step” elicits more organized, logical, and multi-step reasoning, which is critical for challenging reasoning tasks (system-2). Plain zero-shot prompts often generated only a one-step answer.
- **Prompt quality matters:** The phrasing must *encourage stepwise reasoning*; misleading or irrelevant prompts did not help (see Table 4).
- **Model size matters:** Chain-of-thought type prompting is only effective with sufficiently large models; smaller models did not improve with these prompts (see Figure 3/Table 26).
- **Few-shot-CoT is effective** because it gives the model explicit format for reasoning *with task-specific examples*; but a generic zero-shot-CoT prompt is more versatile and much less labor-intensive.

---

**8. All specifics of the researcher's prompt strategies.**

**Zero-shot-CoT Prompting Pipeline (Section 3, Figure 2):**
- **Two stages:**
    1. **Reasoning Extraction:** Input question is converted into the prompt:  
       `"Q: [question] A: Let's think step by step."`  
       Model generates a reasoning path.
    2. **Answer Extraction:** Take the generated reasoning, append a specific answer extraction phrase (depending on answer type), e.g.:  
        - `"Therefore, the answer (arabic numerals) is"` (for numbers)
        - `"Therefore, among A through E, the answer is"` (for MCQ)
    LM then generates or completes the final answer.

**Prompt template variants (robustness tested in Table 4):**
- Best-performing: “Let's think step by step” (78.7% accuracy on MultiArith, Table 4).
- Others: “Let’s solve this problem by splitting it into steps,” “Let’s think about this logically,” “Let’s be realistic and think step by step,” etc.
- Misleading/irrelevant: (“Don’t think. Just feel.”) led to poor performance.

**Answer extraction triggers (Appendix A.5):**
- Vary by task type. For example:
    - Math: “Therefore, the answer (arabic numerals) is”
    - Multiple-choice: “Therefore, among A through E, the answer is”
    - Yes/No: “Therefore, the answer (Yes or No) is”
- **“Answer cleansing”** rules (Appendix A.6): For post-processing the model output to extract the final answer robustly.

**Few-shot-CoT (baseline):**
- 2–8 in-context examples, each with step-by-step reasoning and answer in the correct format.
- “Zero-plus-few-shot-CoT”: Combine in-context examples with the additional “Let’s think step by step” phrase.

**Prompt sensitivity:**
- If the formatting of the answer extraction phrase or the type of reasoning encouraged is off, performance degrades significantly.
- If the demonstration examples (in few-shot-CoT) are from a different *task* but have similar *answer formats*, performance is retained, but lower than task-matched examples (Table 5).

---

**Summary Table: Key Prompt Strategies**

| Prompting Strategy     | Example Prompt Structure                                          | Needs Task Examples? | Template Task-Specific? | Performance on Reasoning  |
|----------------------- |-------------------------------------------------------------------|----------------------|------------------------|---------------------------|
| Zero-shot             | Q: [question] A: The answer is...                                 | No                   | Somewhat               | Low (esp. on multi-step)  |
| Zero-shot-CoT         | Q: [question] A: Let's think step by step. [model reasons...]     | No                   | No                     | High (for system-2)       |
| Few-shot              | [Ex: Q/A pairs shown, no reasoning] + Q: [question] A:            | Yes                  | Yes                    | Better than zero-shot     |
| Few-shot-CoT          | [Ex: Q, stepwise-reasoning, A shown] + Q: [question] A:           | Yes                  | Yes                    | Highest                   |

---

**In summary:**
- The main innovation is showing that a generic, explicit "think step by step" prompt can extract strong reasoning from LLMs without the need for in-context samples or training, making rigorous multi-step reasoning possible as a *true* zero-shot ability. The paper shows systematic, cross-task, and cross-model gains (as models get larger), presents comprehensive robustness and error analysis, and positions Zero-shot-CoT as a new, strong baseline for zero-shot reasoning on complex tasks.
