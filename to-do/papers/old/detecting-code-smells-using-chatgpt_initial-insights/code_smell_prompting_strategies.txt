Prompt Engineering Strategies in 'Detecting Code Smells using ChatGPT: Initial Insights'

Location: Section 2.2 Prompts Definition (Figures 1 & 2) 

Prompt #1 (Generic Prompt):
---------------------------
I need to check if the Java code below contains code smells (aka bad smells). Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.
Please start your answer with “YES I found bad smells” when you find any bad smell. Otherwise, start your answer with “NO, I did not find any bad smell”.
When you start to list the detected bad smells, always put in your answer “the bad smells are:” amongst the text your answer and always separate it in this format:
1. Long method, 2. Feature envy...

[Java source code]

Details:
- Generic guidance without enumerating specific smells.
- Explicit output format for reliable parsing.
- Iteratively refined via pilot tests to ensure clarity and consistency. 

Prompt #2 (Detailed Prompt):
----------------------------
The list below presents common code smells (aka bad smells). I need to check if the Java code provided at the end of the input contains at least one of them.
* Blob
* Data Class
* Feature Envy
* Long Method
Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.
Please start your answer with “YES I found bad smells” when you find any bad smell. Otherwise, start your answer with “NO, I did not find any bad smell”.
When you start to list the detected bad smells, always put in your answer “the bad smells are:” amongst the text your answer and always separate it in this format:
1. Long method, 2. Feature envy...

[Java source code]

Details:
- Explicitly names the four target smells to provide context.
- Same strict output formatting for automated parsing.
- Designed to test whether detailed instructions improve detection performance. 

Iterative Refinement:
---------------------
- Both prompts were refined through multiple pilot tests to ensure clarity, distinct specificity levels, and consistent output formatting. 
- Output parsing strategy relies on the prefix “YES I found bad smells” and the marker “the bad smells are:” for automated extraction.

These two prompt strategies form the core of the study’s investigation into how prompt specificity affects ChatGPT’s ability to detect code smells.

Results of Prompting Strategies:

Generic Prompt (#1):
- Blob: not detected (Precision: –, Recall: –, F1: –)
- Data Class: Precision 0.52, Recall 0.06, F1 0.11
- Feature Envy: Precision 0.14, Recall 0.75, F1 0.23
- Long Method: Precision 0.27, Recall 0.92, F1 0.41

Detailed Prompt (#2):
- Blob: Precision 0.31, Recall 0.14, F1 0.19
- Data Class: Precision 0.53, Recall 0.67, F1 0.59
- Feature Envy: Precision 0.28, Recall 0.16, F1 0.21
- Long Method: Precision 0.31, Recall 0.90, F1 0.46

Comparison:
- Detailed Prompt enabled detection of Blob (F1 0.19) and substantially improved Data Class (F1 +0.48) and Long Method (F1 +0.05).
- Feature Envy performance slightly decreased (F1 –0.02).
- McNemar’s test shows Detailed Prompt is 2.54× more likely to produce correct results, with a 16% absolute increase in overall correctness.

Performance by Severity Level (Prompt #1 → Prompt #2):
- Minor:   Precision 0.30 → 0.41, Recall 0.36 → 0.47, F1 0.33 → 0.43
- Major:   Precision 0.30 → 0.45, Recall 0.36 → 0.53, F1 0.33 → 0.49
- Critical:Precision 0.25 → 0.48, Recall 0.29 → 0.58, F1 0.27 → 0.52

F1 by Smell and Severity (Prompt #2):
- Blob:          Minor 0.20, Major 0.18, Critical 0.21
- Data Class:    Minor 0.53, Major 0.64, Critical 0.67
- Feature Envy:  Minor 0.22, Major 0.16, Critical 0.29
- Long Method:   Minor 0.45, Major 0.47, Critical 0.47
