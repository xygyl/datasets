Prompting Strategies in Multi-role Consensus through LLMs Discussions

Location: Section 2 “Multi-role Approach”

1. Initialization Stage:
   - Prompt Description: Tester receives the initial prompt specifying its role (tester), the task (binary vulnerability detection), and the code segment to analyze.
   - Prompt Template:
     System: “You are acting as a Tester. Analyze the following code snippet and decide whether it is vulnerable (1) or non‑vulnerable (0). Provide your judgment and a brief reasoning.”
     User: <code snippet>
   - Response Requirements: Output must be a binary indicator (“1” or “0”) followed by concise reasoning. Response length is constrained by a maximum token limit to ensure clarity and brevity.
   - Transfer: The tester’s judgment and reasoning are automatically forwarded to the Developer with an analogous initial prompt for the developer role. 

2. Discussion Stage:
   - Prompt Mechanism: Tester and Developer engage in an iterative “pose query – deduce response – relay insight” loop.
   - Loop Structure:
     a. Pose Query: One role questions or challenges the previous judgment.
     b. Deduce Response: The other role reevaluates and provides a refined judgment with updated reasoning.
     c. Relay Insight: The initial role integrates the new insight and may pose the next query.
   - Constraints: A maximum discussion depth (e.g., 5 rounds) and response length (e.g., 120 tokens) prevent infinite loops and maintain efficiency.
   - Purpose: Incremental prompting pushes both roles to refine reasoning and converge toward a well‑informed consensus. 

3. Conclusion Stage:
   - Prompt Summary: Once a consensus is reached or the maximum discussion depth is met, the final prompt instructs the Tester to summarize the discussion and record the final judgment.
   - Final Output: The Tester’s latest binary judgment is taken as the definitive result for vulnerability presence.
   - Note: Tester’s role is prioritized in the final decision, reflecting real‑world code review responsibility. 

Results of Prompting Strategies:

Overall Performance Improvement:
- Average precision increased by 13.48%.
- Average recall increased by 18.25%.
- Average F1 score increased by 16.13%.

Consistency Across Scenarios:
- Improvements held across all vulnerability types (function calls, arithmetic expressions, array usage, pointer usage) and dataset mixes (Groups 1–3).

Cost Analysis:
- Achieved these gains at the expense of a 484% increase in token consumption due to multi-role dialogue rounds.
