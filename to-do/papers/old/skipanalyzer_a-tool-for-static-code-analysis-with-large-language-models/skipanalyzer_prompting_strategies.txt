Prompting Strategies in SkipAnalyzer (SkipAnalyzer paper)

Location: Section V.C “Prompting Strategies” 

1. Zero-shot Strategy
   - Definition: The LLM is prompted without any preceding examples.
   - Usage: Employed in all three components:
     • Static Bug Detector (Component I)
     • False-Positive Warning Filter (Component II)
     • Bug Repair (Component III, exclusively) due to example size constraints.
   - Purpose: Leverages the model’s generalization without relying on exemplars. 

2. One-shot Strategy
   - Definition: The LLM prompt includes a single prior input–output example.
   - Usage: Applied in Component I and Component II.
   - Purpose: Provides minimal in-context guidance to shape the LLM’s responses. 

3. Few-shot Strategy
   - Definition: The prompt contains multiple (K) prior examples; in this work, K=3.
   - Rationale for K=3: Chosen to balance contextual guidance with token‑limit constraints (Section V.B). 
   - Usage: Used in Component I and Component II.
   - Purpose: Supplies richer in-context demonstrations for more accurate diagnostics.

4. Zero-shot Chain-of-Thought Prompting
   - Definition: Prompts all components to “explain its decision-making process and the steps it takes to arrive at its conclusions.”
   - Implementation: Instruct the model to articulate reasoning alongside its judgment.
   - Purpose: Improves robustness and validity by exposing intermediate reasoning steps. 

Additional Prompt Content Details:
- Each component’s initial prompt is specialized per bug type (e.g., supplying common Null Dereference patterns or Resource Leak patterns to the LLM in Component I).
- Structured output requirements are defined to facilitate automated parsing.
- All prompts follow best practices of precise, context-aware instructions. 


Results of Prompting Strategies:

1. Static Bug Detection (Component I):
   - Zero-Shot:
     • GPT-3.5 Turbo: Null Dereference – Accuracy 50.66%, Precision 52.32%, Recall 40.64%, F1 45.75%; Resource Leak – Accuracy 56.31%, Precision 44.34%, Recall 40.44%, F1 42.30%.
     • GPT-4: Null Dereference – Accuracy 68.37%, Precision 63.76%, Recall 88.93%, F1 74.27%; Resource Leak – Accuracy 76.95%, Precision 82.73%, Recall 55.11%, F1 66.15%.
   - One-Shot:
     • GPT-4: Null Dereference – Accuracy 64.54%, Precision 62.20%, Recall 79.18%, F1 69.67%; Resource Leak – Accuracy 72.78%, Precision 68.39%, Recall 63.55%, F1 65.88%.
   - Few-Shot:
     • GPT-4: Null Dereference – Accuracy 64.31%, Precision 65.21%, Recall 64.96%, F1 65.09%; Resource Leak – Accuracy 75.25%, Precision 74.12%, Recall 59.55%, F1 66.04%.

2. False-Positive Warning Removal (Component II):
   - On Infer-generated warnings:
     • Zero-Shot with GPT-4: Null Dereference – Precision improved from 65.2% to 93.88% (+28.68%); Resource Leak – Precision improved from 53.8% to 60.0% (+6.2%).
     • Few-Shot with GPT-4: Null Dereference – Precision up to 93.88% (+28.68%); Resource Leak – up to 60.0% (+6.2%).
   - On SkipAnalyzer-generated warnings:
     • Few-Shot with GPT-4: Null Dereference – Precision improved from 81.87% to 98.18% (+16.31%). No false positives for Resource Leak in this set.

3. Bug Repair (Component III):
   - Zero-Shot Chain-of-Thought:
     • GPT-3.5 Turbo: Null Dereference – Logic Rate 94.25%, Syntax Rate 100.0%; Resource Leak – Logic Rate 87.11%, Syntax Rate 97.77%.
     • GPT-4: Null Dereference – Logic Rate 97.30%, Syntax Rate 99.55%; Resource Leak – Logic Rate 91.77%, Syntax Rate 97.77%.
   - Baseline (VulRepair): Null Dereference – Logic Rate 18.39%; Resource Leak – Logic Rate 12.90%.

Overall, SkipAnalyzer’s zero-shot and few-shot strategies with GPT-4 achieved the best trade-off between precision and recall in detection and removal tasks, and nearly perfect syntax and logic rates in patch generation, significantly outperforming baselines.
