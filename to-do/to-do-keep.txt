OVERALL
	what prompts they used
	methodology
	why did one methodology give a better result than another

checker-bug-detection-and-repair-in-deep-learning-libraries
	why did few-shot give a lower F1 score?
		It becomes less likely to call something a bug unless it closely matches one of the provided examples, so it misses more actual bugs
comparison-of-static-application-security-testing-tools-and-large-language-models-for-repo-level-vulnerability-detection
	download datasets detailed in 2.2
		java: https://github.com/OWASP-Benchmark/BenchmarkJava/releases/tag/1.2beta
		C: https://zenodo.org/records/6515687
detecting-code-smells-using-chatgpt_initial-insights
	why does adding a few extra key words make the prompt go from generic to detailed?	
		those small changes affect how the model approaches the problem.
effective-vulnerable-function-identification-based-on-cve-description-empowered-by-large-language-models
	what makes this paper unique? what is their approach to prompting the llm?
		it compares generic and detailed prompts for code smell detection, showing that explicitly listing the smells in the prompt greatly improves detection accuracy. Their approach tests how prompts affect performance by using broad and targeted prompts, revealing that simple prompt engineering can significantly enhance LLM results.
	look in section 2
exploring-chatgpts-capabilities-on-vulnerability-management
	get dataset
		https://github.com/Jamrot/ChatGPT-Vulnerability-Management
	get specific prompts
	what kind of prompts are they using? (see results Tables 3–11, section 4, pages 12–19)
		The paper evaluates six prompt types:
	    0-shot (task description only),
	    1-shot (one example),
	    few-shot (multiple examples),
	    general-info (adds system role and general instructions),
	    expertise (manual domain knowledge in the prompt),
	    self-heuristic (domain knowledge extracted by ChatGPT itself from examples).
	    Examples and templates for each are in Table 2 and Figure 3 (pages 7–8)
	which prompt is the best? get conclusions (see results Tables 3–11, section 4, pages 12–19)
		Few-shot prompts work best for bug report summarization
		   This is because summarization is similar to general NLP tasks, and providing a few examples helps the model match the desired style without overwhelming it with unnecessary information.
		expertise prompts are most effective for security bug report identification and vulnerability repair
		   Explicit expert rules clarify domain boundaries and reduce model hallucination, making it easier for the LLM to apply correct logic for knowledge-driven tasks.
		self-heuristic prompts are best for vulnerability severity evaluation and patch correctness assessment.
		  These tasks require nuanced, context-specific understanding, so letting the model extract domain heuristics from examples enables it to perform better than relying on generic or manually written rules.
		The choice of prompt depends on the task, but advanced prompts (expertise, self-heuristic) generally outperform simple ones for more complex or knowledge-intensive tasks
	testing looks boring; only two models (3.5, 4). why is that?
		The authors use only gpt-3.5 and gpt-4 because these are the most widely available and economical OpenAI models for API-based large-scale evaluation at the time of the study. (2024)
exploring-the-influence-of-prompts-in-llms-for-security-related-tasks
	understand the full prompt methodology (tt and ft)
		can explain that
	what do the tables mean? (what is going on in table 1)
		can explain that
