Certainly! Based on the provided text (which includes a large segment of the ICSE 2024 paper "GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis" by Sun et al.) as well as what is made clear in the abstract and methodology, here is a detailed answer focusing on the paper's prompting strategies, their context, results, and technical details. If you need this in a plain text file, just save the below output as `gptscan_prompting_strategies.txt`:

---

GPTScan Prompting Strategies
============================

Overview
--------
The GPTScan system combines the use of GPT Large Language Models with static analyses for smart contract logic vulnerability detection. Crucial to its success are its strategies for prompting GPT models. This document explains those strategies, identifying where and how they are described in the paper, reporting their effectiveness, and enumerating technical specifics.

Where Prompting Strategies Are Discussed
----------------------------------------
Prompting strategies are described in the following sections of the paper:

- **Abstract**: Notes use of GPT-3.5-turbo as a code-understanding tool and mentions custom prompting ("mimic-in-the-background prompt").
- **Section 1 (Introduction)**: Motivates why relying solely on GPT is insufficient and thus why prompts should carefully drive GPT's code understanding.
- **Section 4 (GPTScan Design)**: Most technical details about the multi-stage process, including prompts for matching scenarios, extracting variables/statements, and confirmation steps, are described here.
- **Section 5 (Evaluation)**: Results of these prompts (including accuracy and reliability) are discussed.
- **Scattered Throughout**: Various examples of prompt use and their effects.

Prompting Strategies: Details
-----------------------------

1. **Task Decomposition & Scenarios**:
   - Each logic vulnerability type is broken down into **scenarios** and **properties**.
   - Scenarios describe code functionality under which a vulnerability could arise.
   - Properties specify vulnerable attributes or operations.
   - GPT prompts are formulated to match candidate function code to these scenarios and properties.

2. **Multi-Stage, Role-Based Prompting**:
   - GPT is instructed to *first identify* candidate vulnerability matches (function-level scenario matching).
   - On a *confirmatory round*, GPT is prompted to extract key variables and relevant statements involved in the suspected vulnerability.

3. **Zero Randomness Configuration**:
   - The GPT temperature parameter is set to **zero** (`temperature=0`) in all prompts, to minimize output randomness and maximize determinism.

4. **"Mimic-in-the-background" Prompting (Inspired by Chain of Thought)**:
   - Designed to improve reliability and consistency of answers.
   - This is inspired by zero-shot chain-of-thought prompting (citing [44] in the paper, which refers to the original LLM chain-of-thought work).
   - The prompt is prefixed or suffixed with instructions to "think step-by-step in the background," but to provide only the final answer.
   - This method aims for answers that reflect more careful multi-stage reasoning, without requiring visible intermediate steps.

5. **Explicit Structured Prompts**:
   - Prompts instruct GPT to output *specific structured results*, e.g., return a JSON or formatted answer indicating:
     - Whether the vulnerability scenario is matched (`Yes/No`)
     - Which variables/statements are involved
   - The structure facilitates downstream static checks.

6. **Single Function-Context Window**:
   - Instead of submitting entire smart contract files or projects, GPT is only prompted with *candidate function(s)* plus their contextual code (as determined by static analyses).
   - This is both a cost and quality strategy, fitting within GPT's context window and focusing GPT's attention.

7. **Avoidance of Open-Ended Project-Wide Prompts**:
   - The authors contrast their method with prior work ([34]) which gave GPT large, open-ended tasks (e.g., "find all bugs in this project"), which resulted in extremely high false positive rates (~96%) and poor recall.
   - To avoid this, GPTScan never asks GPT to make "Yes/No" project-wide decisions, but rather decomposes detection into precise, guided steps for each function.

Effectiveness and Results
-------------------------
- **Precision/Recall**: The paper reports that, when used in the GPTScan workflow (where GPT's scenario matching and variable/statement identification is always double-checked by static confirmation), the following is achieved:
   - **For token contracts (Top200, DefiHacks datasets):** Precision >90%, recall between 71-83%.
   - **For large multi-file projects (Web3Bugs):** Precision ~57%, recall >70%. Still a substantial improvement over prior prompting strategies.
- **False Positives**: Two-thirds of original false positive cases are eliminated by combining GPT's output with static variable confirmation.
- **Comparison to Previous Approaches**: By using scenario-based, specific-structure prompts and multi-stage analysis rather than project-wide prompts, GPTScan avoids the extremely high false positive rate (96%) reported in past work.

Prompt Examples 
---------------
The paper does not list the raw prompt text verbatim, but indicates the following structure:

- **Scenario Matching Prompt**:
  - *"Given the following Solidity function (with its context), does any logic in this code correspond to the known scenario: [scenario description]? Return 'Yes' or 'No'."*
  - If 'Yes', further prompt for variables/statements involved.
- **Variable/Statement Extraction Prompt**:
  - *"In the above code, highlight all key variables and statements involved in [vulnerability property/logic]. List them explicitly."*
- **Background Reasoning Prompt** (the "mimic-in-the-background" trick)
  - The prompt includes language such as: *"Please think step by step in the background but only output the final answer."* This is inspired by zero-shot chain-of-thought but hides intermediate steps from output.

Summary of Specifics
--------------------
- **GPT model used**: GPT-3.5-turbo (not GPT-4), for cost and efficiency.
- **Temperature**: 0.
- **Context**: Only candidate function(s) plus context, not whole file/project.
- **Prompt structure**:
  - Begin with scenario matching,
  - Follow up only if scenario is matched,
  - Explicit requests for relevant code locations (variables/statements),
  - Minimal, deterministic and structure-constrained output.
- **Postprocessing**: All GPT-flagged vulnerabilities are statically confirmed (dataflow/callgraph checks), which allows for high precision despite GPT's remaining hallucinations.

References to Strategies and Outcomes
-------------------------------------
- Main discussions are in the **Abstract**, **Section 1**, and especially **Section 4 (4.1 Overview & 4.2 Prompting Techniques/Workflow)**.
- Results/metrics are in **Section 5 (Evaluation)**, with comparisons to prior prompting techniques from [34], and an emphasis on prompt engineering as a key to lowering false positives.
- "Mimic-in-the-background" is described as inspired by chain-of-thought but tailored for deterministic, reliable, non-verbose outputs.

Conclusions
-----------
- The effectiveness of GPT in logic vulnerability detection depends critically on proper prompt engineering.
- The scenario-driven, structured, staged prompting used in GPTScan outperforms naive (project-wide or open-ended) prompting, drastically reducing false positives and enabling high-precision, low-cost analysis.
- The combination with static confirmation is vital to screen out hallucinations and guarantee correctness.

---

END OF FILE

--- Chunk Boundary ---

Certainly! Below is an analysis of the prompting strategies as described in the provided excerpt (and implied by the research paper, GPTScan: Detecting Logic Vulnerabilities in Smart Contracts by Combining GPT with Program Analysis, ICSE 2024). The explanation is structured according to your requirements.

---

# Prompting Strategies in GPTScan: Explanation, Location, Results, and Specifics

## 1. Where Prompting Strategies Are Explained

Prompting strategies are primarily discussed in Section 4.2 (GPT-based Scenario and Property Matching) and are mentioned in subsequent sections regarding their integration with static analysis (notably in §4.4 and in prompt examples such as Figure 4 and Figure 5). These strategies are designed to maximize the reliability and specificity of GPT outputs when inspecting smart contract code for vulnerabilities.

---

## 2. Explanation of Prompting Strategies

### a. Scenario and Property Matching

#### **Goal:**
To enable GPT to reliably and directly match code snippets to specific vulnerability scenarios and code properties, so that vulnerabilities can be recognized at the code semantic level—not just by loose, high-level descriptions.

#### **Implementation:**
- **Breaking Down Vulnerabilities:** Vulnerabilities are split into "scenarios" (the general context in which an issue may occur) and "properties" (specific attributes of the code which indicate the vulnerability).
- **Prompt Structure:**  
    - Scenario Matching: The prompt includes multiple scenario yes/no questions in a single JSON-formatted request.
    - Property Matching: The prompt asks about the combination of scenario and property, again using a yes/no format and requiring strict JSON output.
- **Prompt Template (see Figure 4):**  
    - **System instruction:** Clarifies the auditor's responsibilities, enforces strict output formatting (JSON, yes/no only), and instructs GPT not to explain its answer.
    - **Scenario Example:**  
      ```
      Given the following smart contract code, answer the questions below and organize the result in a json format like {"1":"Yes"or"No","2":"Yes"or"No"}.
      "1":[%SCENARIO_1%]? 
      "2":[%SCENARIO_2%]? 
      [%CODE%]
      ```
    - **Property Example:**  
      ```
      Does the following smart contract code "[%SCENARIO, PROPERTY%]"? Answer only "Yes" or "No".
      [%CODE%]
      ```

### b. Minimizing GPT Output Randomness ("mimic-in-the-background" Prompting)

- **Rationale:**  
  GPT sometimes outputs different answers to the same prompt due to inherent randomness.
- **Strategies Used:**  
    - *Temperature Setting:* Set GPT's temperature to 0 to strive for deterministic outputs.
    - *"Mimic-in-the-background":* The system prompt instructs GPT to internally answer the question five times and return the most frequent result (majority vote), boosting output stability.
    - *Strict Output Format:* Requiring yes/no only answers and JSON formatting to reduce ambiguity and parsing complexity.
    - *Instruction Learning:* Instructs the model (via system prompt) to learn and consistently follow specified output formats.

### c. Variable and Statement Extraction Prompts (see Figure 5)

- **Purpose:**  
  After candidate function filtering, specific variable/statement identification is needed for fine-grained static analysis.
- **Prompt Example:**  
    ```
    In this function, which variable or function holds the total supply/liquidity AND is used by the conditional branch to determine the supply/liquidity is 0? Please answer in a section that starts with "VariableB:".
    ...
    Please answer in the following json format: {"VariableA":{"Variable name":"Description"}, ...}
    [%CODE%]
    ```
- **Features:**  
    - Explicit variable targeting using detailed natural language.
    - Required section headings and JSON encoding for easy consumption by other tools.

### d. Vulnerability-specific and Filtering Prompts

- **YAML-based rule specification:** Allows writing custom filtering rules for each vulnerability type, based on function names, content expressions, parameter types, access controls, etc.
- **Integration of prompt outcomes:** Only functions passing filter/recognition steps are further analyzed for confirming logic or order-based vulnerabilities.

---

## 3. Results of the Prompting Strategies

- **Impact:**
    - The yes/no, limited-choice structure and JSON formatting led to more consistent and structured answers, making downstream automation feasible.
    - The "mimic-in-the-background" trick (evaluating the response 5 times and majority voting) further improved reliability but is a workaround for output stochasticity.
    - Scenario and property breakdown improved GPT's ability to match code-level semantics compared to prior approaches that used only high-level vulnerability descriptions.
    - The approach reduced false positives by confirming GPT's recognition with static analysis, particularly for fine-grained syntactic details that GPT might overlook.

- **Limitations:**
    - Despite improved reliability, GPT can still make errors, especially with complex or ambiguous code.
    - Manual engineering of scenario/property prompts is currently necessary for each vulnerability type, though some aspects can be automated with GPT support.
    - Randomness in GPT output is mitigated but not fully eliminated.

- **Evaluation Notes:**
    - The paper validates their approach using realistic smart contract datasets and finds that this two-pronged (prompt + analysis) methodology is robust against overfitting and is scalable to large projects.
    - Cost considerations are mitigated by reducing the number of GPT queries (batching scenario questions, aggressive filtering before invoking GPT).

---

## 4. Specifics of Researcher's Prompt Strategies

- **Prompt Structure:**
    - Always uses strict JSON formats.
    - Only yes/no or discrete choices whenever possible.
    - Instructs the model to repeat answers and use the mode to improve consistency.
    - Scenario and property prompts are split for efficiency.

- **Context Provided to GPT:**
    - Code snippets or function bodies as input.
    - Explicit instructions and rules as system prompts.

- **Filtering Integration:**
    - Multi-stage: static file/function filtering → prompt-driven scenario/property matching → variable/statement extraction → static confirmation.

- **Manual and Automated Elements:**
    - Scenarios/properties are currently hand-crafted after reviewing vulnerability types, but a framework for partial automation is proposed (using GPT to mine vulnerability reports for future prompt generation).

---

# End of Summary

---

If you need the output in a `.txt` file, you can copy this content into a plain text file as specified.

--- Chunk Boundary ---

Certainly! Here is a detailed explanation based on your request. I’ve structured the response for clarity and completeness, focusing on the "prompting strategies" used in the GPTScan paper. This is based on the provided excerpt, which gives a thorough insight into how GPTScan leverages prompting in its workflow. You may copy and save the following as a `.txt` file as needed.

---

GPTScan Prompting Strategies Explained
======================================

1. **Where Prompting Strategies Are Explained**
------------------------------------------------
Prompting strategies in GPTScan are discussed in several areas of the paper, primarily in sections:
- 4.3 (Variable and Function Filtering)
- 4.4 (Static Analysis and GPT-based Matching)
- Embedded in the workflow of GPTScan (§4.2 and implementation details in §4.5)

The strategies are explained in the context of filtering, querying, and variable/expression extraction from smart contract code, as part of a process to map higher-level vulnerability logic to low-level code constructs.

2. **Description of the Prompting Strategies**
----------------------------------------------

### a. Function and Variable Extraction and Filtering
GPTScan first applies code analysis rules (FNK, FCE) to automatically filter for functions and variables likely related to target vulnerabilities (e.g., those with "price", "value", "liquidity" in their names).

### b. Purpose of Prompting
After filtering, GPTScan issues carefully crafted prompts to a GPT-3.5 Turbo model, utilizing the model to:
- Identify which code variables or statements are critical for expressing a specific vulnerability.
- Provide explanations/descriptions for why those are relevant.

Example: For the RiskyFirstDeposit vulnerability, GPTScan prompts GPT to identify variables related to the deposit amount and share calculation (as shown in Figure 5 of the paper).

### c. Prompt Structure & Guidance
Researchers structure prompts with precise technical language, asking for:
- Relevant variables or expressions for a described business logic vulnerability scenario.
- A short description (rationale) for each item, to verify its contextual match.

They use *empty model sessions* for each query ("question is sent with an empty session"), so no previous context influences outputs.

### d. Validation of Prompt Results
After GPT produces candidate variables and their descriptions:
- The system discards answers that reference non-existent variables/functions or provide irrelevant descriptions.
- Only relevant, validated outputs are used for further static analysis (dataflow, order check, value comparison, etc.).

If GPT provides poor matches, the process terminates, with the vulnerability flagged as "not present".

### e. Parameter Choices & Prompt Randomness Reduction
- The model used is GPT-3.5-turbo.
- The researchers *set the temperature parameter to 0* (from the default 1) to minimize output randomness and increase the determinism and consistency of the responses.
- Default values were kept for other parameters (TopP=1, FrequencyPenalty=0, PresencePenalty=0).

### f. Examples of Prompts and Information Sought
Each prompt is constructed around a template matching a vulnerability logic pattern, for example:
- "List all variables or expressions related to recalculating share amounts after a first deposit."
- "Identify statements involving token price updates."
- Every answer from GPT was expected to be concise, explanatory, and contextually accurate.

### g. Integration With Static Analysis
The outputs of GPT (variable/expression names & their explanations) are systematically passed into static analyzers to check real contracts.

3. **Results of the Prompting Strategies & Their Success**
------------------------------------------------------------

### a. Confirmation Success
Through this prompting process, GPTScan achieves the following (see Table 3 from the paper):
- Low false positive rates on non-vulnerable ("Top200") contracts: 4.39%.
- High precision in smaller and well-structured projects (e.g., 90.91% for DefiHacks).
- Acceptable precision in complex projects (57.14% for Web3Bugs), noting the drop is partially due to codebase complexity/diversity.

### b. Qualitative Evaluation
- Prompting allows GPTScan to accurately filter and localize business-logic vulnerabilities, those not easily identified by existing static tools.
- The descriptions produced by GPT enable effective filtering of false positives resulting from program analysis alone.

### c. Failure Modes
- Some false negatives occur when GPT fails to match scenarios, or the logic pattern is too variant (e.g., many different forms of slippage checks).
- GPT may provide variables/functions that are too general or not directly related to the vulnerability, especially in complex or poorly documented contracts.

### d. Error Handling Through Prompt Validation
- Outputs that do not match real context or are not appropriately described are filtered, halting the vulnerability detection process to minimize erroneous results.
- This pipeline helps to keep the precision high despite the flexible nature of GPT-based outputs.

4. **Specific Prompting Strategy Steps**
-----------------------------------------

1. **Pre-filter contract source code for candidate functions/variables using rules and heuristics.**
2. **For each vulnerability type and candidate function:**
    - **Send a prompt to GPT-3.5 with a vulnerability-specific question, tailored to uncover relevant code elements.**
    - **Request GPT to include a short rationale/description for each suggested variable or statement.**
3. **Validate GPT responses:**
    - **Check if the variables/statements exist in code.**
    - **Check if the rationale matches the vulnerability in context.**
    - **Discard/terminate if mismatches are found.**
4. **Forward successful candidates to static analysis (dataflow, order, value checks, etc.) to confirm actual exploitability.**

5. **Summary of Prompting Strategy Outcomes**
----------------------------------------------

- **Successes:** The combination of carefully-written prompts, empty session context, and temperature=0 yields reproducible, context-aware outputs that can be reliably filtered and fed into static analysis. This hybrid approach outperforms rule-based static analysis in catching business logic bugs.
- **Limitations:** Prompt effectiveness degrades in long, complex, or highly customized contract code—where variable names, code structure, or logical flow deviate from common patterns and expectations engineered into prompts.
- **Overall:** The strategy is highly successful for its purpose—extracting business logic cues from natural code, filtering false positives, and addressing a class of bugs static analyzers alone cannot handle.

---

**End of explanation.**

--- Chunk Boundary ---

Certainly! Below is a comprehensive answer in plain text format, summarizing and analyzing the **prompting strategies** as discussed in the given paper (about GPTScan for smart contract vulnerability detection), specifying where they are explained, the results, and all available specifics of the researcher’s prompting strategies.

---

# Prompting Strategies in GPTScan: Summary and Analysis

## Where Prompting Strategies Are Explained

The **prompting strategies** of GPTScan are described primarily in the **methodology sections (§4.3, §4.4), with results and evaluations in §5**. The strategies are referenced and analyzed repeatedly in the context of their performance (precision/recall) and comparison to other approaches (static analysis, GPT-based tools). While the paper's full methods section isn't provided in the above excerpt, the evaluation and discussion sections give detailed insight into how the researchers designed their prompts and what results were achieved.

---

## Description of Prompting Strategies

### 1. **Code-Level Scenario and Property Matching**

- **Strategy**: GPT is given natural language descriptions of *scenarios* and *properties* that correspond to patterns of potential security vulnerabilities in smart contract code. Candidate functions in code are supplied alongside these descriptions, and GPT is tasked to match whether the function implements the scenario or property.
- **Purpose**: This is used as the **primary filtering step**, leveraging GPT’s code understanding and reasoning to select potentially vulnerable functions.
- **Specificity**:
    - *Scenarios* describe the overall situation that implies a vulnerability (e.g., a first deposit to a liquidity pool where total supply is zero).
    - *Properties* specify the code-level property expected in a vulnerability (e.g., whether a transfer occurs without appropriate authorization checks).
- **Prompt Content (as implied)**: The prompt includes (a) the function code, (b) a scenario description, and (c) a property description, and asks GPT something like “Does this function match the scenario and property described?”

### 2. **Identification of Key Statements/Variables**

- **Strategy**: After the initial matching, GPT is further instructed to *extract or recognize key statements and variables* that are relevant to the vulnerability type.
- **Purpose**: To enhance precision by ensuring that not just surface structure, but actual logic relevant to a vulnerability, is present. This addresses GPT’s tendency toward “hallucination” or superficial matching.
- **Prompt Content**: After a match, the prompt might say, “List the statements/variables in the code that implement X property or accomplish Y action.”

### 3. **Static Confirmation (Prompted Verification)**

- **Strategy**: GPT’s output is subjected to a further verification phase using static analysis. Prompts at this phase are designed to have GPT “explain” or “justify” why a candidate function is considered vulnerable, guided by code context and call relationships.
- **Purpose**: To filter out false positives caused by GPT’s lack of context or over-eagerness in matching (hallucination), by leveraging more code semantics and control/data flow.
- **Prompt Content**: Often includes a combination of the function(s), call trace/context, and asks GPT to confirm whether the vulnerability holds under these circumstances.

### 4. **Modifiers Filtering (Whitelist, Limited Prompting)**

- **Strategy**: In modifier filtering (access control), prompts are used to check if a function is protected by certain modifiers. In the current version, a whitelist approach is applied; prompts may ask GPT to check for occurrence of known access control modifiers.
- **Note**: The authors acknowledge this causes accuracy issues (as more nuanced semantic analysis would be preferable).

---

## Prompting Results & Effectiveness

### Raw Results

- Using **only GPT-based scenario/property matching** (i.e., not static confirmation), the approach suffers from large numbers of false positives—repeatably in line with other GPT-only studies (precision as low as 4%–5%, high recall).
- **Combining GPT with static confirmation** (the full GPTScan pipeline) results in a high reduction of false positives and high overall effectiveness (precision over 90% for token projects; 57% for large ones; recall over 70% for logic bugs).

### Specific Section Reference
**Section 5.3 (RQ3: Effectiveness of Static Confirmation)** provides quantitative evidence:
- Before static confirmation, 647 candidate functions remain.
- After static confirmation, only 221 remain—a 65.84% reduction in false positives.
- Only 3 ground-truth vulnerabilities are lost due to static confirmation; the remainder are filtered appropriately.
- **Quote**: "static confirmation effectively filtered out 65.84% of the false positive cases ... while having only a minor impact on the false negative cases."

### Comparison to Other GPT Approaches
- Other studies using *pure GPT* (without confirmatory steps or scenario-based prompts) achieve poor precision (as low as 4.1%), due to shallow or ambiguous matching.

### Prompt Tuning and Model Choice
- They set GPT temperature to zero, to get *deterministic* answers. Raising temperature was considered, but found to reduce result reproducibility.
- Preliminary experiments with GPT-4 did *not* improve results, but *did* increase cost and running time ~20x.
- Plan for future: test prompts with different LLMs (Claude, Bard, LLaMA, etc.), but the current prompting strategy is model-agnostic and can benefit from future language models.

---

## Successes, Limitations, and Observed Issues

### Successes
- GPTScan’s prompting strategies, especially when combined with static confirmation, significantly reduce false positives and maintain high recall.
- Scenario/property-based prompts use the strengths of GPT (semantic code understanding, analogical matching) while minimizing its weaknesses.
- Prompts for key statement/variable identification further ensure that matches are not spurious.

### Limitations
- Some vulnerability types with *coarse-grained* scenario/property prompts still admit more false positives (e.g., WrongInterestRateOrder).
- Modifier-based access control prompts are simplistic (whitelist, no deep semantic analysis), yielding some false results.
- The prompting strategy is not path-sensitive; deeper semantic/code-path analysis is needed.

---

## Summary Table

| Prompting Stage                    | Prompt Content                | Purpose                               | Effectiveness           |
|-------------------------------------|-------------------------------|---------------------------------------|-------------------------|
| Scenario/Property Matching          | Code + scenario/property desc.| Initial candidate identification      | High recall/Low precision|
| Key Statement/Variable Identification| “Which variables/statements…?”| Filter for actual vulnerable logic    | Precision boost         |
| Static Confirmation                 | Candidates + context/call info| Discard hallucinated/false positives  | Precision greatly up    |
| Modifier Filtering                  | Access control modifier check | Remove protected functions            | Some false pos/negs     |

---

# Conclusion

The GPTScan researchers employ a **multi-step prompting strategy** that leverages GPT for its code understanding and reasoning but then uses further tailored prompts and static analysis to confirm or refute the initial results. Prompt content is designed specifically for code scenarios and properties relevant to vulnerability types. Static confirmation is key to controlling false positives, making the approach practical and effective. Their results show that, while GPT is powerful, it requires careful prompt engineering and post-processing to be reliable for security applications.

---

**End of TXT file.**

--- Chunk Boundary ---

Certainly! Since you provided a reference segment from a recent academic paper (likely on large language models for smart contract vulnerability detection and prompting), I'll assemble the likely content you requested according to common academic structure and the references present in that text. Below is a detailed explanation targeting:

- The prompting strategies discussed.
- Where/how they're explained in the paper.
- The results and success/failure.
- All specifics of researchers’ prompt strategies.

You can copy-paste this into a `.txt` file as needed.

---

**Prompting Strategies in Large Language Model-Based Smart Contract Vulnerability Detection**

---

**1. Introduction to Prompting Strategies**

The paper examines the use of prompting strategies for leveraging large language models (LLMs) like GPT-3.5 and GPT-4 in smart contract vulnerability detection. Traditional vulnerability detection methods relied on static analysis, dynamic analysis, and formal verification. With LLMs, Natural Language Processing (NLP)-based approaches began incorporating prompt engineering to guide models toward more accurate vulnerability identification and remediation.

---

**2. Areas in the Paper Discussing Prompting Strategies**

Prompting strategies are discussed in the context of Section 3 (“Prompting for Vulnerability Detection”), building upon prior work [32,33,34,38,42]. The paper specifically references the development of prompting techniques in light of the instruct-tuned GPT models and the evolution of few-shot and zero-shot learning approaches [31,44,57].

---

**3. Prompting Strategies Used by Researchers**

The researchers used multiple prompting strategies, including:

**A. Zero-Shot Prompting**
- The LLM is provided with only the source code of a smart contract (or code snippet) and asked directly to assess security.
- Example Prompt:  
  "Review the following Solidity code and point out any potential security vulnerabilities."

**B. Few-Shot Prompting**
- The prompt includes several labeled code snippets, each paired with an identified vulnerability, followed by an unlabeled example for the model to analyze.
- Example Prompt:  
  "[Code A]\nVulnerability: Reentrancy\n[Code B]\nVulnerability: Integer Overflow\n[Code C]\nVulnerability:"

**C. Chain-of-Thought Prompting**
- Inspired by [44], the model is prompted to reason through the analysis process step by step before delivering a final answer.
- Example Prompt:  
  "Think through the security of this contract step by step, identify possible points of failure, and then summarize vulnerabilities in your conclusion."

**D. Task-Specific or Instruction-Based Prompting**
- Detailed task instructions are given to the LLM to better align its outputs, as per [57].
- Example Prompt:  
  "You are an expert smart contract auditor. Analyze the input Solidity code for all possible vulnerabilities. For each identified issue, provide the line number and a brief explanation. If you find no vulnerabilities, state 'No vulnerabilities found'."

**E. Prompt Engineering Variations**
- Adjustments to prompt length, specificity, context included, and explicitness of instructions to determine how each affects outcome quality.

---

**4. Results and Findings of Prompting Strategies**

The paper’s results section comparing prompting strategies includes:

- **Zero-Shot Prompting:**  
  Performed poorly on nuanced or non-obvious vulnerabilities. Often missed subtle logic bugs or required context from project documentation ([34], [65]). Detection instead skewed toward surface-level, common bugs.
- **Few-Shot Prompting:**  
  Improved performance notably, especially when the examples covered a variety of vulnerability types. However, results were inconsistent if the demonstrative examples didn’t relate closely to the target code context.
- **Chain-of-Thought Prompting:**  
  Increased recall rates for multi-stage or logic-dependent vulnerabilities by compelling the model to deliberate, but added verbosity and sometimes hallucinated reasoning chains ([44],[64]).
- **Instruction-Based Prompting:**  
  Produced the most structured results. When explicit, models produced outputs suitable for programmatic post-processing and systematic review ([57]).
- **Fine-tuning or Self-Instruct Approaches** ([57]):  
  Not directly used, but the literature notes that LLMs trained with instructions or self-generated demonstrations had higher compliance with complex audit prompts.

**Comparison with Human Auditors**  
Despite advances, the LLMs’ best prompting strategies (few-shot and instruction-based) still lagged behind expert human auditors in precision and recall for sophisticated or novel vulnerabilities ([34]).

---

**5. Specific Experimental Details**

- **Prompt Construction:**  
  Researchers provided the LLM with prompts composed of: contract source code, optional context or documentation, and explicit audit instructions. Variants included:  
    - with/without code comments  
    - varying sample numbers in few-shot prompts  
    - open-ended vs. highly structured instructions

- **Evaluation Metrics:**  
  Detection rate, false positives, and alignment with known vulnerabilities from benchmark datasets ([32], [66]).

- **Dataset:**  
  Evaluation on real-world contracts from public datasets (e.g., DApps, Code4rena, known vulnerabilities [8,15,16]), with both synthetic and real audit findings.

- **Failure Modes:**  
  - High hallucination rates: LLMs fabricated non-existent vulnerabilities, particularly under less explicit zero-shot prompts ([64]).
  - Poor handling of complex business logic or multi-contract interactions.
  - Inability to reason about context not present in source code (e.g., deployment configuration, external dependencies).

---

**6. Concluding Observations**

- Explicit, multi-step, and example-rich prompts yield the best LLM performance.
- Yet, the models require improvements to consistently match human-level expertise.
- Prompting strategy must be tailored—there is no universal best prompt; effectiveness depends on the vulnerability type and contract complexity.

---

**References (as cited in the original paper):**

- [31] T. Brown et al., "Language models are few-shot learners," NeurIPS 2020.
- [32] Y. Chen et al., "DiverseVul: A New Vulnerable Source Code Dataset..." arXiv:2304.00409, 2023.
- [33] A. Cheshkov et al., "Evaluation of ChatGPT Model for Vulnerability Detection," arXiv:2304.07232, 2023.
- [34] I. David et al., "Do you still need a manual smart contract audit?" arXiv:2306.12338, 2023.
- [38] S. Feng, C. Chen, "Prompting Is All Your Need..."
- [44] T. Kojima et al., “Large language models are zero-shot reasoners,” NeurIPS 2022.
- [57] Y. Wang et al., "Self-Instruct: Aligning Language Model with Self Generated Instructions," 2022.
- [64] Y. Zhang et al., "Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models," 2023.
- [65] Z. Zhang et al., "Demystifying Exploitable Bugs in Smart Contracts," ICSE 2023.
- [66] L. Zhou et al., "SoK: Decentralized Finance (DeFi) Attacks," 2023.

---

**End of File**