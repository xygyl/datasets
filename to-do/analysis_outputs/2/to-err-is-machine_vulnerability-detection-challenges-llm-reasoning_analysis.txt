Certainly! Below is a detailed explanation in plain text format of the prompting strategies discussed in "TO ERR IS MACHINE: VULNERABILITY DETECTION CHALLENGES LLM REASONING," including their description, where they are explained in the paper, and a summary of their effectiveness and results.

---

TO ERR IS MACHINE: EXPLANATION OF PROMPTING STRATEGIES

1. WHERE THE PROMPTING STRATEGIES ARE EXPLAINED

The prompting strategies are primarily described in Section 2 ("CAN LLMS EFFECTIVELY DETECT VULNERABILITIES?") of the paper. Further references to their use and error analysis appear in Section 3 and in detailed methodology sections (e.g., Section A) alluded to in the body text.

2. DESCRIPTION OF THE PROMPTING STRATEGIES

Researchers implemented both baseline and enhanced prompting methods in their evaluation:

**A. Baseline Prompting Methods**

- **Basic (Zero-shot):**  
  The simplest approach. The model is given only a system prompt with necessary instructions and the vulnerability definition (from MITRE, 2024), along with the raw program source code. No examples or stepwise guidance is provided.  
  *References:* Fu et al. (2023)

- **In-context (n-shot):**  
  The prompt includes several example tasks (with code and vulnerability labels) before showing the target task, intended to guide the model by demonstration.  
  *References:* Liu et al. (2023b); Zhou et al. (2024)

**B. Enhanced, Domain-Aware Prompting Methods**

Three new strategies were developed to leverage metadata and explicit reasoning steps:

- **(1) Contrastive Prompting:**  
  - Uses *contrastive pairs* of functions before and after a bug fix as in-context examples.  
  - Goal: Instructs the model to pay attention to the fine-grained differences causing the vulnerability/fix, aiming for improved discrimination between vulnerable and safe code.

- **(2) Chain-of-Thought from CVE descriptions (CoT-CVE):**  
  - Uses explanations from real-world CVE (Common Vulnerabilities and Exposures) bug reports, prompting the model to generate an explanatory response about why code is vulnerable or not.  
  - These detailed step-by-step explanations are intended to encourage the model to “think out loud” about security-relevant properties.

- **(3) Chain-of-Thought from Static Analysis (CoT-StaticAnalysis):**  
  - Applies reasoning modeled on the proofs generated by a static code analyzer.  
  - Prompts contain step-by-step logical reasoning as extracted from tools like D2A dataset and Infer static analyzer, conditioning the model to follow similar logic in identifying vulnerabilities.

**Other Controls/Baselines:**

- **Random Prompt:**  
  Randomly structured prompt, serving as a practical lower-bound baseline.

- **Embedding Prompt:**  
  Not discussed in detail in the provided excerpt, but likely involves providing code embeddings or semantic hinting.

3. PROMPT CONSTRUCTION DETAILS

- All prompts included a system prompt with the vulnerability definition and program source code.
- The enhanced prompts (Contrastive, CoT-CVE, CoT-StaticAnalysis) incorporated detailed, domain-specific information or stepwise logic reflecting expert reasoning about vulnerabilities.
- Examples for contrastive or chain-of-thought prompts were chosen from metadata and curated explanations or static analysis artifacts.

4. RESULTS OF THE PROMPTING STRATEGIES

- **Across all models and prompting strategies, performance was poor:**  
  Models scored only slightly above the random guessing baseline on the SVEN dataset (Balanced Accuracy 50–55%). No prompt/model combination exceeded 55% Balanced Accuracy.

- **Marginal Improvements:**  
  - The new (enhanced) prompts offered a *slight* improvement over the baselines for 11 out of 14 models.
  - The Contrastive prompt provided a small boost for 8 out of 14 models.

- **No Breakthroughs:**  
  - Not a single prompt/model setting improved performance beyond 5% Balanced Accuracy over random guessing.
  - Even advanced models (e.g., GPT-4, StarCoder2) failed to distinguish vulnerable from non-vulnerable code in more than two-thirds of cases.

- **Simple Examples:**  
  The researchers also evaluated with simple code examples (avg. 25 lines) from CWE and SARD, but the results remained lackluster (42–67% Balanced Accuracy).

- **Domain Knowledge and Stepwise Reasoning:**  
  Adding domain knowledge and explicit multi-step reasoning (via Chain-of-Thought and static analysis) reduced errors in some reasoning sub-steps but failed to resolve multi-step inference failures overall. Models still struggled, particularly with localization of relevant checks and multi-step semantic analysis.

- **Error Analysis:**  
  Many errors occurred at all reasoning steps: 
    * Step 1 (localization) – Missed key security-relevant statements (e.g., null/bounds checks).
    * Step 2 (semantic understanding) – Misinterpreted code statements involving strings, pointers, integer operations.
    * Step 3 (logical connection) – Failed to connect facts or performed faulty, inconsistent reasoning.

5. SUMMARY

- Simple and sophisticated prompting (including in-context examples, chain-of-thought, explicit domain prompts) *did not* enable current LLMs to perform effective vulnerability detection.
- Results were consistent across diverse state-of-the-art foundation models, regardless of size or training data used.
- The findings suggest fundamental limitations in current LLM pre-training for extracting executable code semantics, not solvable by prompt engineering alone.

6. TABLES AND FIGURES

- **Figure 2** in the paper provides a bar graph for prompt/model combos, showing Balanced Accuracy mostly hovering around 50–55%.
- **Table 1** shows that while LLMs perform strongly on code generation, code execution, math, and natural language reasoning, the same models perform poorly on vulnerability detection, with no meaningful gains from advanced prompting.

---

END OF SUMMARY

--- Chunk Boundary ---

Certainly! Here is a structured explanation in plain text format, suitable for a `.txt` file, summarizing the prompting strategies discussed in the paper, their descriptions, methodology, findings/results, and details about the researchers' prompt designs.

---

PROMPTING STRATEGIES IN LLM VULNERABILITY DETECTION: SUMMARY AND EVALUATION

------------------------------------------------------------
1. INTRODUCTION TO PROMPTING IN THE PAPER
------------------------------------------------------------
The paper provides an in-depth evaluation of large language models (LLMs) for code vulnerability detection and diagnosis. Given the complexity of the task and the numerous failure points identified in model performance, researchers explored a variety of prompting strategies to assess whether more advanced or targeted prompts could overcome LLMs’ reasoning limitations.

Prompting strategies refer to the methods by which questions or tasks are posed to the LLMs, including the structure, content, and background information included in the prompts.

------------------------------------------------------------
2. PROMPTING STRATEGIES EXAMINED
------------------------------------------------------------

A. BASE/INSTRUCTION PROMPTS  
----------------------------  
- **Description:** The simplest form of prompting, where the LLM is given code (sometimes with a bug or fix), and directly asked whether the code is vulnerable or not.
- **Variants:**  
    - "Base" prompts: straight, unadorned questions (e.g., "Is this code vulnerable?").
    - "Instruction" prompts: in the style of instruction-following, possibly with some framing (suited for instruction-tuned models).
- **Goal:** To establish a baseline for model capability with minimal contextual guidance.

B. IN-CONTEXT LEARNING / FEW-SHOT EXAMPLES  
-------------------------------------------  
- **Description:** Prompts that include additional examples (either buggy, fixed, or both) with explanations before the target code (the query example). The idea is that seeing examples helps the LLM learn the task's structure and expected output.
- **Implementation:** For example, providing 1-3 annotated bug/fix examples and then asking about a new code snippet.
- **Goal:** Leverage the LLMs’ limited ability for in-context learning to improve reasoning.

C. CONTRASTIVE (PATCHED/UNPATCHED) PAIRS  
-----------------------------------------  
- **Description:** Prompting the model with *pairs* of code—one buggy, one fixed (patched)—and asking it to distinguish between them (i.e., identify which version is vulnerable).
- **Rationale:** Forces the model to focus on the nuanced semantic difference that distinguishes buggy from fixed code.
- **Implementation:** Pairs are presented together (e.g., "Here are two versions of a function: … Which, if any, is vulnerable?").
- **Where discussed:** Results shown at the start of the user-provided text (the table and discussion about distinguishing code pairs).

D. CHAIN-OF-THOUGHT (CoT) PROMPTING  
-------------------------------------  
- **Description:** Prompts instruct the model to "think step by step," often by breaking down the reasoning required to identify a vulnerability.
- **Variants:**  
    - *Generic CoT*: Prompting with "Let's think step by step" to trigger intermediate reasoning.
    - *CoT-CVE*: Integrating external information, such as CVE (Common Vulnerabilities and Exposures) descriptions, to make the model reason with more context.
    - *CoT-StaticAnalysis*: Including static analysis insights within the prompts.
- **Goal:** To encourage the LLM to outline its reasoning process, potentially revealing errors at each step and enabling finer control and diagnosis.

E. CHAIN-OF-THOUGHT WITH DOMAIN ANNOTATIONS (CoT-Annotations)  
---------------------------------------------------------------  
- **Description:** An advanced form of CoT prompting where the prompt is augmented with externally-derived domain knowledge, for example static analysis annotations marking pointer assignments, null-checks, dereferences, etc.
- **Implementation:**  
    - Static analysis tool processes the code and marks relevant operations.
    - These annotations are included in the prompt, e.g.  
    ```
    CodeExplanation: The pointer cl is passed as a parameter... 
    Instructions: For the QueryExample only, think step-by-step using these steps, then give an answer...
    ```
    1. Identify likely null pointers.
    2. Identify dereferences of those pointers.
    3. Identify which dereferences are covered by safety checks.
    4. Decide vulnerability based on remaining unchecked dereferences.

- **Example Prompt:**
    - As shown in Fig. 8 in the text, with step-by-step guidance integrated with code-specific annotations.
- **Targeted Vulnerability:** This strategy was specifically tested on Null Pointer Dereference (NPD) vulnerabilities.

------------------------------------------------------------
3. RESULTS OF THE PROMPTING STRATEGIES
------------------------------------------------------------

A. BASE/INSTRUCTION & CONTRASTIVE PAIRS (Table 2, Main Results)
----------------------------------------------------------------
- On average, 69.7% of (buggy/fixed) code pairs could not be distinguished by the LLMs.
- The best model (RBO) achieved 80.9% "can't distinguish"; even the best performing LLM-based system did not reliably distinguish buggy from fixed.
- High percentage of indistinguishable pairs suggests these prompts do not lead to deep semantic understanding.

B. CoT AND DOMAIN ANNOTATIONS (Section 3.3, Figures 8-9)
----------------------------------------------------------
- *CoT-Annotations* (step-by-step with static analysis) did help reduce specific types of errors (like misunderstanding bounds/null checks):
    - For CodeLLAMA, MagiCoder, and Mistral, error rates dropped by 15-70% (Figure 9a).
    - However, overall vulnerability detection accuracy *did not improve significantly* (many models still missed 23-67% of bounds checks, and overall performance gains were minimal).
    - This is attributed to remaining reasoning barriers: models failed to carry out correct logical or multi-step inference, even when domain knowledge was made explicit.

- *In-context/Few-shot* improvement was minimal; the additional context occasionally helped larger models better follow instructions, but didn't lead to larger performance gains in vulnerability detection.

C. TRAINING DATA & PROMPT ALIGNMENT
-------------------------------------
- Instruction/fine-tuned models (e.g., WizardCoder, CodeLLAMA-Instruct) were tested with instruction-style prompts, but instruction tuning did not translate to improved vulnerability detection.
- Adapter-fine-tuning with bug labels also failed to deliver notable gains.

------------------------------------------------------------
4. DETAILED RESEARCHER PROMPT STRATEGIES AND RATIONALE
------------------------------------------------------------

- **Prompt Engineering**: Prompts were tailored to:
    - Frame questions in an instruction-following style (for instruction-tuned models).
    - Provide bug/fix example pairs to encourage contrastive reasoning.
    - Integrate domain-specific code annotations when static analysis could identify logic relevant to vulnerabilities.
    - Direct models through explicit stepwise reasoning to clarify which specific logical steps were being skipped or misunderstood.

- **Manual Analysis & Evaluation**: Researchers manually inspected 300 LLM responses, classifying error types (localization, semantic misunderstanding, logical inference, hallucination, repetition, etc.). They observed recurring failures regardless of prompt sophistication.

- **Coverage**: Four novel prompt approaches (not all detailed in the excerpt) were designed, variously using bug-fix pairs, CVE descriptions, static analysis, and direct stepwise reasoning ("Chain-of-Thought")—all aiming to probe or circumvent the LLMs' reasoning bottlenecks.

------------------------------------------------------------
5. OVERALL CONCLUSIONS & INSIGHTS
------------------------------------------------------------
- Prompting strategies tested—ranging from basic queries, in-context examples, instruction following, Chain-of-Thought, to bespoke domain annotation—were *not sufficient* to overcome systemic incapacity in current LLMs for semantic vulnerability reasoning.
- Improvements were present in some specific subtasks (e.g., identifying null-checks with annotation), but did not transfer to holistic vulnerability detection performance.
- The researchers emphasized that while prompt engineering can alleviate or diagnose *some* kinds of model errors, the primary limitations are architectural and training-based, not prompt-based.
- Future solutions likely require fundamental changes to modeling/training rather than further prompting tweaks.

------------------------------------------------------------

**Summary Table of Prompting Strategies**

| Prompting Strategy             | Description                                         | Outcomes/Effectiveness                 |
|-------------------------------|-----------------------------------------------------|----------------------------------------|
| Base/Instructional Prompt     | Direct question (vulnerable/not?)                   | Little to no improvement               |
| In-context/Few-shot           | Precede query with bug/fix examples                 | Minimal effect                         |
| Contrastive Pairs             | Ask to distinguish buggy vs. fixed code             | Most pairs “indistinguishable”         |
| CoT (Chain-of-Thought)        | Think step by step                                  | Helps illustrate, not fix, reasoning   |
| CoT-Annotations (with static) | Stepwise prompt with static analysis annotations    | Reduces certain errors, little gain in overall detection |

------------------------------------------------------------
END OF SUMMARY
------------------------------------------------------------

**References to Where Discussed in Paper**
- Table 2: Results for contrastive pairs (“can’t distinguish”)
- Section 3.3: Chain-of-Thought and domain annotation prompts; effects on reasoning and detection (Figures 8 and 9).
- Introduction to Section 4 and 5: Discussion of the limits and comprehensive summary of strategies.

---

You can copy and save this content to a `.txt` file as requested. If you need further granularity or excerpts of any specific prompt, please let me know!

--- Chunk Boundary ---

Certainly! Below is an organized summary answering your request. Please copy and save the content as a `.txt` file as needed.

---

PROMPTING STRATEGIES IN THE PAPER: SUMMARY AND ANALYSIS
========================================================

Overview
--------
The paper evaluates various prompting strategies for vulnerability detection using large language models (LLMs). These strategies are detailed in “APPENDIX A VULNERABILITY DETECTION PROMPTS,” with results and discussions sprinkled throughout the main text and appendices (Section 3.3, Section A, Section B, and results discussion).

Prompting Strategies Discussed
-----------------------------

1. **Basic (Zero-Shot) Prompting**

    - *Description:* A simple approach where the LLM is provided with a system prompt for context and a natural language query about code vulnerability, but no specific in-context examples or prior cases.
    
    - *Prompt Design:*  
        - System prompt: “I want you to act as a vulnerability detection system.”
        - Basic query: “Is the following function buggy? Please answer Yes or No.”
        - Variant: Tried “Is the following function vulnerable?” but found inferior performance.
        - CWE List: “Does the following function contain one of the following bug types?” followed by a list (e.g., “CWE-190: Integer Overflow”).
        - Q/A format: Begins with “Question:” in the prompt and expects the model to reply with “Answer:”, enforcing a QA exchange format.

    - *Location in Paper:* Appendix A, referencing Fu et al., 2023.
    
    - *Results and Outcome:*  
        - As expected, zero-shot prompting establishes a baseline.
        - The performance is notably weaker than strategies with richer context.
        - Variant queries (“vulnerable” vs. “buggy”) impacted accuracy, with “buggy” performing better.

2. **In-Context (n-shot) Prompting**

    - *Description:* Provides the model with one or more (n) examples of code and their vulnerability status before querying the target code sample, allowing the model to learn from format and content.

    - *Prompt Design:*  
        - Inclusion of input-response pairs.
        - Explored methods of in-context example selection:
            1. Random selection
            2. Retrieval-augmented generation (RAG) using CodeBERT embeddings to match most similar examples to the query.
            3. Contrastive pairs (see below).
        - Formatting: Compared all examples in one assistant message versus separate messages, finding better results with one message.
        - Varying n (number of examples); best results with n=6.

    - *Location in Paper:* Appendix A, referencing Brown et al., 2020; Liu et al., 2023b; Zhou et al., 2024; Xie & Min, 2022.
    
    - *Results and Outcome:*
        - In-context examples improved model performance over zero-shot.
        - Higher n (>1, especially n=6) yielded better performance.
        - Example selection method mattered: retrieval-based selection, using CodeBERT to find the most relevant code, usually outperformed random.
        - Single-message example presentation performed best.

3. **Contrastive Pair In-Context Prompting**

    - *Description:* In each in-context example, provides both the vulnerable (pre-bug-fix) and fixed (post-bug-fix) versions, highlighting the bug’s context and resolution.

    - *Prompt Design:*
        - Each pair contains the code “before” and “after” the bug fix with corresponding labels.
        - Intended to clarify subtle, but crucial, code differences responsible for vulnerabilities.

    - *Location in Paper:* Appendix A.
    
    - *Results and Outcome:*
        - This strategy helps models more effectively focus on relevant bug-inducing code changes.
        - Detailed results (including true positive/negative improvement) likely discussed in Section 3.3 (see: “CoT-Annotations prototype”).
        

4. **Chain-of-Thought (CoT) Prompting with Reasoning Derived from CVE Descriptions**

    - *Description:* Supplies intermediate reasoning steps in the prompt that lead to the vulnerability label, inspired by Wei et al., 2022. Examples drawn from Big-Vul dataset CVE bug reports.

    - *Prompt Design:*
        - Incorporates both code and a “reasoning path” (step-by-step analysis) per example.
        - For vulnerable code, uses the associated CVE bug report’s annotation and reasoning.

    - *Location in Paper:* Appendix A; chain-of-thought with references to Wei et al., 2022b.
    
    - *Results and Outcome:*
        - Generally, CoT prompting results in improved accuracy and interpretability, emphasizing the need for “reasoning-based” models (as discussed elsewhere in the paper, e.g., OpenAI o1 and DeepSeekR1).
        - The CoT-Annotation prototype demonstrated concrete performance gains (Section 3.3).

5. **Additional Context & Prompt Engineering Variants**

    - Explored multiple message arrangements (all examples in one message vs. separate chat messages).
    - Explored prompt engineering through phrasing analysis (“buggy” vs. “vulnerable”, QA formatting, explicit bug type lists, etc.).
    - Noted prompt phrasing sensitivity (see: “Isthefollowingfunctionbuggy?” yielded better results).

----------------

Results Summary
---------------

- **Zero-shot**: Useful baseline, but inferior to in-context and CoT strategies.
- **In-context (n-shot)**: Markedly better—retrieval-based selection and 6 examples per prompt yielded best results.
- **Contrastive pairs**: Helped model distinguish subtle, bug-causing differences in code.
- **CoT (chain-of-thought) prompts**: Outperformed others, especially when models were fine-tuned for reasoning (OpenAI o1, DeepSeekR1, etc.), improving not just core accuracy but also reducing context/understanding errors (e.g., missing NULL/bounds checks).
- **Prompt phrasing**: Precise wording (e.g., “buggy” vs “vulnerable”) impacts performance.

The **best overall results** were attained with a blend of the following:  
- Retrieval-based in-context prompting (n=6, all in one message)
- CoT annotations informed by CVE bug reasoning
- Thoughtful phrasing (“buggy?”) and explicit instructions
- Model choice: Reasoning-specialized models (OpenAI o1, DeepSeekR1, etc.)

All prompts, datasets, and code are openly available for reproducibility and further exploration (see “REPRODUCIBILITY STATEMENT” and link).

----------------

References to Where in the Paper
-------------------------------
- **Appendix A**: Primary details on prompt design and strategies.
- **Section 3.3**: Analysis and results with CoT-Annotations.
- **Section B**: Model IDs used.
- **Section F**: Error analysis methodology.
- **REPRODUCIBILITY STATEMENT**: Data/code availability URL for reproduction.

----------------

Key Insights
------------
- **Prompt quality and in-context example selection matter heavily.**
- **Fine-tuning models for reasoning (chain-of-thought) yields large gains.**
- **Contrastive and CoT-based in-context examples most beneficial.**
- **Prompt phrasing, explicitness, and message construction are all critical.**
- **Open datasets and transparent prompt sharing allow for robust replication and benchmarking.**

---

End of summary.

--- Chunk Boundary ---

Certainly! Below is a detailed and comprehensive answer to your query, elaborating on the prompting strategies discussed in the provided text, their context, experimental results, and all relevant specifics. You can copy-paste this into a .txt file for your needs.

---

prompting_strategies_summary.txt

---
**Prompting Strategies for Vulnerability Detection with LLMs: Analysis and Results**

**1. Overview of Prompting Strategies**

The paper explores multiple prompting strategies to improve Large Language Models' (LLMs) performance on vulnerability detection tasks. The focus notably lies on Chain-of-Thought (CoT) prompting, with two variations:

- **CoT-Annotation Prompting:** Custom, lightweight analysis targeting specific vulnerability semantics, and
- **CoT-Static Analysis Prompting:** Buggy execution paths (traces) generated by heavyweight commercial static analyzers, such as Infer.

Additionally, default in-context prompts with standard Q&A are used for non-buggy examples.

---

**2. Detailed Description of Prompting Strategies**

**A. Chain-of-Thought-Annotation Prompting (Section 3.3, Appendix A)**

- **Method:** Researchers adapt public bug report descriptions (e.g., CVEs) to serve as natural language explanations describing how a bug manifests. These descriptions include information on symptoms, attack surface, and relevant program variables.
- **Structure:** For buggy examples, the adapted natural language manifests as a "chain of thought" (CoT) that mirrors the logical reasoning a security analyst would use when reviewing code. Each explanation concludes with "Therefore, the example is buggy." For non-vulnerable examples, a default safe response is used.
- **Quality Control:** To ensure robust examples, they:
  - Removed duplicates,
  - Excluded examples with overly short/long code (50-750 tokens),
  - Retained examples tied specifically to the same vulnerability types,
  - Ensured all examples had sufficient static or bug report context.

**B. Chain-of-Thought-Static Analysis Prompting**

- **Method:** Utilizes the output of the Infer tool (a heavy static analysis engine), which generates a single "buggy path" for each sample. This path consists of a step-by-step statement trace leading to the vulnerability.
- **Structure:** The buggy path is translated to plain English, listing each relevant code step (e.g., variable allocation, index assignment, out-of-bounds access). The explanation ends with "Therefore, the example is buggy." Default responses are used for non-buggy examples.
- **Quality Measures:** The team:
  - Removed duplicates,
  - Enforced source code size constraints,
  - Required complete vulnerability proofs within the function.
  
**Contrasts Between CoT-Annotation vs. CoT-Static Analysis (Appendix A):**
- **CoT-Annotation** is more lightweight and customizable, targeting specific bug semantics (bounds, null checks) and tailored to known LLM pain points.
- **CoT-Static Analysis** is heavier, relying on commercially available analyzers, providing mechanical proof traces but lacking granularity/customizability.

---

**3. Results of Prompting Strategies**

**A. Overall Model Performance (Appendix D, Figure 11, Table 6):**
- The paper indicates that performance with CoT-style prompting remains far from perfect, with top models achieving around 65% accuracy on simple CWE examples.
- Default/non-CoT responses ("No, the function does not contain...") remain common for safe samples.

**Model Performance on Simple CWE Examples (Table 6):**
```
Model                   Performance (%)
----------------------  ---------------
GPT-4 (OpenAI,2024)          65.78
Gemini 1.0 Pro               50.87
GPT-3.5                      56.14
Mixtral-MoE                  61.40
CodeLLAMA                    61.40
LLAMA2                       46.49
WizardCoder                  51.75
DeepSeek-Coder               66.67
StarChat2                    55.26
StarCoder2                   50.87
StarChat                     50.00
StarCoder                    41.52
MagiCoder                    62.28
Mistral                      57.01
```

**Performance by Bug Type (Table 7):**
Per CWE family, performance fluctuates between 50 - 58%, indicating no single family was particularly "solved" by existing prompting/LLM approaches.

**B. Effectiveness and Shortcomings**

- While CoT techniques, both annotation-based and static analysis-based, provide richer context and reasoning than vanilla Q&A prompt formatting, LLMs still struggle:
  - Common errors include misunderstanding bounds/null checks, string operations, arithmetic, and order of execution.
  - Even with explicit chains-of-thought, LLMs frequently default to incorrect or superficial explanations, or fail to reason correctly about crucial vulnerability semantics.
- The study concludes both CoT prompt styles offer advantages (structured thinking, context) but do not reliably resolve the core reasoning and inference limitations of current LLMs, especially in subtle code-vulnerability scenarios.

---

**4. Specific Prompting Procedures and Implementation Details**

- Both strategies require filtering, data curation, and post-processing to prepare the in-context exemplars.
- Every buggy example is paired with a natural language, step-by-step explanation (inspired by bug reports or bug execution traces).
- Both methods end explanations with the phrase: "Therefore, the example is buggy."
- For negative examples, responses are standard Q&A indicating absence of known CWE vulnerabilities.
- The researchers relied on public datasets such as SVEN and D2A, and additional static analysis tools including Infer.
- They document all model IDs and text generation parameters (Appendix B, Table 5). For example, Top-p=0.9/1.0, Temperature=0.1, Max tokens=512.

---

**5. Summary Table (for quick reference)**

| Prompting Strategy      | Description                               | Source     | Customizable | Mechanical Proofs | Example Ending Text              |
|------------------------|-------------------------------------------|------------|--------------|-------------------|----------------------------------|
| CoT-Annotation         | Explains bug semantics (e.g., bounds)     | CVEs, SVEN | Yes          | No                | "Therefore, the example is buggy."|
| CoT-Static Analysis    | Steps from static analyzer buggy paths    | D2A, Infer | Somewhat     | Yes               | "Therefore, the example is buggy."|
| Default/Q&A            | Standard findings (no explanation)        | -          | No           | No                | e.g., "No, the function does not..."|

---

**6. Researcher Reflections and Limitations**

- The need for filtering and careful curation underscores label noise problems in vulnerability data.
- There's explicit recognition that neither prompt style fully overcomes LLMs' context and inference shortfalls.
- The richer, targeted CoT-Annotation style may help on specific vulnerability types, but is less scalable than automated static analysis traces.

---

**References to Prompt Explanations in the Paper:**
- Prompting strategies are described most thoroughly in **Appendix A** ("IN-CONTEXT EXAMPLES AND PROMPTING STRATEGIES").
- Performance analysis and results of prompting techniques appear in **Appendix D & E** ("SIMPLE CWE EXAMPLES", "PERFORMANCE BREAKDOWN BY BUG TYPE").
- Additional context for context length, model selection, and parameterization can be found in **Appendix B**.

---

**In Summary:**
The research implements two core prompt engineering strategies—custom CoT-Annotation, and static analysis-derived CoT—applied as in-context exemplars for vulnerability detection with LLMs. While richer prompting boosts interpretability, LLMs remain limited by deeper reasoning gaps, and model performance on these tasks hovers between 50-67% even with sophisticated prompting interventions.

---

END OF DOCUMENT
