Below is a detailed answer in plain text format, addressing your request on the prompting strategies discussed in "Prompt-Enhanced Software Vulnerability Detection Using ChatGPT" by Chenyuan Zhang et al.

---

Prompting Strategies in "Prompt-Enhanced Software Vulnerability Detection Using ChatGPT"

==================================================================================

1. Where Prompting Strategies Are Explained
--------------------------------------------

The explanation and experimentation on prompting strategies are centered in SECTION 4 ("Prompt Designs") and expanded in the Abstract, Introduction, and Results/Discussion sections. The relevant parts can be traced across the Abstract, Introduction, and especially Section 4, with evaluation outcomes in Section 5 ("Experimental Results").

2. Overview of the Researchers' Prompt Strategies
-------------------------------------------------

The authors systematically explore and refine prompting approaches for leveraging ChatGPT’s large language model capabilities in automated software vulnerability detection. They note that prior work used simple, generic prompts and so missed much of ChatGPT’s reasoning and dataset-specific potential.

In their investigation, the authors design and test various prompting strategies, including:

  A. **Basic Prompts**:  
     These prompts are straightforward queries (e.g., "Is the following code vulnerable?"). This is the baseline method, similar to strategies in related, earlier studies.

  B. **Improved Prompt Engineering**:  
     Various improvements are applied over the basic prompt. Here, the team explores different wording, context, and instructional detail to better direct ChatGPT to identify vulnerabilities.

  C. **Structural and Sequential Auxiliary Information**:  
     Prompts are augmented by incorporating higher-level code structure information, such as control-flow graphs (CFGs) and sequences, as part of the message provided to ChatGPT. This leverages established program analysis techniques combined with text prompts, aiming to inform ChatGPT of contextual patterns in the code.

  D. **Chain-of-Thought (CoT) Prompting / Multi-turn Dialogue**:  
     The researchers use ChatGPT’s ability to retain information across multiple conversational turns. In these strategies, they structure the prompt in a way that requires the model to reason through the problem step-by-step (the so-called "chain-of-thought"), for example, by asking clarifying or guiding questions about the code in stages, or simulating a multi-round human-LLM dialogue before a vulnerability determination is requested.

  E. **Cross-Language and Cross-Dataset Prompt Adaptation**:  
     The researchers test if prompts designed for one programming language (e.g., Java) or context can be adapted for another (e.g., C/C++).

3. Specifics of Researchers’ Prompt Strategies
-----------------------------------------------

**A. Basic Prompt Template:**
- Direct question to the model.
  ```
  Please examine the following code and determine whether it is vulnerable or not. Provide a brief explanation.
  <insert code snippet>
  ```

**B. Improved Prompt Templates:**
- The prompt might specify vulnerability types, clarify what constitutes a vulnerability, or ask for more detailed explanations. Example:
  ```
  Analyze the following function for potential security vulnerabilities, such as buffer overflows or SQL injection. Indicate whether the function is SAFE or UNSAFE and explain your reasoning.
  <insert code snippet>
  ```

**C. Structure-Enhanced Prompt:**
- Prompts include structured information about the code such as control-flow graphs, function call graphs, or other auxiliary information, e.g.:
  ```
  For the function below, here is its control-flow graph (CFG): [CFG details].
  Based on both the code and the CFG, judge if the function contains security vulnerabilities. Specify SAFE or UNSAFE and detail your reasoning.
  <insert code snippet>
  ```

**D. Chain-of-Thought Prompting (Multi-Round Dialogue):**
- Instead of asking for an immediate answer, the approach involves stepwise reasoning. The researcher may stage a dialogue or guide ChatGPT through a series of questions, like:
  1. "What does this function do?"
  2. "What are the possible inputs and their effects?"
  3. "Are there any parts of the code where user input is used in unsafe ways?"
  4. "Based on the above, is the code vulnerable? State SAFE or UNSAFE and explain why."
  This method often means giving feedback in-between and asking for deeper analysis.

4. Results of the Prompting Strategies
--------------------------------------

The effectiveness of different prompting strategies is analyzed experimentally in Section 5 of the paper, with key findings summarized as:

- **Basic prompts** perform adequately as a baseline but have significant room for improvement, sometimes leading to generic or incomplete answers, and lower accuracy in detecting real vulnerabilities.
- **Improved prompts** that are more specific/instructive demonstrated noticeable improvement over the basic prompts, especially when examples or clearer instructions were included.
- **Structural/Sequential Information**: Providing additional code context such as CFGs markedly enhanced ChatGPT’s performance. This is the first reported use of such classic program analysis-state information within an LLM-based prompt for vulnerability detection, and results showed a meaningful increase in detection accuracy compared to prompts with just code.
- **Chain-of-Thought/Multi-round prompts**: This strategy further improved detection accuracy. By guiding the LLM stepwise to reason about code semantics, vulnerable patterns, and consequences, the researchers observed that ChatGPT became more reliable and accurate, especially for more complex code samples.
- **Cross-language robustness**: Prompts (especially enhanced/structural ones) transferred adequately across Java and C datasets, showing some generalizability.

**Quantitative Results:**
- The paper reports the following (approximately, as per main text and summary figures):
    - Structural/sequential auxiliary information yields the largest relative improvement.
    - Chain-of-thought-style dialogue plus enhanced prompts achieves the best average performance (the specific numbers may vary per dataset, but improvements in accuracy and recall were generally between 5-15 percentage points over the basic prompt).
    - The combination of structural information and stepwise (CoT) prompting performed best overall across datasets.

**Qualitative Observations:**
- Prompts that are too brief or ambiguous lead to less insightful or sometimes incorrect LLM outputs.
- Prompts that are explicit, context-rich, and sequentially structured substantially help the LLM to "think through" the code security assessment more like a human analyst.

5. Summary Table (from the paper)
---------------------------------

| Prompt Type                     | Description                               | Effectiveness                      |
|----------------------------------|-------------------------------------------|-------------------------------------|
| Basic Prompt                     | Simple "Is it vulnerable?"                | Baseline, least accurate            |
| Improved Prompt                  | More instructive, lists vulnerability types | Moderate improvement                |
| Structure-Aware Prompt           | Includes code and control/structure info  | Significant improvement             |
| Chain-of-Thought/Multi-Turn      | Stepwise reasoning, multi-round           | Best accuracy and recall            |

6. Conclusions of Prompts' Success
-----------------------------------

The paper concludes that prompt engineering—especially via structural code information and chain-of-thought stepwise reasoning—materially improves ChatGPT’s ability to perform nuanced vulnerability detection. This supports the LLM's generalization, critical analysis, and outputs that align with expert human reasoning.

**Limitations**:
- LLMs occasionally "hallucinate" and may overfit to prompt phrasing; complex prompts also require more user expertise to construct.
- For unknown or very complex vulnerabilities, LLMs may still struggle despite improved prompting.

**Repository**:
All prompts and code used in experiments are available at: https://github.com/KDEGroup/LLMVulnerabilityDetection

---

END OF TEXT FILE

--- Chunk Boundary ---

Sure! The following is a detailed summary of the prompting strategies discussed in the text you provided, including where they are explained, their results/effectiveness, and all meaningful specifics of the researchers' prompt engineering approach. You can copy the following into a `.txt` file.

---

# Prompting Strategies for Vulnerability Detection Using ChatGPT

This summary reviews the prompt engineering techniques explored by researchers for enhancing software vulnerability detection by ChatGPT, as explained in the paper (sections 3.3, 4, and related discussions), and summarizes results on their effectiveness.

## 1. **Basic Prompting**

### **Description**
- The **basic prompt** (denoted as Pb) asks ChatGPT directly if a program is buggy:
  ```
  Is the following program buggy? Please answer Yes or No.
  [CODE]
  ```
  Here, `[CODE]` is replaced with the function-under-test.

- The researchers also devised a **role-based basic prompt** (Pr-b), reminding ChatGPT of its "role":
  ```
  I want you to act as a vulnerability detection system. My first request is “Is the following program buggy?” Please answer Yes or No. [CODE]
  ```
  This primes ChatGPT to focus on code vulnerabilities.

- To combat potential bias (since the prompt includes "buggy" as a keyword, which may skew ChatGPT's response toward "No"), a **reverse-question prompt** (Pr-r-b) asks:
  ```
  I want you to act as a vulnerability detection system. My first request is “Is the following program correct?” Please answer Yes or No. [CODE]
  ```
### **Where Explained**
- Section 4.1 "Basic Prompting"

### **Effectiveness/Results**
- The basic prompt helps set the context. However, prompts containing certain keywords might inadvertently bias the LLM’s answer.
- The role-based prompt offers improvement by focusing on the vulnerability detection context.
- The reverse-question is used as a sanity check for bias, though the results of this particular test are not quantified in the provided excerpt.

---

## 2. **Prompting with Auxiliary Information (API Calls, Data Flow, Program/Control Dependence)**

### **Description**
Researchers found that ChatGPT, when given code alone, has limitations in extracting:
- Data Flow Graph (DFG) information (e.g., where variable values originate).
- API call sequences (order and type of API calls).
- Program dependence (which parts of code depend on others for control or data).

**Auxiliary data extraction process:**
- Used tree-sitter to obtain ASTs for code.
- Extracted API call sequence and data flow as structured information, attached to prompts as descriptive text (see Section 3.3).

**Prompts with auxiliary information:**
- Formulated by modifying Pr-b and adding extracted data, creating:
  - **Pr-b-d**: Role-based prompt, Data Flow description appended after code.
  - **Pr-a-b**: Role-based prompt, API Call description BEFORE code.

Examples:

**Pr-b-d (Data Flow):**
```
I want you to act as a vulnerability detection system. I will provide you with the original program and the data flow information, and you will act upon them. Is the following program buggy?
[CODE]
[DFdescription]
```

**Pr-a-b (API Call):**
```
I want you to act as a vulnerability detection system. I will provide you with the original program and the API call sequence, and you will act upon them.
[APIdescription]
Is the following program buggy?
[CODE]
```
- **[DFdescription]** is structured as:
  ```
  The data value of the variable v_i at the p_i-th token comes from/is computed by the variable v_j at the p_j-th token.
  ```
- **[APIdescription]** is structured as:
  ```
  The program first calls a1, then calls a2, ..., and finally calls an.
  ```

### **Where Explained**
- Section 3.3 "Feature Extraction": Describes extraction process for API calls and data flow.
- Section 4.2 "Prompting with Auxiliary Information": Describes how auxiliary data is included in prompts.

### **Effectiveness/Results**
- The researchers ran tests on 20 randomly sampled functions from each dataset, asking ChatGPT to output data flow/control flow/program dependence based only on code. Results showed:
  - ChatGPT could **not accurately reconstruct data flow** or recognize API call order/depth, especially when flows are nontrivial.
  - The addition of explicit data flow ([DFdescription]) and API sequences ([APIdescription]) improved ChatGPT's understanding and presumably its downstream performance.
- For **control flow**, ChatGPT performed better and could identify execution paths.

- The **order of auxiliary information** (i.e., whether to put [APIdescription]/[DFdescription] before or after code) was experimented with. Placing them immediately after (for DFG) or before (for API calls) the code yielded the best performance.

---

## 3. **Chain-of-Thought (CoT) Prompting**

### **Description**
- Chain-of-Thought prompting encourages ChatGPT to process code in steps, providing intermediate "thoughts" or reasoning as it works toward the final answer, rather than jumping straight to "Yes" or "No".
- Though details about the precise template are not in the excerpt, in such prompts, the LLM is asked to "think step by step" before giving a judgment.

### **Where Explained**
- Section 4.3 "Chain-of-Thought Prompting"

### **Effectiveness/Results**
- CoT prompts help LLMs with complex tasks that require multiple reasoning steps, such as those in vulnerability detection.
- While precise numerical results aren't quoted in the provided excerpt, the implication is that CoT improves detection performance, especially on more subtle vulnerabilities.

---

## 4. **Key Results from Prompt Strategy Comparison**

### **General Observations**
- **Base/role-based prompts** provide a useful baseline but have limited performance, especially on complex vulnerabilities.
- ChatGPT cannot, from code alone, reliably synthesize functionally correct data flow or API call chains, especially for real-world projects (complex logic, function call nesting, etc.).
- **Enriching prompts** with **data flow** (DFG) and **API sequences** led to more accurate vulnerability detection. Explicit prompts help the LLM overcome limitations in static analysis.
- The **ordering** of how auxiliary information is presented in the prompt affects performance. "Best practices" involve placing data flow info close to the code snippet it describes.
- **Chain-of-Thought (CoT)** prompting further enhances performance by forcing the model to articulate its reasoning process.
- These strategies **reduce feature engineering cost**—no need for deep, model-specific feature pipelines, as the LLM can leverage structured natural language.

---

## 5. **Prompt Construction Specifics**

### **Data Pre-processing Notes**
- All function names like 'bad', 'good', 'VULN', 'PATCHED' are replaced with 'func' to avoid giving hints to the LLM.
- Duplicated code and functions under three lines in length are removed to improve data quality.
- For Java, vulnerable and patched pairs are paired for balance.

### **Auxiliary Information Extraction**
- **API call sequences**: Extracted using tree-sitter to parse ASTs. All constructor/function calls are mapped to a consistent natural language description.
- **Data flow graphs**: Custom extraction process identifies tuples of the form (variable, token position, origin variable, origin token position), mapped into template sentences.

---

## 6. **Baselines for Comparison**
- Compared ChatGPT (using these prompt strategies) to deep learning vulnerability detection baselines such as CFGNN and Bugram.

---

## **Summary Table of Prompt Strategies**

| Prompt       | Code Only? | Describes Job/Role? | Auxiliary Info (DFG/API)? | Reasoning Steps (CoT)? | Placement          | Experimentally Effective? |
|--------------|------------|---------------------|---------------------------|------------------------|--------------------|--------------------------|
| Pb           | Yes        | No                  | No                        | No                     | -                  | Baseline                 |
| Pr-b         | Yes        | Yes                 | No                        | No                     | -                  | Better than Pb           |
| Pr-r-b       | Yes        | Yes                 | No                        | No                     | -                  | Used to check bias       |
| Pr-b-d       | Yes        | Yes                 | Yes (DFG after code)      | No                     | After code         | YES                      |
| Pr-a-b       | Yes        | Yes                 | Yes (API before code)     | No                     | Before code        | YES                      |
| CoT          | Varies     | Yes (implied)       | Varies                    | Yes                    | -                  | YES (esp. complex cases) |

---

## **Conclusion**

- The most effective prompting strategies for ChatGPT-based vulnerability detection are those that combine:
  1. An explicit role-based setup ("act as a vulnerability detection system")
  2. Rigorous, templated **auxiliary information** about **data flow** and **API call sequences** derived from static analysis.
  3. The use of stepwise, chain-of-thought prompting where appropriate.
- Proper prompt engineering (choice of content, order, and specificity) substantially improved the model's ability to reason about vulnerabilities, on par or superior to traditional learning approaches requiring far more handcrafted features.

---

**End of summary**


--- Chunk Boundary ---

Below is a comprehensive summary of the prompting strategies discussed in the provided experimental research excerpt, including where they are explained, their results, their effectiveness (successes and failures), and all relevant prompt design specifics. This is formatted for inclusion in a `.txt` file.

---

prompting_strategies_chatgpt_vuln_detection.txt
------------------------------------------------------------

1. INTRODUCTION TO PROMPTING STRATEGIES

The study investigates multiple prompting strategies to assess ChatGPT's effectiveness at software vulnerability detection. The main focus is on "basic" prompting, role-based prompting, chain-of-thought prompting, and prompts enhanced with auxiliary information (API calls, data flow, and code summaries). The researchers compare these strategies against baseline models CFGNN and Bugram.

2. PROMPTING STRATEGIES

A. Basic Prompting (Sec. 4.1)
----------------------------------
- **Description**: The most straightforward approach, directly instructing ChatGPT to act as a vulnerability detector and judge code as "buggy" or "not buggy".
- **Standard Basic Prompt (Pb)**:  
  - "I want you to act as a vulnerability detection system. Is the above program buggy? Please answer Yes or No."
- **Reverse Basic Prompt (Pr-r-b)**:  
  - Replaces "buggy" with "correct" to assess prediction bias.

B. Role-Based Prompting
---------------------------
- **Description**: Specifies an explicit role for the model, based on OpenAI documentation that system messages specifying persona could affect replies.
- **Role-Based Prompt (Pr-b)**:  
  - Adds a system message—ChatGPT is told to act as a vulnerability detector.
- **Reverse Role-Based Prompt (Pr-r-b)**:  
  - Simulates asking if the code is "correct" rather than "buggy".

C. Auxiliary Information in Prompts (Sec. 4.2.2)
-----------------------------------------------------
- **Description**: Enhances prompts with information like API call sequences ([APIdescription]) and dataflow ([DFdescription]).
- **API Call Enhanced Prompt (Pr-a-b)**:  
- **Data Flow Enhanced Prompt (Pr-b-d)**:  
  - These prompts inject either API call summaries or data-flow summaries before or after the code.

D. Chain-of-Thought Prompting (Sec. 4.3)
--------------------------------------------
- **Description**: Involves multi-step prompting—in Step 1 the LLM summarizes the intent/purpose of the code, and in Step 2 it makes a vulnerability judgment.
- **Chain-of-Thought Sequence**:
    - Step 1: "Please describe the intent of the given code. [CODE]"
    - Step 2: (using prior output/context): "I want you to act as a vulnerability detection system. Is the above code buggy? Please answer Yes or No."
- **Variants**:
  - With and without additional API/data-flow info provided at Step 2.

E. Varying Position of Auxiliary Information (Sec. 5.5)
----------------------------------------------------------
- **Description**: Researchers tested prompts by placing API/data-flow information before ([POS1]) or after ([POS2]) the code snippet in the prompt to test sensitivity.

F. Adding Code Summary as Auxiliary Info (Sec. 5.4.2)
---------------------------------------------------------
- **Description**: Instead of a two-step chain, the code summary from Step 1 is included as auxiliary information in a single prompt (like API/data-flow info).

3. RESULTS OF PROMPTING STRATEGIES

A. Basic and Role-Based Prompts
------------------------------------
- **Table 2 (Key Results):**
  - On Java data, basic prompt accuracy (Pb) was 0.691—significantly outperforming CFGNN (0.437) and Bugram (0.436).
  - Role-based prompt (Pr-b) yielded a 5% improvement in accuracy over Pb in Java (0.725 vs. 0.691).
  - For C/C++, Pr-b and Pb both achieved similar accuracy (~0.52–0.53), which outperformed the baselines but less dramatically.
  - Bias: When the prompt used "buggy", ChatGPT predicted most functions as vulnerable; when using "correct", it predicted most as non-vulnerable (Pr-r-b).

**Finding**: Including a task role can enhance performance for Java, but effects are language-specific. Basic prompts outperform baselines significantly for Java, modestly for C/C++.

B. Prompts with Auxiliary Information
------------------------------------------
- **Accuracy** (Table 2):
  - For Java, Pr-a-b achieved the highest accuracy among all basic/role-based prompts (0.747), an 8% boost over Pr-b.
  - For C/C++, adding data-flow info (Pr-b-d) didn't yield significant gains over the basic prompt.
- **Finding**: API call info most benefits Java, data-flow info slightly helps C/C++.

C. Chain-of-Thought Prompting Results
-----------------------------------------
- **Table 3 (Key Results):**
  - For C/C++, chain-of-thought achieved notable improvements; e.g., P(chain)2,r-b accuracy was 0.741 (up from ~0.52 in basic prompts).
  - For Java, chain-of-thought provided no improvement or slight degradation (0.701 for best chain-of-thought vs 0.747 for best basic).
  - Adding API or data-flow info in chain-of-thought reduced vulnerable detection but improved non-vulnerable detection.
- **Finding**: Chain-of-thought aids C/C++ significantly, less so (even harms) for Java.

D. Quality of Code Summarization and Detection Explanations
------------------------------------------------------------------
- Human ratings for code summaries (Step 1 in chain-of-thought):  
  - ~90% rated 4 or higher (on a 5-point scale)—ChatGPT understood code intent well.
- Human ratings for vulnerability explanations:  
  - Only 52% (C/C++) and 64% (Java) rated 4 or 5—ChatGPT sometimes claims to detect vulnerabilities it doesn’t properly understand.

**Finding**: ChatGPT capably describes code intent, but its vulnerability reasoning can be shallow.

E. Position of Auxiliary Information in Prompts (Table 5)
-------------------------------------------------------------
- Placing [API description] before code ([POS1]) gives better accuracy for both basic and role-based prompts.
- For Pr-a-b (API before code), accuracy peaked at 0.747 (Java); for Pr-b-a (API after code), it was 0.675.
- Data-flow info order less affected results.

**Finding**: Prompt ordering affects accuracy, with auxiliary info before code being superior.

F. Having Code Summaries as Auxiliary Input (Table 4)
----------------------------------------------------------
- For Java, code summary as auxiliary decreased accuracy (e.g., to 0.657 from 0.691 for Pb).
- For C/C++, slight performance boost (~0.526 from 0.523).

**Finding**: High-quality code summaries help C/C++, not Java.

G. Vulnerability Type Detection (Fig. 4)
------------------------------------------
- ChatGPT achieved 100% accuracy on 7 vulnerability types, >50% on 41 out of 50 types.
- Types with clear logic/boundary errors (e.g., Off-by-one, Logic Bombs) easiest.
- Some types still misclassified.

**Finding**: ChatGPT covers many vulnerability types but misses subtler bugs.

4. SUMMARY OF STRATEGY SUCCESS AND SPECIFICS

- **Most successful strategies:**
    - Basic/role-based prompting for Java (Pr-a-b best).
    - Chain-of-thought prompting for C/C++ (P(chain)2,r-b best).
- **Auxiliary information**:
    - API info boosts Java detection, especially before code.
    - Data-flow info only slightly helps C/C++.
- **Ordering matters**: Placing contextual info before the code snippet in prompts produces better results.
- **Chain-of-thought**:
    - Increases ability to comprehend and explain vulnerabilities in C/C++.
    - For Java, chain-of-thought can degrade raw prediction accuracy compared to one-shot with auxiliary info.
- **Bias risk**: Prompt wording (buggy/correct) leads to prediction bias (overcalling vulnerable or non-vulnerable).
- **Human evaluation**: ChatGPT forms plausible code intent summaries, but vulnerability explanations are sometimes shallow or incorrect.
- **Vulnerability type coverage**: ChatGPT can detect a wide range, with higher success for types with clear logic flaws.

5. REPRESENTATIVE PROMPT FORMULATIONS
------------------------------------------------
- "Please describe the intent of the given code. [CODE]."
- "I want you to act as a vulnerability detection system. Is the above program buggy? Please answer Yes or No."
- "I want you to act as a vulnerability detection system. Is the above code buggy? Only answer Yes or No. Here is its API call sequence/data flow information that you may use: [APIdescription]/[DFdescription]."
- Variants where [APIdescription]/[DFdescription] is placed before or after [CODE].

------------------------------------------------------------
END OF FILE

--- Chunk Boundary ---

Certainly! Below is a detailed explanation **formatted as plain text** of the prompting strategies discussed in the provided section of the paper. This includes where the strategies are explained, their results, how successful they were, and all notable specifics. You can copy/paste this into a `.txt` file as needed.

---

# Prompting Strategies in Prompt-Enhanced Vulnerability Detection Using ChatGPT

## Overview

The paper examines the use of prompt engineering to enhance ChatGPT's performance in software vulnerability detection. Specifically, it investigates the effects of including **auxiliary information** (API calls and dataflow info) in prompts, their **order of presentation** in the prompt, and the utility of **chain-of-thought prompting**. The study conducts experiments using Java and C/C++ vulnerability datasets.

---

## 1. Where Prompting Strategies are Discussed

Prompting strategies are discussed throughout the results and findings sections. The explanation and evaluation are predominantly in:

- **Section 5.6: Performance on Different Vulnerability Types (RQ5)**
- **Findings 9 & 10, following analysis of accuracy on various prompt formulations**
- **Earlier Results/Discussion sections** (not fully included here, but referenced)
- General comments in the **Conclusion** and **Threats to Validity** sections

---

## 2. Types of Prompting Strategies Used

The researchers explore several key variables in prompt construction:

### a. Inclusion of Auxiliary Information

- **API Call Information:** Providing details about the API calls used in the code sample.
- **Dataflow Information:** Providing information about the control and data flow within the code.

### b. Positioning of Auxiliary Information

- The order in which API calls and dataflow information are presented is varied:
    - **[POS1]:** Placing API calls or dataflow info before the code
    - **[POS2]:** Placing information after the code

### c. Chain-of-Thought Prompting

- Involves prompting ChatGPT to articulate step-wise reasoning when making a vulnerability determination, rather than predicting directly from code alone.

---

## 3. Specifics of the Prompting Strategies

**Empirical prompting strategies tested:**
- **Code Only Prompt:** Only the raw code is presented.
- **Code + API Calls Prompt:** The code is provided together with a listing or summary of API calls utilized. Order of presentation is varied.
- **Code + Dataflow Prompt:** The code augmented by dataflow or control flow exposition.
- **Code + API Calls + Dataflow:** Incorporates code, API information, and dataflow details, with varied ordering.
- **Chain-of-Thought:** Prompts ChatGPT to generate intermediate reasoning steps or explanations for its decision.

**Example of positioning experiment:**
- Present API calls before code, or after
- Present dataflow before code, or after

---

## 4. Results and Effectiveness

### a. Influence of Position

- **Main finding:** _"Placing API calls before the code and placing dataflow information after the code yields the best performance."_
    - **Prompting can achieve better performance when placing API calls before the code and dataflow information after.**
    - The optimal position may depend on the type of auxiliary information.

### b. Contribution of Information Types

- **API call details** help most with correctly predicting _non-vulnerable_ samples.
- **Dataflow information** is more effective in correctly identifying _vulnerable_ samples.

### c. Chain-of-Thought

- This strategy improved explanations and reasoning, but specific details about its quantitative impact are not provided in the snippet ("chain-of-thought" is mentioned as part of the general methodology).

### d. Accuracy by Vulnerability Type

- Some vulnerability types (e.g., CWE-759/328: cryptographic issues requiring domain knowledge) are difficult for ChatGPT to recognize, regardless of prompt enhancements.
- Grammar-related or boundary-related vulnerabilities (e.g., issues with buffer boundaries or control flow) see **notable improvements** from prompt engineering.
- For vulnerabilities not directly inferable from the context (e.g., suspicious comments, subtle semantic bugs), prompt enhancement is **less effective**.

### e. Quantitative Examples

- **CWE-759 (Use of One-Way Hash without Salt):** 0% correct detection, even with enhanced prompts—problem requires external knowledge/rules.
- **CWE-328 (Use of Weak Hash):** Improved, but still low at 41.1% accuracy.
- **CWE-482 (Comparing Instead of Assigning):** ChatGPT performs poorly because the mistake (using `==` instead of `=`) is not a grammatical error; current models cannot distinguish via context alone.
- **CWE-546 (Suspicious Comment):** Not detected, as code comments are not processed by default.

---

## 5. Conclusions and Lessons Learned

- Prompt engineering can **significantly improve ChatGPT's performance** on certain software vulnerability detection tasks, especially when:
    - API calls are presented before code
    - Dataflow is presented after code
- However, **not all vulnerabilities benefit**; those requiring external rules or understanding of intended logic remain challenging for ChatGPT.
- **Takeaway:** Prompt engineering is a powerful, though not universal, tool for improving LLM-based code analysis.

---

## 6. Threats to Validity

- Results may differ with future versions of ChatGPT or different LLMs.
- The effectiveness of strategies depends on:
    - Programming language evaluated
    - Dataset composition (types and number of vulnerabilities)
    - Human annotator expertise in evaluating outputs

---

## 7. References (for Further Reading)

Key papers on prompt engineering, software vulnerability detection, and LLMs are cited at the end of the excerpt.

---

# Summary Table

| Prompt Strategy                     | Description                                | Result/Outcome                                              |
|-------------------------------------|--------------------------------------------|-------------------------------------------------------------|
| Code-only                          | Raw code                                   | Baseline                                                    |
| API Calls before code               | API info presented first                   | Best accuracy for non-vulnerable samples                    |
| Dataflow after code                 | Dataflow info presented after code          | Best accuracy for vulnerable samples                        |
| API Calls after code, Dataflow before| Varied order                              | Lower accuracy than optimal configuration                   |
| Chain-of-thought prompting          | Request stepwise reasoning                 | Improved explanations, some cases better decisions          |
| Code + comments                     | Comments included                          | Not effective unless comments processed purposefully        |

---

# End of Document

You can now save or format this information as needed for your reference or reporting.

---

--- Chunk Boundary ---

Certainly! Below is an analysis based on the citation context you provided. Since your text contains multiple references on the use of prompting and prompt engineering for code generation, vulnerability detection, and evaluation of large language models (LLMs) like ChatGPT, I have synthesized the most probable prompting strategies discussed, highlighting sources, details, results, and success.

I've formatted the output as a coherent text file, but if you would like an actual downloadable file, let me know.

---

**Prompting Strategies for Software Vulnerability Detection Using Large Language Models: Analysis and Effectiveness**

---

**1. Overview of Prompting Strategies**

Prompting strategies refer to the methods and techniques used to elicit productive, accurate, or insightful outputs from large language models (LLMs) like ChatGPT, GPT-3, etc. In the context of software vulnerability detection and code analysis, prompting strategies are designed to improve code generation, bug detection, and vulnerability discovery.

**Key References Explaining Prompting Strategies:**
- Liu, P. et al., 2023. *Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in NLP* [40]
- Liu, C. et al., 2023. *Improving ChatGPT Prompt for Code Generation* [38]
- White, J. et al., 2023. *ChatGPT Prompt Patterns for Improving Code Quality* [70]
- Wei, J. et al., 2022. *Chain-of-Thought Prompting Elicits Reasoning* [68]
- Zhang, J. et al., 2023. *Detecting Condition-Related Bugs with Control Flow Graph Neural Network* [72]
- Lin, G. et al., 2021. Software Vulnerability Discovery via Learning Multi-Domain Knowledge Bases [34]

---

**2. Explanation of Prompting Strategies**

Prompting strategies in the surveyed research can generally be categorized as follows:

**a. Zero-shot Prompting:**
- The LLM is given just the code and a brief instruction, e.g.:  
  "Identify if there is a vulnerability in this code snippet."

**b. Few-shot Prompting:**
- The LLM is shown multiple examples of code with annotations (e.g., labeled as 'vulnerable' or 'safe'), followed by a target code snippet for evaluation.
- This is discussed in [39] and [40], with emphasis on quality and diversity of examples influencing performance.

**c. Chain-of-Thought (CoT) Prompting:**
- The model is encouraged to reason step by step, e.g.,  
  "Let's think step by step. Examine the code for common buffer overflow patterns..."
- Wei et al. [68] provide evidence that this improves reasoning for complex tasks and is increasingly used in vulnerability detection.

**d. Prompt Engineering/Refinement:**
- Iterative adjustment of prompt wording, formatting, and context to elicit better responses.
- Liu et al. [38] propose guidelines to improve prompt specificity and clarity when asking ChatGPT to write or assess code, e.g., by specifying the programming language, asking for security checks, or requesting an explanation.
- White et al. [70] list prompt patterns, such as requesting "Refactor this code for security best practices" or "Generate unit tests to cover edge cases."

**e. Automated/Template-based Prompt Generation:**
- Systems may use templates to automatically generate prompts tailored to specific vulnerabilities or project types [35, 36, 40].

---

**3. Where These Strategies Are Explained**

- Prompt engineering strategies are systematically surveyed in [40] (Liu et al., 2023), which discusses strategies for NLP and extends to code generation and analysis.
- [38] (Liu, C. et al., 2023) directly examines prompt formulation for code tasks with ChatGPT.
- Empirical studies on effectiveness (e.g., bug detection/auto-fixing) using LLMs and prompts can be found in [60] (Sobania et al., 2023).

---

**4. Results and Effectiveness**

**Effectiveness of Strategies:**
- **Few-shot prompting** strongly outperforms zero-shot prompting for code understanding and vulnerability detection [39, 40].
- **Chain-of-thought prompting** significantly increases the accuracy of complex reasoning tasks, including security analysis, by making internal reasoning explicit and thus surfacing LLM capabilities and limitations [68].
- **Prompt refinement/engineering** improves both the syntactic correctness and practical applicability of code snippets (e.g., mitigation of common security issues) [38, 70].
- Prompt patterns (e.g., ask for code with comments, ask for input validation) increase the likelihood that LLMs generate safer or more robust code [70].
- However, as [60] (Sobania et al., 2023) find, LLM-based bug fixing via automatic prompting is limited; it can miss context, misinterpret code purpose, or fail with subtle vulnerabilities. Success is notably higher with clearer and more targeted prompts.

**Metric examples:**
- ChatGPT and similar LLMs, with well-designed prompts, achieve competitive recall in vulnerability detection tasks, but with occasional hallucinations or over-predictions.
- Some benchmarks report 10-20% improvement in detection over basic prompts by carefully calibrating examples and explicit instructions [38, 39].
- Automated bug fixing rates range from 25-50% correct fixes depending on prompt quality and bug type [60].

**Limitations Noted:**
- Prompting alone does not guarantee vulnerability detection, particularly for complex semantic bugs [79].
- Excessively verbose or ambiguous prompts can confuse models.
- LLMs may generate plausible but incorrect answers if the prompt is underspecified.

---

**5. Specifics of Researchers’ Prompt Strategies**

- Examples are used that explicitly classify code as safe/unsafe (few-shot).
- Prompt templates are customized for vulnerability type (e.g., "Is there a buffer overflow in this code? Why or why not?").
- Iterative prompting: Start with a generic prompt, then progressively refine to focus the model's attention (e.g., "Check for SQL injection," then "Check the user input sanitization in this function").
- Step-by-step prompting: Request models to output reasoned explanations rather than just labels.
- Prompt patterns emphasize language and structure (asking for code, then explanation, then unit tests).

---

**6. Summary Table**

| Strategy       | Description                                            | Results                     | Success Factors           |
|----------------|-------------------------------------------------------|-----------------------------|---------------------------|
| Zero-shot      | Direct question, no examples                          | Baseline                    | Prompt clarity            |
| Few-shot       | Add labeled code examples in prompt                   | +10-20% detection           | Example diversity         |
| CoT            | Force step-by-step reasoning                          | +10-30% reasoning tasks     | Explicit instructions     |
| Prompt Eng.    | Refined prompt wording, template use                  | More correct, secure code   | Specificity, context      |
| Auto/Template  | Automated prompt gen. for code/vuln type              | Variable                    | Template adaptability     |

---

**References for Prompts and Evaluation**

- [38] Chao Liu et al. (2023) – Improving prompt instructions, clarity.
- [40] Pengfei Liu et al. (2023) – Comprehensive survey of prompt design.
- [39] Jiachang Liu et al. (2022) – Impact of in-context examples.
- [68] Jason Wei et al. (2022) – Chain-of-thought prompting increase reasoning accuracy.
- [60] Dominik Sobania et al. (2023) – Analytical results on bug fixing with LLMs.

---

**Conclusion**

Prompting strategies are critical to harnessing LLMs for software vulnerability detection and code improvement. Few-shot learning, stepwise reasoning (CoT), and prompt engineering (refinement) consistently lead to higher-quality outputs than naive prompting. However, effectiveness depends on clear prompt design, relevant context, and awareness of LLM limitations.

---

**END OF FILE**