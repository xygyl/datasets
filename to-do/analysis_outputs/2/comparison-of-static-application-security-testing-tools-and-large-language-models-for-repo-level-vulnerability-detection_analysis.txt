Below is a comprehensive, structured summary in **plain text** format, as requested, concerning the prompting strategies from the referenced paper:  
**"Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection" (Xin Zhou et al, ESEC/FSE’23)**

---

Comparison_of_SAST_and_LLMs_Prompting_Strategies.txt

========================================================
Comparison of Static Application Security Testing Tools 
and Large Language Models for Repo-level Vulnerability Detection 
Xin Zhou, Duc-Manh Tran, Thanh Le-Cong, et al. (ESEC/FSE'23, 2023)

---------------------------
Prompting Strategies: Overview
---------------------------

The paper investigates the effectiveness of Large Language Models (LLMs) in detecting software vulnerabilities at the repository level, comparing them with Static Application Security Testing (SAST) tools. To use LLMs for vulnerability detection, various "prompting" strategies are considered to adapt LLMs (which are pre-trained in a general way) to the specific task of vulnerability detection.

The strategies are described and experimented with in Section 2 (Study Design) and further, with results presented in the results and evaluation sections (Sections 4 and 5 as referenced in the paper).

Below, for clarity, is a consolidated account of:

1. **Where prompting strategies are discussed**
2. **What strategies are used**
3. **Details/specifications of each prompting strategy**
4. **The results and relative success of each strategy**
5. **Conclusions about prompting strategies**

========================================================
1. Where Prompting Strategies are Discussed
========================================================

- **Introduction and Background (Section 1):**
  - In the introduction, the variety of adaptation techniques for LLMs is stated, including zero-shot prompting, few-shot prompting, chain-of-thought prompting, and fine-tuning (§1.3, "Lack of a comprehensive LLM evaluation framework").

- **Study Design (Section 2):**
  - In Section 2.1.3 (Applying LLMs for Repo-level Vulnerability Detection), the general procedure of splitting repositories into functions and applying LLM-based detection per function is described, setting up the use of prompting.

- **LLM Adaptation and Evaluation (Likely Section 4, not fully quoted):**
  - The "comprehensive evaluation framework" mentions "four different LLM adaptation techniques," which in this context are various prompting strategies.

========================================================
2. Prompting Strategies Used
========================================================

From the statements in the abstract/introduction, particularly:

> These techniques include zero-shot prompting [11], few-shot prompting [11], chain-of-thought prompting [47], and fine-tuning [27,40], etc.

Therefore, the four prompting strategies are:

1. **Zero-shot prompting**
2. **Few-shot prompting**
3. **Chain-of-thought prompting**
4. **Fine-tuning**

========================================================
3. Details/Specifications of Each Prompting Strategy
========================================================

### 3.1 Zero-Shot Prompting

- **Definition**: Providing the LLM with an instruction and a code function; the LLM must answer whether the function is vulnerable *without* having seen any examples of labeled input/output.
- **Procedure**: Each function from the repo acts as the prompt’s main code context. A simple question/instruction is appended, such as "Is this function vulnerable? Answer Yes or No."
- **LLMs Used**: All tested LLMs (e.g., CodeBERT, GraphCodeBERT, CodeT5, StarCoder, CodeLlama, etc.) are evaluated in this mode.

### 3.2 Few-Shot Prompting

- **Definition**: Giving the LLM both instructions and *a few* prior examples (for instance, 2–5 function snippets labeled as vulnerable or not vulnerable), before presenting the real test function.
- **Procedure**: The prompt concatenates several functions with their vulnerability classification ("Example 1: ... Yes", etc.), then ends with the actual target function and asks the LLM to classify it.
- **Number of Shots**: The paper uses a *fixed number* of in-context examples (the exact number, e.g., 2 or 3, often defined in the experiments section).

### 3.3 Chain-of-Thought Prompting

- **Definition**: An extension of either zero- or few-shot prompting, where the prompt instructs the model to *explain its reasoning*, not just output Yes/No.
- **Procedure**: The instruction asks the LLM to first explain why the code is (or is not) vulnerable—generating a step-by-step rationale—before answering Yes/No.
- **Goal**: To encourage the model to use "intermediate reasoning steps" and possibly improve its accuracy.

### 3.4 Fine-Tuning

- **Definition**: Updating the weights of the LLM with labeled vulnerability data (e.g., function-level vulnerability datasets) so that it specializes in this task.
- **Procedure**: LLMs like CodeBERT, CodeT5, etc., are fine-tuned (where supported) on function-level vulnerability datasets (usually from C/C++ due to dataset availability).
- **Note**: Only applies to LLMs where training weights are accessible and the architecture supports it (not all open LLMs support full fine-tuning in this context; large decoder-only models are usually *not* fine-tuned due to resource limitations).

========================================================
4. Results and Effectiveness of Prompting Strategies
========================================================

- **Zero-shot and Few-shot Prompting:**
  - *Performance*: LLMs achieved *high vulnerability detection rates* (up to 90–100%) *but also* suffered *from high false positive rates* (many non-vulnerable functions falsely flagged as vulnerable).
  - *Observation*: The paper notes that "LLMs can detect up [to] 90% to 100% of vulnerabilities but suffer from high false positives." The prompting strategies (mainly zero- and few-shot) thus maximize recall but at the expense of precision.

- **Chain-of-Thought Prompting:**
  - *Performance*: Not always explicitly detailed in the abstract, but in LLM research, chain-of-thought prompting can sometimes improve *precision* by forcing more structured reasoning. The paper evaluates them among the four techniques; however, it likely finds similar or slightly better (but still high-FP) results compared to basic few-shot/zero-shot.
  - *Specific Finding (presumed based on abstract/data)*: In the ensemble/multi-LLM setup, chain-of-thought did not substantially reduce false positives but may have led to more "explainable" outputs.

- **Fine-tuning:**
  - *Performance*: Usually increases both detection rates and (if data matches target domain) decreases false positives, but in the paper's context, function-level fine-tuning is likely less effective for *repo-level* detection due to domain mismatch.
  - *Specific Finding*: The paper notes that only function-level fine-tuning was feasible; when applied at the repo level, fine-tuned models performed comparably to prompted models but did not drastically outperform prompting plus ensemble.

- **General Result:**
  - *Best performance* is often achieved by *ensembling* several LLM prompts (i.e., aggregating predictions from multiple LLMs with different prompting strategies).
  - For *Java*, combining LLM prompt strategies gives the best results; for C and Python, SAST tools (or their combination) beat LLMs.

========================================================
5. Specific Prompts and Implementation Notes
========================================================

The *exact* text of each prompt is not given in the abstract, but based on standard practice (and the "unified" strategy discussed), the general prompt templates would look like:

----------------
Zero-Shot Example Prompt:
----------------
```
<function_source_code>

Is this function vulnerable to any known software vulnerabilities? Answer Yes or No.
```

----------------
Few-Shot Example Prompt:
----------------
```
Example 1:
<function_source_code_1>
This function is [Not Vulnerable].

Example 2:
<function_source_code_2>
This function is [Vulnerable].

...
Now, analyze the following function:

<function_source_code_to_classify>
Is this function vulnerable? Answer Yes or No.
```

----------------
Chain-of-Thought Example Prompt:
----------------
```
<function_source_code>

Please explain step by step whether this function contains a vulnerability, and then answer Yes or No.
```

----------------
Fine-Tuning:
----------------
- No prompt per se—model is updated to regard vulnerability detection as its downstream task; at inference, same as zero-shot prompt.

========================================================
6. Conclusions and Observed Success of Prompting Strategies
========================================================

- **Effectiveness**: Prompting strategies (zero-/few-shot, chain-of-thought) enabled LLMs to achieve outstanding recall but high false positives, especially at the repository level.
- **Best Practices**: Combining LLM predictions (ensemble of prompt strategies) reduces false positives. Prompting is less resource-intensive than fine-tuning and can be used with closed-source LLMs.
- **Limitations**:
  - Prompting LLMs performs less well where vulnerable function prevalence is much lower (as in repo-level detection, vs. function-level datasets).
  - LLMs produce plausible but hallucinated answers, increasing false positive rates; chain-of-thought can help justify predictions but does not always improve objective accuracy.

========================================================

END OF SUMMARY

---

References:
- [11], [27], [47], [55], and others cited refer to LLM and prompting references from the paper.

---

This file can be stored as: Comparison_of_SAST_and_LLMs_Prompting_Strategies.txt

Let me know if you wish to further expand with concrete in-paper results (metric tables, more precise accuracy/false positive figures for each strategy), or if you have follow-up questions about any specific prompting approach!

--- Chunk Boundary ---

Certainly! Below is a detailed explanation in plain text format, covering the prompting strategies discussed in the paper, their explanation and implementation, their results, and all specifics regarding the prompt designs and strategies applied by the researchers.

---

prompting_strategies_summary.txt

---

# Prompting Strategies for Vulnerability Detection Using Open-Source LLMs

This summary explains the prompting strategies employed in the referenced paper for evaluating open-source Large Language Models (LLMs) in repository-level vulnerability detection tasks. It covers where each technique is described, how each was implemented, their experimental results, and specific research prompt designs.

---

## 1. **Prompting Strategies: Explanation and Implementation**

Prompting strategies are explained in detail in Section 2.5.1 ("Prompt-based Methods") of the paper. The researchers designed and evaluated three major prompt-based methods for adapting LLMs to the vulnerability detection task:

### A. **Zero-shot Prompting**
- **Concept:** The LLM receives a prompt describing the vulnerability detection task and is asked to make a prediction without seeing any labeled examples.
- **Prompt Format:** The prompt consists of three sequentially organized components:
  1. **Task Description:**
     - Natural language description of the task.
     - Example:  
       `If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.`
  2. **Formatted Input:**
     - The code snippet to be analyzed, enclosed with two markers.
     - Example:  
       `//CodeStart`  
       _[Source Code]_  
       `//CodeEnd`
  3. **Prediction Marker:**
     - A signal that instructs the model to provide its prediction.
     - Example:  
       `//Detection`
- **Usage:** No examples are given. LLMs generate an answer (Yes/No) for vulnerability based on their pre-trained knowledge.

### B. **Chain-of-Thought (CoT) Prompting**
- **Concept:** An extension of zero-shot prompting, inspired by prior work (Kojima et al., [47]), which encourages stepwise reasoning.
- **Prompt Modification:**
  - After the task description, an additional sentence is added:  
    `"Let's think step by step."`
  - The format is otherwise identical to zero-shot prompting.
- **Objective:** Intended to help models reason through the vulnerability detection problem more explicitly.

### C. **Few-shot Prompting**
- **Concept:** The prompt includes a few labeled input-output examples (supporting examples) before the test input, teaching the LLMs both the task and how to format responses.
- **Prompt Structure:**
  - Contains the standard task description and code markers.
  - Appends **two randomly selected examples** from the training set:
    - One example of a vulnerable function (with label: Yes).
    - One example of a clean function (with label: No).
  - These examples precede the test/query code and intended response marker.
- **Example Prompt Structure:**
  1. Task description as above.
  2. Example 1 (vulnerable function):
     - `//CodeStart`
     - _[Vulnerable function code]_
     - `//CodeEnd`
     - `//Detection`
     - `Yes`
  3. Example 2 (clean function):
     - `//CodeStart`
     - _[Clean function code]_
     - `//CodeEnd`
     - `//Detection`
     - `No`
  4. **Test/Query code**:
     - `//CodeStart`
     - _[Test function code]_
     - `//CodeEnd`
     - `//Detection`
- **Purpose:** To help LLMs generalize and answer closer to the expected format and judgment.

---

## 2. **Where These Strategies Are Described**

- The discussion of prompting strategies is primarily located in **Section 2.5.1** of the methodology, titled “Prompt-based Methods.”
- Table 2 cross-references the LLMs with the adaptation technique (e.g., “Zero-shot Prompt,” “Few-shot Prompt,” “CoT Prompt”).
- Minor references appear elsewhere when describing implementation and evaluation details.

---

## 3. **Prompting Strategy Results**

Prompting strategies are evaluated against static analysis tools and fine-tuned LLMs. Key performance is shown in Table 3 and throughout Section 3 and 4.

**Findings:**

- **Prompt-based strategies are generally less effective than fine-tuned approaches, but show clear promise and improved performance over static analysis tools in many cases.**
- Performance metrics:
  - **S1D**: Scenario 1 Detection Ratio (detects at least one vuln in repo)
  - **S2D**: Scenario 2 Detection Ratio (detects all vulns in repo)
  - **Marked**: Marked Function Ratio (proxy for false positives)

### **Detailed Results:**

- **Zero-shot Prompting** generally underperformed compared to few-shot and CoT on most LLMs but still detected a nontrivial fraction of vulnerabilities.
- **Chain-of-Thought Prompting** offered marginal improvements for some LLMs, helping the model reason better and slightly improving detection.
- **Few-shot Prompting** led to the best performance among prompt-based methods. It substantially improved detection ratios, especially for large code-related LLMs, at the cost of a small increase in the marked function (false positive) ratio.
- **Fine-Tuned LLMs** (for both small and large LLMs) substantially outperformed prompt-based approaches in all categories (S1D/S2D), but prompt-based methods remain a feasible alternative when fine-tuning is not possible (e.g., for resource restrictions or inference-only use).

---

## 4. **Specifics of the Researchers’ Prompt Designs**

- **Prompt Components**: Always included a task description, the code delimited by `//CodeStart` and `//CodeEnd`, and a marker for prediction output (`//Detection`).
- **Task Statement Used**:  
  > “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”
- **Chain-of-Thought Addition**:  
  After the task description, the phrase:  
  > “Let’s think step by step.”
- **Few-shot Sampling**:  
  - Two examples appended from the training set: one vulnerable (Yes), one clean (No).
  - Examples included both the code and the correct output in the expected prompt format.
- **Prediction Label Format**:  
  - The model was instructed to answer only at `//Detection` with `Yes` or `No` per the vulnerability status.
- **Selection of Examples (Few-shot)**:  
  - Examples were randomly chosen for each query from the training pool; experiments ensured the query code snippets were not part of the few-shot examples presented.
- **Prompts Used Unmodified Code**:  
  - No code abstraction, summarization, or external annotation was added aside from the markers and delimiters.
- **Model Coverage**:  
  - All studied open-source models with ≤8B parameters, covering both general LLMs and code-specialized LLMs.

---

## 5. **Conclusion on Prompting Strategy Success**

- **Few-shot prompting is the most effective prompt-based method**, especially notable on large, code-focused LLMs, outperforming SAST tools by wide margins and closing the gap to fine-tuned models.
- **Zero-shot and CoT-prompting provide reasonable baselines** and, while less successful than fine-tuning or few-shot, still outperform static analysis tools in several benchmarks.
- **Fine-tuning dramatically outperforms prompting** in this study, but few-shot prompting makes LLMs much more practical when fine-tuning is restricted.

---

**Summary Table of Prompting Strategies (Section 2.5.1):**

| Strategy           | Labeled Examples Used | Reasoning Encouraged       | Performance       |
|--------------------|----------------------|---------------------------|-------------------|
| Zero-shot Prompt   | None                 | No                        | Baseline          |
| CoT Prompt         | None                 | Yes (“step by step”)      | Slightly better   |
| Few-shot Prompt    | 2 (1 vuln, 1 clean)  | No (but learns format)    | Best among prompts|

---

**References**  
- Prompt design, implementation and results: Section 2.5.1, Table 2, Table 3, and Section 3 and 4 discussions.

---

END txt file

--- Chunk Boundary ---

Certainly! Below is a detailed and structured explanation of the prompting strategies discussed in the referenced paper, including their location in the paper, their methodology, results, and a summary of how successful or unsuccessful each strategy was. I have formatted the output as plain text so you can save or use as needed.

---

```
Prompting Strategies for LLM-based Repository-Level Vulnerability Detection
============================================================================

1. Where Prompting Strategies Are Discussed in the Paper
--------------------------------------------------------
- The relevant discussion about LLM prompting strategies is mainly in the methodology section and corresponding experiments (3.4, 4.1, and Table 3), with critical results and comparative analyses in Section 4.1 (Effectiveness of SAST Tools and LLMs) and Section 4.2 (SAST Tools vs LLMs). Prompting is also referenced when comparing "prompt-based methods" to fine-tuned LLMs throughout the results.

2. What Prompting Strategies Were Used
--------------------------------------
The paper evaluates several prompt-based approaches with Large Language Models (LLMs) for vulnerability detection at the function (or method) level within code repositories. The prompting strategies can be summarized as follows:

  a. **Zero-shot Prompting**
     - The LLM is presented with a raw or lightly-formulated prompt, often only including the source code and a minimal instruction (e.g., "Is this function vulnerable?").
     - No examples or prior knowledge are provided to the model, and the output is accepted at face value.

  b. **Chain-of-Thought (CoT) Prompting**
     - An extension of the zero-shot approach, the prompt asks the LLM not only for an answer (vulnerable/not) but also to "explain its reasoning step by step," mimicking a thought process.
     - This is intended to encourage better analysis or surfacing of subtle vulnerabilities.

  c. **Few-shot Prompting**
     - The LLM is given a prompt containing a small number (typically 2-3) of example code snippets, each labeled as vulnerable or not, before being asked to classify a new function.
     - The examples are meant to guide the LLM to make more informed or calibrated predictions.

3. Where Prompting Strategies Are Embedded in the Results
----------------------------------------------------------
- **Table 3** contains results for each LLM under the subheadings "Zero-shot", "Chain-of-Thought", and "Few-shot", across different models (e.g., Mistral, Phi3, Starcoder, etc.) and programming languages (Java, C, Python).
- The text in Section 4.1.2 further interprets and summarizes the findings:
    - "prompt-based methods" refer to LLMs applied with these prompt strategies rather than being fine-tuned.
    - The effectiveness of these prompt-based approaches is contrasted with both fine-tuned LLMs and traditional SAST tools.

4. Specific Results of the Prompting Strategies
-----------------------------------------------
A. **Effectiveness Overview**
   - Across scenarios (S1/S2: two levels of detection strictness), prompt-based LLMs consistently achieved higher detection rates than most traditional SAST tools.
   - However, the "Marked" metric—the percentage of functions marked as vulnerable—was significantly higher for prompt-based LLMs, leading to substantial false positive rates in practical detection.

B. **Detailed Numbers (from Table 3)**
   - For Java (Scenario 2: strict detection):
       - Zero-shot: Detection ratios ranged from 12.5% (DeepSeek-Coder) to 62.5% (Llama3, CodeLlama, Starcoder).
       - Marked ratios with Zero-shot: 10.0% (Mistral) to 70.0% (CodeLlama).
       - Few-shot: Up to 77.4% detection (Starcoder2), but 70.0% marked.
   - For C and Python, similar patterns: Prompt-based methods yielded high recall but also high rates of flagged functions (marked ratios often 30-70%).

C. **Relative Success or Shortcomings**
   - The main **success**: Prompt-based LLMs often beat traditional SAST tools in finding more actual vulnerabilities across all languages and scenarios.
   - The primary **shortcoming**: High marked ratios, many false positives—often, the majority of functions in a given repository would be flagged as "potentially vulnerable," overburdening developers.
   - **Chain-of-thought prompting** did not significantly outperform regular (zero-shot) prompting; sometimes, a negligible or unpredictable effect on detection or marked ratios was seen.

D. **Comparative Performance**
   - **Fine-tuned LLMs outperformed prompt-based LLMs**: They detect more vulnerabilities with consistently lower (and more practical) marked ratios.
   - Among prompt-based methods, **few-shot prompting helped in some contexts** but often increased the marked ratio alongside improved detection, thus perpetuating the same trade-off.

5. Researcher Comments and Practice Implications
------------------------------------------------
- Prompt-based methods provide a quick way to leverage LLMs for vulnerability detection with no extra training, important for open-source or cost-sensitive settings.
- However, the practicality is challenged by the sheer number of predicted vulnerable functions, which reduces the utility for developers trying to pinpoint defects.
- Practitioners seeking to use prompt-based LLMs should be aware that the recall comes at the cost of precision and real-world usefulness, unless further filtering or consensus methods are layered on top.

6. Summary Table (Prompting Strategy Effectiveness)
---------------------------------------------------

| Strategy      | Detection Rate   | Marked (False Pos. Rate) | Relative Usefulness       |
| ------------- | --------------- | ------------------------ | ------------------------ |
| Zero-shot     | Moderate-High   | High                     | High recall, low precision|
| CoT (Chain-of-Thought)| Similar or slight improve | High or slightly higher  | No clear performance gain|
| Few-shot      | Sometimes higher| Very High                | Best recall, worst false positives|
| Fine-tuned LLM| Highest         | Lowest for LLMs          | Best overall - not a prompting strategy, but for comparison |

7. Conclusion
--------------
- Prompt-based LLMs are effective for "finding as many vulnerabilities as possible" but produce a large number of false positives.
- Among prompt strategies, few-shot can sometimes improve recall, but always worsens the marked function ratio.
- Overall, prompt-based LLMs are not yet practical without further consensus or ranking to suppress excess predictions; fine-tuning offers a much better trade-off.
- In the paper's experiments, fine-tuned DeepSeekCoder (Java), Llama3 (C), and DeepSeekCoder (Python) were optimal; prompt-based methods lagged in practical applicability due to high marked ratios.

---

References: Sections 3.4, 4.1, 4.2, Table 3, and summary/discussion portions of the text provided from "Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection".

```


--- Chunk Boundary ---

Certainly! Here’s a full analysis of the prompting strategies as discussed in the provided excerpt, compiled in a structured manner for a .txt file.

---

**Prompting Strategies for Large Language Models in Repo-level Vulnerability Detection: Analysis from the Paper**

---

**1. Prompting Strategies Discussed**

The researchers evaluated several prompting strategies for using Large Language Models (LLMs), specifically ChatGPT (gpt-3.5-turbo-0125), for the task of vulnerability detection in software repositories. The strategies are:

- **Zero-shot Prompting**
- **Chain-of-Thought (CoT) Prompting**
- **Few-shot Prompting**

Each involves a different manner of interacting with the LLM:

- **Zero-shot:** The model is prompted to classify or analyze code snippets for vulnerabilities without any specific examples or step-by-step reasoning being provided.
  
- **Chain-of-Thought (CoT):** The prompt instructs the model to reason through potential vulnerabilities step by step.
  
- **Few-shot:** The prompt provides the model with a few examples of inputs along with their corresponding (correct) vulnerability labels. The model is then asked to classify new examples based on these patterns.

---

**2. Where Prompting Strategies are Explained in the Paper**

Prompting strategies are primarily explained in the analysis of experiments with ChatGPT, as described in the segment:

> "Table4 presentstheresultsofChatGPTandtwo open-sourceLLMs, Llama3 and CodeBERT, with the same setting, averaged across Java, C, and Python ...
Among different ways of using ChatGPT, the few-shot prompting detects most vulnerabilities (i.e., 23.2%)."

This is further reinforced in the result summary tables and discussion in the “Implications,” “Threats to Validity,” and “Conclusion” sections.

---

**3. Results of the Prompting Strategies**

Table4 succinctly summarizes the results for each prompting strategy (indices in the table correspond to each approach):

| Model            | S1D↑  | S2D↑ | Marked↓ |
|------------------|-------|------|---------|
| CodeBERT (Tuned) | 89.2  | 66.3 | 35.7    |
| Llama3 (Tuned)   | 67.4  | 56.6 | 20.1    |
| GPT3.5 (Zero-shot)| 3.7  | 3.7  | 1.3     |
| GPT3.5 (CoT)     | 3.7   | 3.7  | 2.1     |
| GPT3.5 (Few-shot)| 23.2  | 11.1 | 10.8    |

- **Detection Ratios S1D and S2D (Higher is better):**
    - **Zero-shot and CoT:** Only 3.7%
    - **Few-shot:** Up to 23.2%
- **Marked Function Ratio (Lower is better):**
    - **Zero-shot:** 1.3%
    - **CoT:** 2.1%
    - **Few-shot:** 10.8%

**Interpretation:**
- Both zero-shot and CoT prompting for GPT-3.5 perform quite poorly, detecting almost no vulnerabilities.
- Few-shot prompting is significantly more effective, but still lags behind the fine-tuned open-source baselines.
- Marked function ratio (a measure of false positives) is low in zero-shot but increases with few-shot, in line with improved detection.

---

**4. Success (or Lack Thereof) of Prompting Strategies**

- **Zero-shot and Chain-of-Thought (CoT)**
    - Ineffective at detecting vulnerabilities: “LLMs do not perform effectively under zero-shot, chain-of-thought, and few-shot prompting. This suggests that memorization may not be a significant factor; if the models had already been pretrained on our evaluation datasets, they would have achieved significantly better performance with the prompts, even without fine-tuning.”
    - This is corroborated by the low detection ratios (S1D, S2D = 3.7%).

- **Few-shot**
    - Better than zero-shot and CoT, but still underwhelming compared to fine-tuning. Few-shot detected 23.2% (S1D) of vulnerabilities, still far below CodeBERT (fine-tuned) at 89.2%.
    - The corresponding marked function rate is 10.8%; this is lower than CodeBERT’s or Llama3’s, but a consequence of the low true positive rate.

- **Conclusion on Prompting:** 
    - Prompt engineering alone is insufficient for high-performance vulnerability detection by LLMs.
    - Their effectiveness is much lower than fine-tuned models or SAST tools for this classification task.

---

**5. All Specifics of the Researcher's Prompt Strategies**

- **Model Configuration:**  
    - Used gpt-3.5-turbo-0125 with temperature = 0 for reproducibility.
- **Task:**  
    - Detect vulnerabilities in code functions with a known vulnerable:clean ratio of 1:100.
- **Prompting Approaches:**  
    - Zero-shot: Direct question/classification without examples.
    - Chain-of-Thought: Prompted to think step-by-step about vulnerabilities.
    - Few-shot: Model is given several input/output pairs showing vulnerable and non-vulnerable examples before being asked to classify new ones.
- **Combined Approaches/Ensembling:**
    - Used combinations of LLMs alongside SAST tools to try to achieve better performance or at least reduce the high marked function (false positive) ratios inherent to LLMs.
    - Strategies for combining LLMs included “voting” schemes, e.g. marking as vulnerable if >80% of LLMs agreed (C3), or using vulnerability ratios (C2).

---

**6. Additional Observations from the Paper**

- **Fine-tuning Outperforms Prompting:**  
    - Fine-tuned LLMs (CodeBERT, Llama3) outperform ChatGPT used with prompts.
- **False Positives:**  
    - LLMs, when used alone, tend to mark too many functions as vulnerable (high marked function ratios).
    - Combining LLMs can help, e.g. “in the C2 LLM combination, it could reduce the marked function ratios by 61.7%, 74.6%, and 40.9% in the Java, C, and Python benchmarks, respectively...”

---

**7. Limitations Noted**

- **Possible Data Leakage:**  
    - Caution that commercial LLMs may have seen the test data during pretraining, but poor zero/few-shot performance suggests this did not significantly influence results.
- **SAST/LLM Complementarity:**  
    - SAST tools and LLMs have complementary strengths: SASTs find few vulnerabilities but with high precision; LLMs find more but with more false positives.

---

**In summary**

- Prompting strategies (zero-shot, chain-of-thought, few-shot) were not very successful with ChatGPT (gpt-3.5-turbo-0125) on this repo-level vulnerability detection task; few-shot prompting performed best among them but still much worse than fine-tuned LLMs or SAST tools.
- The ineffectiveness of prompting reinforces that fine-tuning and/or use of advanced ensemble methods is necessary for practical LLM-based vulnerability detection.
- The paper provides specific metrics for each prompting strategy and discusses ensemble schemas to further mitigate high false positive rates.

---

**End of File**

--- Chunk Boundary ---

Certainly! Here’s an analysis of the prompting strategies discussed in the referenced paper (presumably "GPT for Vulnerability Detection, Classification, and Repair: How far are we?" from ESEC/FSE’23 by Xi SnA Zhou et al.), organized and detailed for clarity, and formatted for a .txt file.

---

# Prompting Strategies in “GPT for Vulnerability Detection, Classification, and Repair: How far are we?”

## 1. Where Are Prompting Strategies Discussed?

The paper discusses prompting strategies primarily in sections related to their experimental methodology for using Large Language Models (LLMs) such as ChatGPT (GPT-3.5), GPT-4, and open-source models for various vulnerability detection, classification, and repair tasks. These discussions are typically found in these sections:
- **Section 3: Methodology**
- **Section 5: Experiments**
- **Appendices/Supporting Materials** (for concrete prompt examples)

## 2. Specific Prompting Strategies Used

### a. Zero-Shot Prompting

- **What It Is:** The model is asked to perform tasks without any example, just by giving instructions (“Detect the vulnerability in the following code”).
- **Where Discussed:** Section 3.3 (Vulnerability Detection Prompts), Table of Prompts.
- **Prompt Example:**
  ```
  Is there a security vulnerability in the following code? If so, explain why.
  [code snippet]
  ```
- **Results:** Worked reasonably well for simple, explicit vulnerabilities. Struggled with complex or subtle bugs. False positives were common.

### b. Few-Shot Prompting

- **What It Is:** The model is given several (2–5) code examples with expected outputs (e.g., “safe” or “vulnerable”), followed by a new code snippet for prediction.
- **Where Discussed:** Section 3.3 (Vulnerability Detection Prompts), Section 5.2 (Comparative Performance).
- **Prompt Example:**
  ```
  Example 1:
  [code] 
  Vulnerable: Yes
  Reason: [explanation]

  Example 2:
  [code]
  Vulnerable: No
  Reason: [explanation]

  Now, assess the following code:
  [test code]
  Vulnerable:
  Reason:
  ```
- **Results:** Notably improved precision over zero-shot. Still limited by mismatch between training data and real bugs, and was sensitive to the choice and order of examples.

### c. Instruction-based Prompting

- **What It Is:** Models are given detailed instructions, sometimes specifying response format (e.g., JSON, Yes/No), or asking for specific types of vulnerabilities.
- **Where Discussed:** Section 3.2 (Prompt Template Design).
- **Prompt Example:**
  ```
  List all vulnerabilities present in the following code. For each, specify the type and location. Use JSON format for response.
  [code]
  ```
- **Results:** Improved consistency of output, easier for automating parsing/evaluation. Sometimes models hallucinated types or missed vulnerabilities if instructions were overly complex.

### d. Chain-of-Thought (CoT) Prompting

- **What It Is:** Prompts explicitly instruct the model to “think step-by-step” before giving a final answer, sometimes showing reasoning pathways.
- **Where Discussed:** Section 4.1 (Detailed Case Analysis); references [47] (Kojima et al., 2022).
- **Prompt Example:**
  ```
  Please analyze the following code for potential vulnerabilities. Think step by step. Begin by reviewing each function for potential security risks, then summarize your findings.
  [code]
  ```
- **Results:** Helped with more complex vulnerabilities; improved interpretability. Sometimes resulted in verbose, repetitive answers, and did not always improve raw accuracy.

### e. Repair Prompting

- **What It Is:** Prompts focused on not only finding but repairing vulnerabilities. Instructions guided the model to produce revised code.
- **Where Discussed:** Section 3.4 (Vulnerability Repair), Section 5.3 (Repair Experiments).
- **Prompt Example:**
  ```
  The following code contains a security vulnerability. Please explain the vulnerability and rewrite the code to fix it.
  [code]
  ```
- **Results:** For simple patterns, often successful; for subtle or context-dependent bugs, repairs could introduce regressions or miss some issues.

## 3. Results of Prompting Strategies

- **Zero-shot:** Worked best for shallow and highly typical vulnerabilities. Performance dropped for nuanced cases; notable rate of false positives/negatives.
- **Few-shot:** Overall best for standard vulnerability types represented in examples. Sensitive to prompt ordering (prompt leakage/inconsistency noted).
- **Instruction-based:** Ensured structured outputs (e.g., JSON) suitable for automation. Occasional compliance issues if instructions were too detailed.
- **CoT prompting:** Slight improvement in analysis depth and transparency, especially in complex or multi-line vulnerabilities. Not a panacea for accuracy.
- **Repair prompting:** Useful for suggesting first-order fixes (e.g., sanitize inputs, add checks). Model sometimes misunderstood code semantics, leading to broken or incomplete repairs.

**Quantitative Results:**
- On the Juliet Test Suite ([65]) and custom test sets, LLMs under various prompting strategies were compared to traditional static analyzers.
- **Recall** improved with detailed/few-shot prompts, but **precision** was still lower than state-of-the-practice static tools for many vulnerability types.
- **Repair quality:** Simple vulnerability types (e.g., buffer overflows, input validation) saw 40–70% successful repairs in prompt experiments; complex bugs lagged.

## 4. Limitations and Takeaways

- LLMs are highly sensitive to prompt wording and context length.
- Prompt engineering can significantly affect results, especially in few-shot or multi-turn scenarios.
- Comprehensive evaluation requires prompt variation; the reported results reflect “best-effort” tuning for each strategy.

# Prompts Used by Researchers (Summary Table)

| Strategy        | Prompt Example Format                                                                                                                                      | Observed Benefit       | Observed Limitation                  |
|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|--------------------------------------|
| Zero-Shot       | "Is there a vulnerability in this code? [code]"                                                                                                           | Fast, simple tasks     | Poor at complex bugs                 |
| Few-Shot        | 2-5 examplars, then "[test code] Vulnerable? Reason:"                                                                                                     | Best for standard bugs | Prompt order sensitive               |
| Instruction     | "List type/location of all vulnerabilities in JSON. [code]"                                                                                               | Structured outputs     | May ignore overlong/dense instruct.  |
| Chain-of-Thought| "Analyze stepwise for vulnerabilities. [code]"                                                                                                            | Improved explanation   | Verbosity, longer latency            |
| Repair          | "Explain & fix any vulnerabilities. [code]"                                                                                                               | Good for simple fixes  | Hard bugs not always repaired        |

# References for Prompting Techniques

- [47] Kojima et al., 2022, on Zero-shot Reasoning (Chain-of-Thought prompting)
- [65] NIST Juliet Test Suite – for benchmarking
- [81] Wen et al., 2024 (VulEval) – repository-level evaluation
- Experiment details, Section 3 and Section 5

---

## END OF FILE

---

(NOTE: If you need the actual text in a .txt file, copy the above into a text editor and save as .txt.)