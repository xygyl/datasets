Prompting Strategies in LLMSAN (Sanitizing Large Language Models in Bug Detection with Data-Flow)

1. Few-Shot Chain-of-Thought Prompting (Section 3, Figure 4) 
   Template:
     Program: [Example Program with a DBZ bug]
     Explanation: v is assigned with 0 at line 3. Then u is initialized with v at line 7 and used as a divisor at line 9, causing a DBZ bug.
     Dataflow path: [(v, ℓ1), (u, ℓ2), (u, ℓ3)]
   Purpose: Instructs the LLM to emit data-flow paths as rigorous proofs of bugs.

2. Functionality Sanitizer Prompt (Section 4.1, Figure 5) 
   Template:
     Task: Analyze the code: [function]. Please check whether [value] at line [line number] can be zero. Please think step by step and conclude the answer with Yes or No.
     Examples: Here are several examples containing DBZ bugs: [examples]. Please understand how zero values are produced.
   Purpose: Validates that the origin of the data-flow path semantically produces the specified faulty value.

3. Reachability Sanitizer Prompt (Section 4.2, Figure 6) 
   Template:
     Task: Analyze [function1], [function2]. Please check whether [value1] at line [n1] can be propagated to [value2] at line [n2]. Please think step by step and conclude the answer with Yes or No.
     Examples: There are examples containing DBZ bugs: [examples]. Please understand how program values are propagated.
   Purpose: Ensures that each step in the data-flow path is reachable under program semantics.


Results of Prompting Strategies:

Few-Shot Chain-of-Thought Prompting (baseline FSCoT):
- Precision: 69.04%, Recall: 74.40%.

Sanitization Prompts:
- Type Sanitizer: pruned syntactic inconsistencies without sacrificing valid paths.
- Functionality Sanitizer: pruned semantic invalid start/end values; e.g., for NPD bugs pruned 32 of 46 spurious paths (69.6%).
- Order Sanitizer: removed control-flow order violations; pruned 8 of 46 spurious paths (17.4%).
- Reachability Sanitizer: eliminated unreachable data-flow facts; pruned 26 of 46 spurious paths (56.5%).

Overall Performance (LLMSAN with gpt-4):
- Precision: 91.03%, Recall: 74.00% (vs FSCoT’s 69.04%/74.40%).
- Average spurious path pruning rate: 78.24%.

Real-World Evaluation (TaintBench, gpt-4):
- Sanitizers pruned 86.6–94.2% of spurious paths across models.
- LLMSAN achieved 44.04% precision and surpassed CodeFuseQuery by +15.36pp precision and +3.61pp recall.
