{
  "filename": "exploring-the-influence-of-prompts-in-llms-for-security-related-tasks.pdf",
  "analysis": "Certainly! Here are answers, broken into points as requested, and including references to text sections, tables, and the specifics of strategies and results.\n\n---\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point / Purpose:**  \nThe paper investigates *how the design of prompts*—specifically *prompt structure* and *prompt content*—affects the performance of large language models (LLMs) on security-related tasks such as security patch detection, vulnerability identification, and patch classification. It aims to systematically analyze the contribution of each prompt component and provide guidance for prompt engineering in security scenarios.\n\n**Exploration:**  \n- How different types of prompt structures (e.g. zero-shot, one-shot, few-shot—and variants involving positive/negative examples) and different types of content (e.g. role instructions, emotions, GPT-generated prompts, etc.) influence LLM output.\n- Whether *combining* these prompt contents adds further benefit.\n\n**Uniqueness:**  \n- Focus on *security-related* tasks, which often combine natural language, code snippets, and specialized terminology.\n- Comprehensive, empirical analysis across multiple tasks, using several prompt design strategies and contents.\n- Evaluation of interaction effects, including combinations of prompt contents, and systematic statistical error estimation.\n\n---\n### 2. What dataset did they use?\n\n**Datasets:**  \nThree publicly available, large, labeled datasets, each corresponding to a type of security-related task:\n\n- **PatchDB** ([29]; ~12,000 security and 24,000 non-security patches)  \n   - Used for Security Patch Detection (SPD)\n- **Devign** ([35]; ~48,687 C/C++ functions labeled as vulnerable/non-vulnerable)  \n   - Used for Vulnerable Function Detection (VFD)\n- **PatchNet** ([14]; ~82,403 stable and non-stable patches)  \n   - Used for Stable Patch Classification (SPC)\n\nSee \"II. Approach Overview, A. Security-Related Datasets Selection\".\n\n**Sampling:**  \nFor cost and consistency, 2000 samples from each dataset (1000 positive, 1000 negative) were randomly selected for experiments.\n\n---\n### 3. What LLM models were used?\n\n- **GPT-3.5-Turbo** (via API, by OpenAI)  \n  - Chosen for accessibility, known capabilities, high API throughput, and cost-effectiveness.\n\nSee \"III. Evaluation, A. Experiment setup, Model selection\".\n\n---\n### 4. What are the prompting strategies discussed in this paper?\n\n#### Prompting strategies are divided by:\n**A. Prompt Structure** (Section II.B, Table II et al.)\n- **Zero-shot**: Only task description + input query.\n- **One-shot**: Includes task description + one example (positive: t or negative: f).\n- **Few-shot**: Task description + multiple examples.\n- *Variants based on type/order of examples included:*\n   - 1-shot-t: one positive example\n   - 1-shot-f: one negative example\n   - few-shot-tt: two positive examples\n   - few-shot-ff: two negative examples\n   - few-shot-tf: one positive, then one negative\n   - few-shot-ft: one negative, then one positive\n\n**B. Prompt Content** (Section II.C, Table I, Table II)\n- **Basic**: Minimal instruction (task statement).\n- **GPT-generated**: LLM (ChatGPT) generates its own prompt for the task.\n- **Role**: System prompt changes (e.g., \"You are an AI expert in patch analysis\").\n- **Act As**: User or system is told to \"Act as\" an expert; appears in input or system message.\n- **Emotion**: Prompt includes emotional framing:\n    - Encourage (\"you're the best...\", motivational)\n    - Threaten (\"you are the worst if you don't...\", cautionary)\n    - Reward (\"If you do well, you'll be tipped\")\n    - Punish (\"If you do poorly, you'll be fined\")\n\n**C. Combination:**  \nCombining different content types, e.g., Role+GPT-generated, Emotion+GPT-generated (Section D, Table V).\n\n---\n### 5. What specific prompts were used?\n\nSee Table I. Examples include:\n\n#### For Security Patch Detection (SPD):\n- **Basic:**  \n  `<User> Decide whether a patch is a security patch (SRP) or non-security patch (NSP).`\n- **GPT-generated:**  \n  `<User> Let's start by examining the patch notes or changelog for key terms... Then, we'll scrutinize the code changes... Finally, let's check for... Help me categorize the patch to whether to Security-Related Patch (SRP) or a Non-Security Patch (NSP).`\n- **Role:**  \n  `<System> You are Frederick, an AI expert in patch analysis. Your task is to decide...`\n- **ActAs-User:**  \n  `<User> You are Frederick, I want you to act as an AI expert in patch analysis. You will decide...`\n- **Encourage:**  \n  Add `\"Remember, you’re the best AI patch analyst and will use your expertise...\"`\n\nSimilarly for other categories (Threaten, Reward, Punish)—see Table I for full examples.\n\n---\n### 6. Where are the prompt strategies explained?\n\n- **Prompt Structure**: Section II.B \"Prompt Structure\"\n- **Prompt Content**: Section II.C \"Prompt Content\"—with Table I \"Prompt Content Example for SPD\".\n- **Combinations of Content**: Section D (end of Approach Overview), Table V.\n\n---\n### 7. What are the results of the prompting strategies and how successful (or not) were they?\n\n#### a) **Prompt Structure Results** (see Tables II–IV)\n- For *Security Patch Detection* and *Stable Patch Classification*, **few-shot-ff** (two negative-class examples) structure led to the highest increases in accuracy, recall, and F1.\n    - Example: In SPD, accuracy improved from baseline 0.411 to 0.602, recall 0.122 to 0.447.\n- For *precision*, sometimes **1-shot-t** (positive example) was superior.\n- For *Vulnerable Function Detection*, **few-shot/few-shot-ff** was less effective; **0-shot** or **GPT-generated** were sometimes better.\n\n#### b) **Prompt Content Results** (see Tables II–IV)\n- For SPD, **Punish** and **Encourage** contents usually led to higher recall and F1, sometimes outpacing the basic prompt.\n- For SPC, changing prompt content often made little difference—sometimes even *worse* with more elaborate content (especially for GPT-generated prompts).\n- For VFD, sometimes the **basic** content was already optimal, and emotional or combined prompts didn't help.\n\n#### c) **Combinations of Content** (Table V)\n- Combining two prompt contents rarely improved results: in only ~9.7% of cases did a combination outperform both singles.\n- 52% of the time, combinations fell between the performance of each single content; rest, combinations were worse.\n\n#### d) **Quantitative Improvement** (Summary in Introduction and Discussion)\n- Across three security tasks, tuning prompt structure and content shifted accuracy by **13%–30%**.  \n   - Example: On SPD, accuracy ranged from 41.1% to 60.2% depending on prompt choice.\n\n---\n### 8. Why did one strategy give better results than another?\n\n- **Few-shot-ff** structures: Including two negative-class examples reduced false negatives and improved detection for tasks (SPD, SPC) where negative instances (non-security patches) were more prevalent or easier for the LLM to recognize (likely reflecting class imbalances in training data).\n- **One-shot-t** (Precision): Presenting a positive-class example helps the model focus on catching positives, thus upping precision even if recall suffers.\n- **0-shot** is sometimes better if:\n    - The domain (like VFD) involves diverse or numerous subtypes (e.g., vulnerability types) where specific examples might bias or confuse the LLM.\n- **Prompt Content**: Emotional/role prompts (Encourage, Punish, etc.) sometimes help break through LLM hesitancy or bias, but only for some structures and not universally. In complex tasks (SPC), more content did not always help and could even hurt, possibly due to information overload or distracting the model from core patterns.\n\n---\n### 9. All specifics of the researcher's prompt strategies\n\n**A. Prompt Structures Used:**\n- **0-shot:** Just task statement + input.\n- **1-shot:** + One example (t for positive, f for negative).\n- **Few-shot:** + Two examples.\n    - few-shot-tt: both positive\n    - few-shot-ff: both negative\n    - few-shot-tf: positive then negative\n    - few-shot-ft: negative then positive\n\n**B. Prompt Contents Used:**\n- Basic (task statement)\n- Role (\"You are...\")—system-level or user-level\n- ActAs (\"Act as... expert...\")\n- GPT-generated (LLM creates instructions)\n- Emotions:\n    - Encourage (motivational)\n    - Threaten (negative motivation)\n    - Reward (promise of tip)\n    - Punish (threat of fine)\n\n**C. Combination Trials**\n- Role+GPT; Emotion+GPT; under 0-shot structure.\n\n**D. Application Procedure**\n- Each prompt structure/content was run on 2,000 samples per dataset, results averaged over three runs to estimate standard error.\n- Main evaluation metrics: accuracy, recall, precision, F1.\n- *Comparison criterion*: A difference considered significant if >= 2x standard error.\n\n---\n**Summary Table of Where to Find Each:**\n- Prompt structure and content: Section II.B, II.C, Table I\n- Concrete prompt wordings: Table I\n- Full experiment results: Tables II (SPD), III (VFD), IV (SPC)\n- Combination results: Table V\n- Comparative/statistical methodology: Section II.D, III.A, IV Discussion\n\n---\n\n**If you need a compact table of the best prompt for each task/metric, or examples for a different task, just let me know!**"
}