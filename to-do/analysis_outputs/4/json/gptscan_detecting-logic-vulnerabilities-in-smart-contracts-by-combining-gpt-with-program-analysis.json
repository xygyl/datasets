{
  "filename": "gptscan_detecting-logic-vulnerabilities-in-smart-contracts-by-combining-gpt-with-program-analysis.pdf",
  "analysis": "Sure! Here are detailed answers to your questions about the \"GPTScan\" paper, organized by your requested points:\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Point / Exploration:**  \nThe paper introduces **GPTScan**, a novel tool for the detection of logic vulnerabilities in smart contracts (mainly Solidity code for Ethereum and similar blockchains). The authors identify a critical gap: existing static analysis tools are good at fixed-pattern, control-flow, and data-flow bugs (like re-entrancy, integer overflow), but **fail to detect complex business-logic vulnerabilities**, which make up ~80% of real-world bugs and lead to huge financial losses in DeFi.\n\n**Uniqueness:**\n- Previous attempts to use Large Language Models (LLMs) for this task either relied solely on LLMs (which led to high false positive rates and required expensive, powerful models) or just traditional static analysis (missing high-level semantics).\n- **GPTScan innovatively combines GPT (LLMs) as a code-understanding tool with static analysis:**  \n  - GPT is used not for end-to-end detection, but to interpret code scenarios and properties (semantics).\n  - It breaks down each vulnerability type into code-level scenarios and properties to guide GPT analysis, rather than vague high-level descriptions.\n  - Static confirmation then checks the results from GPT to reduce false positives.\n\n---\n\n**2. What dataset did they use?**\n\nThey use three datasets, summarized in Table 2:\n- **Top200:**  \n  - Contracts with the top 200 market capitalizations (widely used/audited; ~303 projects, 555 Solidity files, 134K lines; no known vulnerabilities).  \n  - Used to measure false positive rates.\n\n- **Web3Bugs:**  \n  - 72 projects (~2573 files, 319K lines) from the Code4rena audit platform, from the Web3Bugs dataset (with 48 ground-truth logic vulnerabilities).\n\n- **DefiHacks:**  \n  - 13 projects, 29 files, 17.8K lines. Known hacks with past attack incidents, 14 logic vulnerabilities.\n\nTotal: ~400 projects, 3,100 files, 472K lines, 62 ground-truth logic vulnerabilities.\n\n---\n\n**3. What LLM models were used?**\n\n- **GPT-3.5-turbo** (default model for the tool, due to cost-effectiveness and practicality for large-scale analysis).\n- Some preliminary tests with **GPT-4**, but these showed little increase in accuracy with a 20x cost increase.\n- The choice of GPT-3.5-turbo was enabled by their \"multi-dimensional filtering,\" which kept queries small enough for the 4K token context window.\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\nMain strategies:\n- **\"Scenario and Property Matching\":**  \n  - Instead of vague vulnerability descriptions, they create code-level \"scenarios\" where the vulnerability might happen and \"properties\" that characterize the vulnerable code.\n  - GPT receives these scenarios and properties and is asked, for each function, whether these are satisfied (yes/no, enforced answer format).\n\n- **\"Mimic-in-the-background\" prompting:**  \n  - To reduce GPT's randomness, they instruct GPT to internally \"think\" of five answers and give the most frequent as final answer (inspired by zero-shot chain-of-thought).\n  - Additionally, the temperature parameter is set to zero for determinism.\n  \n- **Separation of scenario and property queries:**  \n  - Scenario matching first, then property matching only for those that pass.\n\n- **JSON-format output enforced:**  \n  - Limits GPT's answer structure, making it easy to parse and harder to drift.\n  \n- **Asking for key variable/statement identification:**  \n  - After detection, GPT is asked to identify the exact variables/statements responsible for the condition, which are then validated through static analysis.\n\n---\n\n**5. What specific prompts were used?**\n\nExamples from the paper:\n\n- **Scenario Matching:**\n  ```\n  System: You are a smart contract auditor...\n  Given the following smart contract code, answer the questions below and organize the result in a JSON format like {\"1\":\"Yes\" or \"No\", \"2\":\"Yes\" or \"No\"}.\n  \"1\": [SCENARIO_1]?\n  \"2\": [SCENARIO_2]?\n  [CODE]\n  ```\n  \n- **Property Matching:**\n  ```\n  Does the following smart contract code \"[SCENARIO, PROPERTY]\"? Answer only \"Yes\" or \"No\".\n  [CODE]\n  ```\n\n- **Variable/Statement Recognition:**\n  ```\n  In this function, which variable or function holds the total supply/liquidity AND is used by the conditional branch to determine the supply/liquidity is 0? Please answer in a section that starts with \"VariableB:\".\n  ... [more such questions] ...\n  Please answer in the following json format: {\"VariableA\":{\"Variable name\":\"Description\"}, ...}\n  [CODE]\n  ```\n\n- **\"Mimic-in-the-background\":**  \n  - The system prompt instructs GPT to internally answer the same question five times and return the most frequent answer; strict output format is enforced.\n\n---\n\n**6. Where are the prompt strategies explained?**\n\n- Scenario and property matching: Section 4.2 and Figure 4.\n- Mimic-in-the-background strategy: Section 4.2, last third (and Figure 4), and also mentioned in Introduction.\n- Variable/statement recognition: Section 4.4 and Figure 5.\n- The prompt formats and details are explicitly shown in Figures 4 and 5.\n\n---\n\n**7. What are the results of the prompting strategies, and how successful or not were they?**\n\n- **Accuracy:**\n  - **Top200 (no known vulns):** 4.39% FP rate (low). Precision 90%+ for these contracts.\n  - **Web3Bugs:** 57.14% precision, recall 83.33%, F1 67.8%.\n  - **DefiHacks:** Precision 90.91%, recall 71.43%, F1 80%.\n\n- **Comparison to other tools:**\n  - Traditional static analysis tools (e.g., Slither) had 0% recall for business logic vulnerabilities, or very high FP rates.\n  - GPT-only tools (i.e., pure prompting with high-level descriptions, as in prior work) had extreme false positive rates (96%, per [34]).\n  - MScan (industry tool) could detect only some price manipulation cases (recall 58%) but nothing else.\n  \n- **Static confirmation (combination with static analysis):**\n  - Cut false positives by 66% compared to GPT-only step, while adding only a few (3) additional false negatives.\n\n- **Discovered 9 previously undocumented vulnerabilities missed by human auditors.**\n\n---\n\n**8. Why did one strategy give better results than another?**\n\n- **Scenario/property-based prompting** with code-level details is better than high-level abstract descriptions:  \n  - This makes it much easier for GPT to map code to vulnerability concepts, avoiding relying on the LLM's prior knowledge or reasoning skills.\n\n- **Forcing structured outputs (JSON, Yes/No), and \"background mimic\" voting** increases determinism, reliability, and parsing, compared to open-ended answers.\n\n- **Multi-dimensional filtering** (pre-filtering functions using names, structure, call graph, access control etc.) means that the LLM sees only realistically relevant code, making detection much more precise and cost-effective.\n\n- **Static confirmation:** GPT can miss or hallucinate details, or not carefully correlate variable dependencies. Static analysis can check data and control dependencies, as well as order of statements and access control, substantially reducing false positives.\n\nWhen GPT-only strategies were used with high-level vulnerability definitions (as in [34]), the LLM was not able to reliably infer codified vulnerabilities, leading to huge false positive rates.\n\n---\n\n**9. All specifics of the researcher's prompt strategies.**\n\n**Prompting Pipeline:**\n- **Filtering:**\n  1. Project-wide filtering (exclude non-source files, library code, OpenZeppelin clones, etc.)\n  2. Function filtering via YAML rules (e.g., function name or content contains certain keywords, public/external visibility, no access control modifiers, etc.)\n  3. Call-graph-based reachability analysis: only include externally reachable functions.\n\n- **Scenario Matching:**\n  - Each logic vulnerability is broken into one or more scenario sentences (what code/functionality should exist for this vuln to occur).\n  - Functions are bulk-checked for scenario match in batched prompts with a forced Yes/No answer, in JSON format.\n\n- **Property Matching:**\n  - For functions that passed scenario matching, send follow-up queries: does the code exhibit certain key properties (\"Is there a clear/reset of approval after transfer completes?\", etc.).  \n  - Again, Yes/No, strictly JSON output.\n\n- **Mimic-in-the-background (Determinism):**\n  - The LLM is told to internally repeat the answer (background \"think\"/\"vote\") five times and return the most frequent, to minimize randomness.\n  - Temperature is set to zero.\n\n- **Variable/Key Statement Recognition:**\n  - For positive matches, the function is sent to GPT with further prompts: \"Which variable holds X?\", or \"Which line updates Y?\", again requiring JSON output.\n  - This focuses static analysis on precise code elements, reducing the verification search space.\n\n- **Static Confirmation:**\n  - For each positive match, tool uses static analysis to check:\n    - Dataflow between variables\n    - Whether value checks are actually enforced in require/assert/if\n    - Statement order\n    - External control of function arguments, etc.\n\n- **Results Merging:**  \n  - Duplications and function → call-graph → function overlaps handled at end for FP/TP/FN calculation.\n\n**Prompt Engineering Details:**\n- System prompts position GPT as an expert auditor.\n- Output format is forced strictly (JSON, Yes/No, labeled variable assignments).\n- No explanation; only structured answers accepted.\n- Double-confirm properties by combining scenario + property in the prompt (\"Does the following smart contract code 'Scenario AND Property'?\").\n\n---\n\nIf you need a breakdown of example YAML filtering rules, or more details from a specific subsection, let me know!"
}