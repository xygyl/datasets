{
  "filename": "detecting-code-smells-using-chatgpt_initial-insights.pdf",
  "analysis": "Here’s a detailed, structured answer to each of your questions based on the supplied paper, \"Detecting Code Smells using ChatGPT: Initial Insights\":\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**\n- The paper investigates the effectiveness of ChatGPT, specifically version 3.5-Turbo, in detecting code smells in Java projects.\n- It aims to see how well ChatGPT can identify four specific code smells—Blob, Data Class, Feature Envy, and Long Method—classified at different severity levels.\n- It asks whether ChatGPT can perform this task with generic prompts (no details about smell types) versus more detailed, explicit prompts (which enumerate the specific smells of interest).\n\n**Uniqueness:**\n- According to the authors, this is one of the first empirical studies to assess a large language model’s (LLM) ability to detect code smells—a key software quality and refactoring concern.\n- The study contrasts two prompting strategies (generic vs. specific) on the same dataset, quantifies differences with rigorous statistical tests, and analyzes performance over various severity levels.\n- The work complements prior studies on LLMs for code generation, bug fixing, and unit test generation, but focuses on design flaws rather than functional defects.\n\n\n---\n\n## 2. What dataset did they use?\n\n- The study uses the **MLCQ dataset** ([9]):  \n  - This is an industry-relevant code smell dataset.\n  - Includes 14,739 instances covering four code smells: **Blob**, **Data Class**, **Feature Envy**, and **Long Method**.\n  - Each smell instance is labeled with one of four severity levels: **None, Minor, Major, or Critical**.\n  - For this research, the authors focused **only on samples labeled Minor, Major, or Critical** (excluding \"None\"), resulting in **3,291 instances**:  \n    - 974 Blobs  \n    - 1,057 Data Classes  \n    - 454 Feature Envies  \n    - 806 Long Methods\n\n---\n\n## 3. What LLM models were used?\n\n- **OpenAI’s ChatGPT version 3.5-Turbo** was used for all evaluations in this research.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\n**Two main prompting strategies:**\n1. **Generic Prompt**:  \n   - The prompt asks ChatGPT to identify \"bad smells\" in a given block of Java code, without naming or listing the specific smell types.\n   - Purpose: To test ChatGPT's ability to autonomously recognize code smells.\n\n2. **Specific (or Detailed) Prompt**:  \n   - The prompt lists expressly all four target code smells (Blob, Data Class, Feature Envy, Long Method) and asks ChatGPT to identify which are present in the code.\n   - Purpose: To test whether direct, explicit instruction improves LLM performance.\n\nBoth prompts required a specific answer format to allow automated extraction of results.\n\n---\n\n## 5. What specific prompts were used?\n\n- **Prompt #1 (Generic):**  \n  ```\n  I need to check if the Java code below contains code smells (aka bad smells). Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.\n  Please start your answer with “YES I found bad smells” when you find any bad smell. Otherwise, start your answer with “NO, I did not find any bad smell”.\n  When you start to list the detected bad smells, always put in your answer “the bad smells are:” amongst the text your answer and always separate it in this format: 1. Long method, 2. Feature envy...\n  [Java source code with the smell]\n  ```\n\n- **Prompt #2 (Specific):**  \n  ```\n  The list below presents common code smells (aka bad smells). I need to check if the Java code provided at the end of the input contains at least one of them.\n  *Blob\n  *DataClass\n  *FeatureEnvy\n  *LongMethod\n  Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.\n  Please start your answer with “YES I found bad smells” when you find any bad smell. Otherwise, start your answer with “NO, I did not find any bad smell”.\n  When you start to list the detected bad smells, always put in your answer “the bad smells are:” amongst the text your answer and always separate it in this format: 1. Long method, 2. Feature envy...\n  [Java source code with the smell]\n  ```\n\n(See Figures 1 & 2 in the paper.)\n\n---\n\n## 6. Where are the prompt strategies explained?\n\n- Prompt design and specifics are detailed in **Section 2.2: Prompts Definition** (including Figures 1 & 2 for the actual text).\n- The comparison and justification for design are additionally discussed in the methodology and in the research questions (Sections 2 and 3).\n\n---\n\n## 7. What are the results of the prompting strategies and how successful or not were they?\n\n**In summary:**\n- *Overall*, ChatGPT struggled, but **detailed prompts significantly improved performance**.\n  \n- Using **Generic Prompt** (Prompt #1):\n  - Best performance was on \"Long Method\" (F1 = 0.41), worst on \"Data Class\" (F1 = 0.11), and zero correct on \"Blob\".\n  - ChatGPT failed to detect any Blob smells correctly.\n  - For \"Feature Envy,\" precision and recall were low (F1 = 0.23).\n\n- Using **Specific Prompt** (Prompt #2):\n  - Marked improvement for \"Data Class\" (F1 = 0.59; 0.48 higher than generic).\n  - Still poor for \"Blob\" (F1 = 0.19), moderate for \"Long Method\" (F1 = 0.46), and similar to generic on \"Feature Envy\" (F1 = 0.21).\n  - On severity, Prompt #2 yielded F1 scores of 0.52 for critical smells (vs. 0.27 with Prompt #1 for critical), and 0.43 for minor.\n\n- **Statistical test:**\n  - McNemar’s test found the difference **statistically significant**: the odds of ChatGPT being correct are 2.54 times higher with the specific prompt than with the generic.\n  - Absolute Risk Difference: 16% more correct with Prompt #2 overall.\n\n**Success:**  \n- **Specific prompts lead to a significant, albeit still modest, improvement**, especially for “Data Class” and at higher severity levels (Major, Critical).\n- **Generic prompts performed poorly overall**.\n- Even with specific prompts, F1 scores remained below 0.6 in all cases, showing much room for improvement.\n\n---\n\n## 8. Why did one strategy give better results than another?\n\n- **Lack of focus/context in generic prompts**:  \n  - The generic prompt leaves ChatGPT guessing what to look for, so it may miss less obvious smells or interpret the request too broadly.\n- **Direct instruction aids detection**:   \n  - The specific prompt **explicitly lists the smell types**, narrowing ChatGPT's attention to the correct features (i.e., it “knows what to look for”).\n- **Specialization/relevance**:\n  - Since LLMs are sensitive to the information in the prompt, **listing the smell names helps the model activate relevant linguistic/code features or patterns** associated with those smells.\n\n- The improvement was largest for “Data Class” as its symptoms (data-heavy classes, public fields, little behavior) are perhaps more easily recognized when the model is primed by mentioning the term “Data Class”.\n- For complicated, context-heavy smells like \"Blob\" or \"Feature Envy\", both prompts led to poor performance, likely because these require deeper semantic/contextual understanding.\n\n---\n\n## 9. All specifics of the researcher's prompt strategies.\n\n**Prompt Crafting:**\n- Both prompts were iteratively refined using pilot experiments for clarity and consistency.\n- Both require a specific output format for ease of automated result extraction: \"YES I found bad smells. The bad smells are: 1.Long method, 2.Feature envy...\"\n- In both cases, the actual Java code from the dataset was included after the instruction.\n\n**Prompt #1 (Generic):**\n- Asks generally for code smell identification, without context or enumeration of possible smells.\n- Simulates a “base-case” scenario: can ChatGPT guess \"what’s wrong\" without being told what specific problems to look for?\n\n**Prompt #2 (Specific):**\n- Starts with a list of possible code smells (*Blob, *DataClass, *FeatureEnvy, *LongMethod).\n- The model is told to look for those specific smells only.\n- Tests whether explicit enumeration boosts accuracy.\n\n**Parsing/Aggregation:**\n- Only responses matching the output pattern (“The bad smells are: ...”) were auto-parsed.\n- 27 exceptions, where responses didn’t match the pattern, were manually reviewed.\n- For analysis, standard IR metrics (precision, recall, F1) were calculated per smell and severity.\n\n**Other Context:**\n- The authors plan future improvements (not in this study) like adding more context (e.g., code dependencies), using chain-of-thought prompts, or fine-tuning LLMs on smell-rich code.\n\n---\n\n### Summary Table: Prompt Strategies\n\n| Prompt Type        | Smells Listed Explicitly? | Output Format Specified? | Context Given?          | Goal/Use Case                |\n|--------------------|--------------------------|--------------------------|-------------------------|------------------------------|\n| Generic (Prompt #1)| No                       | Yes                      | No                      | Test baseline/generalization |\n| Specific (Prompt #2)| Yes (4 are listed)      | Yes                      | No                      | Test impact of explicitness  |\n\n---\n\n**Citations in Paper:**\n- Prompts described and explained in Section 2.2 and shown in Figures 1 & 2.\n- Prompts compared and results analyzed throughout Sections 3–4, and summarized in tables 2, 3, 4, 5, and 6.\n\n---\n\n**In summary:**  \nThe study found that spelling out what code smells to look for (the specific prompt) helps ChatGPT detect them much better than a generic “look for bad smells” query—though there’s still a long way to go before LLMs can reliably identify code smells in complex, real-world code. "
}