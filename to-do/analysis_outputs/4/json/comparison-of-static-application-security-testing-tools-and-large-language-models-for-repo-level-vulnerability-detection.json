{
  "filename": "comparison-of-static-application-security-testing-tools-and-large-language-models-for-repo-level-vulnerability-detection.pdf",
  "analysis": "Certainly! Here’s a structured breakdown answering each of your questions with precise details drawn from the paper text:\n\n1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n- Point/Goal:\n  - This paper aims to conduct the first comprehensive comparison between static application security testing (SAST) tools and large language models (LLMs) for automated vulnerability detection at the **repository level** (repo-level) across three major programming languages: Java, C, and Python.\n  - It specifically explores their effectiveness, trade-offs, and potential benefits of combining both approaches.\n\n- What’s unique:\n  - First to conduct a **repo-level** vulnerability detection study (most prior work is on function-level).\n  - First to compare 15 SAST tools and 12 LLMs (with 4 prompting strategies) for *real-world* vulnerabilities, not synthetic datasets, across three languages.\n  - Introduces new/curated real-world datasets for Java, C (extended), and especially Python (novel for this study).\n  - First to systematically test SAST tools on real-world Python repos.\n  - Provides extensive empirical evidence for best/worst strategies, including combining (ensembling) SASTs/LLMs.\n\n2. What dataset did they use?\n\nThey curated/used **three datasets**:\n\n- **Java:** Based on repository information from Li et al. [49].\n  - 50 projects, 87 CVEs, 3,986,848 functions, 0.008% vulnerability ratio.\n- **C:** Dataset from Lipp et al. [51].\n  - 6 projects, 85 CVEs, 1,181,556 functions, 0.009% vulnerability ratio.\n- **Python:** Newly constructed for this work.\n  - 78 projects, 98 CVEs, 312,045 functions, 0.18% vulnerability ratio.\n\nEach dataset contains:\n- Real-world repositories from GitHub.\n- CVE information, CWE type, function-level vulnerable location.\n- Vulnerable function identified based on “lines changed in vulnerability-fixing commit”; others are labeled clean.\n\n3. What LLM models were used?\n\n**12 LLMs** across encoder, decoder, and encoder-decoder architectures, distinguished by parameter size (Table 2):\n\n- Lightweight (<1B params):\n  - CodeBERT (0.13B, encoder-only)\n  - GraphCodeBERT (0.13B, encoder-only)\n  - CodeT5 (0.22B, encoder-decoder)\n  - UniXcoder (0.13B, encoder-decoder)\n- Large (>=1B params):\n  - StarCoder (7B, decoder-only)\n  - CodeLlama (7B, decoder-only)\n  - Mistral (7B, decoder-only)\n  - DeepSeekCoder (6.7B, decoder)\n  - StarCoder2 (7B, decoder-only)\n  - CodeQwen (7B, decoder-only)\n  - Llama3 (8B, decoder-only)\n  - Phi3 (3.8B, decoder-only)\n\n4. What are the prompting strategies discussed in this paper?\n\nFour prompting/LlM adaptation strategies:\n\n- Zero-shot prompting\n- Few-shot prompting\n- Chain-of-thought (CoT) prompting\n- Fine-tuning (with LoRA-PEFT for large models, full-tune for small models)\n\n5. What specific prompts were used?\n\nPrompt format (Section 2.5.1):\n\n- A three-part structure:\n  1. **Task Description:** “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”\n  2. **Formatted Input:** code delimited by markers //CodeStart and //CodeEnd.\n  3. **Prediction Marker:** //Detection to prompt model to give output.\n\nVariants:\n- Zero-shot: the above format, code as-is.\n- Chain-of-Thought (CoT): add “Let’s think step by step” to the end of the task description.\n- Few-shot: prepend the above prompt with two labeled examples from training data (one vulnerable, one clean function, with code + label) to the formatted prompt so model sees them as context.\n\n6. Where are the prompt strategies explained?\n\n- Section 2.5 LLM Adaptation Techniques\n  - 2.5.1 Prompt-based Methods (p. 5 of the main document body)\n  - 2.5.2 Fine-tuning-based Methods\n\n7. What the results of the prompting strategies are and how successful or not were they?\n\n- **Fine-tuning (especially for small models):** Highest detection ratios (up to 100%) with a trade-off of high marked-function ratios (false positives).\n- **Prompting (zero-shot, CoT, few-shot):** In general, lower detection than fine-tuning; CoT sometimes helps marginally.\n- **Few-shot:** Not as effective as full fine-tuning; varying results but rarely outperforms fine-tuning.\n- **Chain-of-thought:** Small improvements or sometimes negligible compared to zero-shot.\n\n**Summary of success:**\n- Fine-tuning beats prompting methods for both detection and false positive rate.\n- Prompt-based methods are less effective for this task, as confirmed by lower vulnerability detection rates and often high false positive rates (marked function ratios).\n\n8. Why did one strategy give better results than another?\n\n- **Fine-tuning** performs better because:\n  - It exposes the model directly to the distribution and specifics of vulnerability patterns in code, allowing specialized adaptation of weights.\n  - Prompt-based methods, especially zero/few-shot, rely on the model’s latent general knowledge, which is insufficient for highly imbalanced and complex tasks like real-world vulnerability detection.\n  - High class imbalance (very few vulnerable functions) means models fine-tuned on balanced data learn to distinguish better, while prompt-based methods over-mark or under-mark due to lack of nuanced adaptation.\n\n9. All specifics of the researcher's prompt strategies.\n\n- **Prompt** always starts:\n  - “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”\n  - Code placed between:\n    ```\n//CodeStart\n<source code of function>\n//CodeEnd\n```\n  - Model is cued to answer at:\n//Detection\n\n- **Zero-shot:** Just above, no examples.\n- **CoT:** As above, but add: “Let's think step by step” after the task description.\n- **Few-shot:** Prompt is preceded by two code label pairs (one clean, one vulnerable) formatted identically, immediately prior to the instruction and test case.\n\n- Fine-tuning: All model weights trained on a balanced set of vulnerable and non-vulnerable functions (with same ratio in val set); for large LLMs, LoRA PEFT is used, small LLMs use full fine-tuning.\n\n**Additional Research Implementation Details:**\n- For each function in a repo, apply the LLM independently and aggregate function labels per repo for repo-level verdict (Section 2.1.3).\n- Metrics: Vulnerability Detection Ratio (true positive rate), Marked Function Ratio (proxy for false positives).\n\n---\n\n**Summary Table for Prompting Strategies in This Paper:**\n\n| Strategy         | Prompt Template | Examples Used | “Let’s think step by step” | Tuning?          |\n|------------------|----------------|---------------|---------------------------|------------------|\n| Zero-shot        | Base prompt    | None          | No                        | No               |\n| Chain-of-thought | Base prompt    | None          | Yes                       | No               |\n| Few-shot         | Base prompt    | Two (1 vuln, 1 clean) | No                | No               |\n| Fine-tune (sm)   | N/A            | N/A           | N/A                       | Full fine-tuning |\n| LoRA-PEFT (lg)   | N/A            | N/A           | N/A                       | PEFT, LoRA       |\n\n**Locations in Paper:**\n- Section 2.5 (Prompt Definition and Variants)\n- Table 3 (Main results)\n- Section 4.x (Results by method and language)\n\n**Performance Summary:**  \n- Fine-tuning clearly outperforms all prompt-based approaches for this code security task. CoT and few-shot have minor impact, but do not close the gap.\n- Prompt choice and adaptation methods directly influence detection rates and marked function ratios.\n- Combining (ensembling) LLMs or SASTs can mitigate certain drawbacks depending on the language/task.\n\nIf you need copyable/example prompts as used for the models, let me know!"
}