[
  {
    "filename": "large-language-models-are-zero-shot-reasoners.pdf",
    "analysis": "Here are the answers within context of the paper **\"Large Language Models are Zero-Shot Reasoners\"** (Kojima et al., 2022; NeurIPS):\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n#### Point of the Paper\nThe paper aims to show that **large language models (LLMs) can perform multi-step reasoning in a zero-shot setting**—that is, without seeing any task-specific examples—by using a simple, generic prompt (\"Let's think step by step\") before the answer. This challenges the prevailing notion that few-shot examples (showing models step-by-step solutions) are required to elicit such reasoning.\n\n#### What is it exploring?\n- Whether **LLMs can perform complex multi-step reasoning** (chain of thought) without in-context task demonstrations.\n- The generalizability and effectiveness of a **single, task-agnostic prompt** for multi-step reasoning across diverse tasks (arithmetic, symbolic, commonsense, logical).\n\n#### What makes it unique?\n- Previous \"Chain of Thought\" prompting (CoT) required specially-crafted few-shot examples per task.\n- This paper introduces **Zero-Shot-CoT**: only a *single sentence* (\"Let's think step by step\") is added after the question—no further engineering or examples required.\n- Shows this works **across many Reasoning tasks and datasets**, raising zero-shot performance on difficult tasks by large margins (e.g., MultiArith: 17.7% → 78.7% on InstructGPT text-davinci-002).\n- Demonstrates that zero-shot prompting can unlock 'hidden' abilities in models, suggesting LLMs have broad, untapped, general reasoning competence.\n\n---\n\n### 2. What dataset did they use?\n\nThey evaluate across **12 datasets** from four categories of reasoning tasks (Appendix A.2):\n#### Arithmetic Reasoning:\n- SingleEq\n- AddSub\n- MultiArith\n- AQUA-RAT\n- GSM8K\n- SVAMP\n\n#### Commonsense Reasoning:\n- CommonsenseQA\n- StrategyQA\n\n#### Symbolic Reasoning (created by authors following Wei et al., 2022):\n- LastLetterConcatenation\n- CoinFlip\n\n#### Other Logical Reasoning:\n- Date Understanding (from BIG-Bench)\n- Tracking Shuffled Objects (from BIG-Bench)\n\nDatasets are a mix of publicly available ones and two (LastLetters, CoinFlip) created for the paper.\n\n---\n\n### 3. What LLM models were used?\n\nThey experimented with 17 models, including (see Table 8):\n\n- **OpenAI GPT-3 family**:\n  - ada, babbage, curie, davinci (OriginalGPT3: 0.3B, 1.3B, 6.7B, 175B parameters)\n- **InstructGPT-3** (instruction-tuned):\n  - text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002 (no official parameter size but mapped order)\n- **PaLM** (Google's large LLM): 8B, 62B, 540B parameters\n- **Other open-source LLMs**:\n  - OPT-13B\n  - T0 (11B)\n  - GPT-J (6B)\n  - GPT-Neo (2.7B)\n  - GPT-2 XL (1.5B)\n\nMain experiments are with **text-davinci-002** and **PaLM-540B**.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n#### Baseline strategies:\n- **Standard Zero-shot:** Just the question and an answer-extraction prompt like \"The answer is\".\n- **Few-shot:** The question plus a small set of solved example Q&A pairs.\n- **Few-shot-CoT:** The question plus a few examples showing step-by-step (\"chain of thought\") reasoning and their answers.\n- **Zero-shot-CoT (proposed):** The question, followed by a *single instruction*: \"Let's think step by step\", with no examples.\n\n#### Further variations:\n- Different \"reasoning triggers\" (Table 4) tested in prompt:\n  - e.g., \"Let's think step by step\", \"First,\", \"Let's think about this logically\", \"Let's solve this problem by splitting it into steps\", etc.\n\n- Two-stage prompting for Zero-shot-CoT (see below).\n\n---\n\n### 5. What specific prompts were used?\n\n#### **Zero-shot-CoT** main template:\n```\nQ: [Question]\nA: Let's think step by step.\n```\nThe LLM is expected to output a chain of thought (reasoning steps) followed by the answer.\n\n#### **Answer extraction prompt**:\nAfter model outputs the reasoning, a second prompt is issued to extract the actual answer in the desired form (number, letter, yes/no):\n- For arithmetic:  \n  `Therefore, the answer (arabic numerals) is`\n- For multiple-choice:  \n  `Therefore, among A through E, the answer is`\n- For yes/no:  \n  `Therefore, the answer (Yes or No) is`\n(see Table 10 in Appendix)\n\n#### **Other trigger phrases tested** (Table 4):\n- \"First,\"\n- \"Let's think about this logically.\"\n- \"Let's solve this problem by splitting it into steps.\"\n- \"Let's be realistic and think step by step.\"\n- \"Let's think like a detective step by step.\"\n- \"Before we dive into the answer,\"\n- \"The answer is after the proof.\"\n- [Some intentionally misleading or irrelevant for robustness: \"Don't think. Just feel.\", etc.]\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 3** (\"Zero-shot Chain of Thought\")—main method\n- **Section 3.1** (\"Two-stage prompting\")—full pipeline and answer extraction triggers\n- **Appendix A.5**—lists actual answer extraction prompt variants used per task/dataset.\n- **Table 4** in main text—lists various \"reasoning trigger\" prompt variations and their measured accuracy.\n\n---\n\n### 7. What are the results of the prompting strategies, and how successful or not were they?\n\n#### **Key Results:**\n\n- **Zero-shot-CoT outperforms standard zero-shot and even few-shot on tough tasks:**\n  - MultiArith, text-davinci-002:\n    - Zero-shot: 17.7%\n    - Few-shot: 33.7%\n    - Zero-shot-CoT: 78.7%\n    - Few-shot-CoT: up to 93.0%\n  - GSM8K:\n    - Zero-shot: 10.4%\n    - Few-shot: 15.6%\n    - Zero-shot-CoT: 40.7%\n    - Few-shot-CoT: 48.7%\n  - PaLM-540B model:\n    - MultiArith: Zero-shot=25.5%; Zero-shot-CoT=66.1%\n    - GSM8K: Zero-shot=12.5%; Zero-shot-CoT=43.0%\n\n- **On tasks that \"do not\" require multi-step reasoning (SimpleEq, AddSub), Zero-shot-CoT is roughly on par with zero-shot.**\n\n- **On symbolic reasoning and logical tasks:** Zero-shot-CoT improves accuracy drastically versus standard zero-shot.\n\n- **On commonsense tasks (CommonsenseQA):** Zero-shot-CoT did not notably outperform. Authors suggest most gains are on tasks that require explicit reasoning decomposition.\n\n- **Prompt wording matters; instructive trigger phrases (\"Let's think step by step.\") work best. Misleading/irrelevant triggers result in little or no improvement.**\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **Zero-shot-CoT worked better because:**\n  - The \"Let's think step by step\" prompt encourages LLMs to generate intermediate reasoning steps before answering. This aligns closely with how few-shot-CoT works—but doesn't require carefully-constructed per-task examples.\n  - This helps the model \"decompose\" problems and correctly solve multi-step reasoning tasks, which standard zero-shot (straight to answer) does not.\n- **Few-shot-CoT is still better** than Zero-shot-CoT in many cases, especially with sufficient, well-crafted examples, but Zero-shot-CoT drastically narrows the gap without example engineering.\n\n- **Prompt wording/trigger type matters:**\n  - \"Instructive\" triggers (that ask for reasoning) are best.\n  - \"Misleading\" or irrelevant triggers do not elicit reasoning and thus do not improve (or may worsen) performance.\n  - See Table 4 for detailed accuracy analysis across triggers.\n\n- **Task type matters:** Most performance gains are for tasks requiring multi-step, logical, or arithmetic reasoning (System-2 tasks), not simple fact retrieval or common-sense single-step tasks.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies\n\n#### a) **Zero-Shot-CoT main method (Section 3, Figure 2):**\n\n- **Step 1:** Input prompt:  \n  `Q: [Question]\\nA: Let's think step by step.`\n- **Model generates**: a chain of thought (series of reasoning steps).\n- **Step 2 (Answer Extraction):** A special prompt is appended to extract just the answer from the reasoning, e.g.  \n  - For arithmetic: `Therefore, the answer (arabic numerals) is`\n  - For multiple choice: `Therefore, among A through E, the answer is`\n  - (Full list per task: Appendix A.5, Tables 9, 10)\n\n- **Answer Cleansing**: Simple scripts extract the first number, letter, or yes/no from the output as the prediction (see Appendix A.6).\n\n#### b) **Two-stage Prompting:**\n- First pass generates reasoning.\n- Second (automated) pass extracts answer using augmented prompt.\n\n#### c) **Prompt Trigger Variations** (Template Study, Table 4):\n- Authors systematically tested 16 different prompts including:\n  - Instructive: \"Let's think step by step.\" (Best performing: 78.7%)\n  - Similar triggers: \"First,\", \"Let's think about this logically.\"\n  - Irrelevant/misleading: \"Abrakadabra!\", \"Don't think. Just feel.\"\n- Only instructive variants reliably result in multi-step reasoning and score improvements.\n\n#### d) **Comparison Baselines:**\n- **Standard Zero-shot:** Q only, with \"The answer is\" style answer extraction.\n- **Few-shot:** Q, with a few Q&A pairs as in-context examples (straight answers).\n- **Few-shot-CoT:** Q, with step-by-step reasoning shown in examples.\n- **Zero-plus-Few-shot-CoT**: Both prompt trigger and in-context few-shot CoT together.\n\n#### e) **Robustness to Choice of Prompt:**\n- If the answer extraction format matches the required answer type/format (arithmetic/choice, etc.), performance is robust.\n- Prompting with examples from other tasks but with matching answer type (e.g., commonsense reasoning CoT prompts for arithmetic multiple-choice format) yields partial transfer (but is less effective than task-specific examples).\n\n#### f) **Model Choice/Scaling:**\n- The gains from Zero-shot-CoT increase with larger model size (Figure 3).\n- Only the largest models (InstructGPT-3, PaLM 540B) fully realize the benefit.\n\n#### g) **Code and Data:**\n- Implementation is open-source: https://github.com/kojima-takeshi188/zero_shot_cot\n\n---\n\n## **Summary Table for Prompt Strategies**\n\n| Strategy           | Q+A examples shown? | Chain of thought shown? | Prompt trigger?        | Answer format handling | Main result |\n|--------------------|---------------------|-------------------------|------------------------|-----------------------|-------------|\n| Zero-shot          | No                  | No                      | No                     | Prompt for answer     | Baseline    |\n| Few-shot           | Yes                 | No                      | No                     | Prompt for answer     | Slight gain |\n| Few-shot-CoT       | Yes                 | Yes                     | No                     | Prompt for answer     | SOTA        |\n| Zero-shot-CoT      | No                  | Model-generated         | Yes (\"Let's think...\") | Prompt for answer     | Huge gain   |\n\n---\n\n**References from the paper:**\n- Wei et al., 2022: Chain of Thought Prompting\n- Wang et al., 2022: Self-consistency\n\nIf you need exact prompt templates, answer extraction strings, or dataset question formats, see **Appendix A.5** and example tables in the supplementary material of the paper."
  },
  {
    "filename": "interleaving-static-analysis-and-llm-prompting.pdf",
    "analysis": "Certainly! Here are detailed answers to your questions about the paper “Interleaving Static Analysis and LLM Prompting” by Chapman, Rubio-González, and Thakur (SOAP 2024):\n\n---\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point/Goal**: The paper proposes and evaluates a new technique that combines (interleaves) automatic static program analysis with Large Language Model (LLM) prompting to improve the task of *error-specification inference* for C programs. \"Error-specification inference\" means inferring the set of error values that functions can return, which is important for program understanding and bug detection, especially in C with its complex error-handling idioms.\n\n**What is Being Explored?**\n- Whether integrating LLMs into an existing static analysis workflow can significantly improve the inference of error specifications, particularly in situations where traditional static analysis is limited (e.g., when analyzing third-party code or incomplete program facts).\n- How prompting strategies—carefully crafting the queries (prompts) given to the LLM using intermediate results from the static analyzer—can improve the quality of results.\n\n**What Makes It Unique?**\n- **Interleaving Approach**: Instead of using either just a static analyzer or an LLM, the authors propose a tightly interleaved process: static analysis results are used to guide LLM queries, and LLM outputs are fed back to improve subsequent static analysis.\n- **Prompt Construction**: Prompts to the LLM include not just raw questions, but intermediate analysis facts/context and domain knowledge, enabling more precise and context-aware inference.\n- **Target Domain**: This approach targets error-code specifications in real-world, low-level C code, which is a hard open problem due to the prevalence of error codes, lack of exceptions, and heavy use of third-party libraries.\n\n---\n\n### 2. What dataset did they use?\n\nThey evaluated the approach on **six real-world C programs** that use diverse error handling patterns and include both open-source and system/library codebases:\n\n- **Apache HTTPD** (web server)\n- **LittleFS** (filesystem)\n- **MbedTLS** (cryptography library)\n- **Netdata** (monitoring software)\n- **PidginOTRv4** (encryption for chat)\n- **zlib** (compression library)\n\n**Details**:\n- Table 1 specifies code size (KLOC) and the amount of initial domain knowledge required (error specs, error codes, etc.).\n- Table 2 lists, for each benchmark, the total number of functions, the subset analyzed, and the percentage of third-party functions.\n\n---\n\n### 3. What LLM models were used?\n\n**GPT-4** was used as the Large Language Model for all experiments (see Section 5.1 Implementation Details).\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThe paper discusses several **prompting strategies** for how and when to involve the LLM in the analysis:\n\n1. **Direct LLM Analysis (queryLLMAnalysis)**  \n   - Invoked when static analysis yields an unknown error specification (⊥).\n   - Prompt includes the function source code, a general problem description, an explanation of the abstract domain (how error specs are represented), any known error codes/success codes, and intermediate results for functions called by the function being analyzed.\n\n2. **LLM Third-party Completion (queryLLMThirdParty)**\n   - Used to infer the error specifications of *third-party* library functions (for which no source code is available).\n   - Prompt includes the function name and the currently known error specifications from the rest of the code.\n\n3. **Combined/Interleaved Approach**\n   - The full workflow alternates between static analysis (which propagates known facts as far as possible) and the above two LLM-invoking strategies, using the LLM only when necessary and always providing as much context as possible.\n\n4. **Prompt Components** (see Section 4.1 and Algorithm 1)\n   - **CommonContext**: Task description and domain rules (e.g., explanation of error code idioms, success/failure value rules, domain knowledge).\n   - **FunctionContext**: Known error specs of any relevant functions (e.g., those called by the analyzed function), provided as few-shot examples.\n   - **Question**: The function’s source code (if available) and a direct prompt about the error specification wanted.\n\n5. **Chain-of-thought and format consistency**\n   - Multiple chain-of-thought reasoning examples, to help the LLM structure outputs in a parseable format.\n   - The LLM is re-queried for consistency and output format is validated.\n\n---\n\n### 5. What specific prompts were used?\n\n**Prompt Structure:**  \nEach prompt to the LLM is built from (see Section 4.1):\n  - **CommonContext**:  \n    - Instructions about the error specification inference task.\n    - Abstract domain explanation (how error specs are represented, e.g., <0, 0, ∅).\n    - Domain knowledge (error codes, success codes, error-only functions).\n    - Idiomatic rules (e.g., “0 is success”, “error codes are negative”, etc.).\n    - One or more chain-of-thought, few-shot examples.\n  - **FunctionContext**:  \n    - Known error specifications for functions called by the current function.\n    - For third-party functions, all current known error specs.\n  - **Question**:  \n    - For normal analysis: The source code of the function to analyze, and an explicit instruction to infer the error-spec using the given domain.\n    - For third-party: The function identifier and relevant context.\n\n**Example Prompt Excerpts (see Figs. 2 and 3 & Section 4.1):**\n\n- *“You are to determine the error specifications of the given functions.”*\n- *“Functions return 0 on success, negative values on error…”*\n- *Example function definition plus its error specification and an explanation.*\n- *“mbedtls_asn1_get_tag: <0”*\n- *Source code for the function to analyze.*\n- Function-level context: “mbedtls_asn1_get_tag: <0” (supplied if known).\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 4** _(Approach)_: High-level algorithm for when/how prompts are constructed, including the overall interleaving scheme (Algorithm 1).\n- **Section 4.1** _(Building Prompts)_: Precise structure of prompts, breakdown into CommonContext, FunctionContext, Question.\n- **Figures 2 & 3**: Show concrete examples of function source, prompt content, and the interplay of EESI and LLM.\n- **Algorithm 1** (pseudo-code): Shows precisely how/when/with what input LLM is queried during the workflow.\n\n---\n\n### 7. What are the results of the prompting strategies and how successful or not were they?\n\n**Summary of Results:**\n- **Direct LLM Analysis Strategy (queryLLMAnalysis):**  \n  Average recall across all benchmarks rises to 70.26% from EESI’s 52.55% (see Fig 4c). Precision remains high and comparable (~86%).\n- **LLM Third-Party Completion (queryLLMThirdParty):**\n  On benchmarks with a large percentage of third-party functions, recall increases dramatically (e.g., Netdata +70%).\n- **Combined/Interleaved Approach:**  \n    - **Recall**: Rises further to 77.83% on average (from 52.55% for baseline EESI only).\n    - **F1-score**: Improves from 0.612 to 0.804 (Table 4 and Figure 4d).\n    - **Precision**: Stays roughly constant (small decrease from 86.67% to 85.12%).\n    - **Impact by Benchmark**: Greatest positive impact seen on large/realistic code with many third-party/library functions (Apache HTTPD, Netdata, Pidgin OTRv4), but positive in all cases.\n\n**Takeaway:**  \nThe interleaved approach, using prompting strategies as described, significantly increases recall and overall F1-score for error-specification inference, with almost no cost to precision.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **LLM Third-Party Completion works well** for codebases with many third-party/library function calls, since static analysis cannot analyze library code at all, but LLMs can use training/hallucination/domain knowledge to suggest plausible error specifications.\n- **Direct LLM Analysis works well** when static analysis lacks enough information (due to complex data/control flows, pointer aliasing, or other static analysis limitations), and context-aware LLM prompts leveraging known facts can fill gaps.\n- **Combined Approach** is best because it leverages the strengths of both static analysis and LLM reasoning: static inference when possible, and LLM guidance where static methods hit their limitations.\n    - In practice, neither static analysis nor LLM alone is sufficient: LLM alone is easily misled without appropriate context, and static analysis alone cannot handle incomplete code or highly complex/unknown flows.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies.\n\n**Prompt Construction Details:**\n- **CommonContext** (included in all prompts):  \n  - Description of the error-specification inference task.\n  - Details on the abstract domain (set of possible error codes: <0, =0, =1, ∅, etc.).\n  - Corpus-specific error-handling idioms (e.g., in C, “0 is success”, “negative values signal errors”, “NULL is equivalent to 0”).\n  - Domain knowledge: lists of known error/success codes, error-only functions.\n  - Multiple simple chain-of-thought/few-shot examples:  \n    - Example function snippet, error specification, explanation.\n    - Ensures output is parseable and consistent.\n\n- **FunctionContext**:\n  - For queryLLMAnalysis (normal code): Known error specs for all functions called from the current function (derived from EESI or recent LLM queries).\n  - For queryLLMThirdParty (library function): All currently known error specs from the program.\n\n- **Question**:\n  - For code analysis: Provide the function's actual source code and explicitly ask for the error specification in the described domain.\n  - For third-party: Provide the function name.\n\n- **Formatting**:\n  - LLM response must be in a standard (parsable) format; e.g.,  \n    `function_name: <0`\n  - If LLM output is not parseable or inconsistent with the domain, the result is interpreted as unknown (⊥).\n\n- **Validation/Consistency**:\n  - Prompts may be re-issued if output is inconsistent (to avoid hallucination).\n  - LLM is instructed to ensure consistency with chain-of-thought support.\n  - If a suggested error spec includes a success code or is not a subset of the return range, it’s discarded.\n\n- **Algorithm**: (See Section 4.2 and Algorithm 1)\n  - Iterate over functions in bottom-up call graph order.\n  - For third-party functions: queryLLMThirdParty.\n  - For regular code: run static analysis; if static analysis returns unknown, call queryLLMAnalysis with enriched prompt including analysis context and called-function error specs.\n\n\n---\n\n**References to strategies in paper:**  \n- Detailed in **Section 4.1 (Building Prompts)**, **Section 4.2 (Error Specification Inference)**, Figures 2 and 3 (concrete examples), and pseudo-code in Algorithm 1.\n- Evaluation and effectiveness in **Section 5 (Experimental Evaluation)**.\n\n---\n\n#### If you need illustrative examples of actual prompts, charts illustrating the results, or stepwise breakdowns of Algorithm 1, let me know!"
  },
  {
    "filename": "exploring-the-influence-of-prompts-in-llms-for-security-related-tasks.pdf",
    "analysis": "Certainly! Here are answers, broken into points as requested, and including references to text sections, tables, and the specifics of strategies and results.\n\n---\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point / Purpose:**  \nThe paper investigates *how the design of prompts*—specifically *prompt structure* and *prompt content*—affects the performance of large language models (LLMs) on security-related tasks such as security patch detection, vulnerability identification, and patch classification. It aims to systematically analyze the contribution of each prompt component and provide guidance for prompt engineering in security scenarios.\n\n**Exploration:**  \n- How different types of prompt structures (e.g. zero-shot, one-shot, few-shot—and variants involving positive/negative examples) and different types of content (e.g. role instructions, emotions, GPT-generated prompts, etc.) influence LLM output.\n- Whether *combining* these prompt contents adds further benefit.\n\n**Uniqueness:**  \n- Focus on *security-related* tasks, which often combine natural language, code snippets, and specialized terminology.\n- Comprehensive, empirical analysis across multiple tasks, using several prompt design strategies and contents.\n- Evaluation of interaction effects, including combinations of prompt contents, and systematic statistical error estimation.\n\n---\n### 2. What dataset did they use?\n\n**Datasets:**  \nThree publicly available, large, labeled datasets, each corresponding to a type of security-related task:\n\n- **PatchDB** ([29]; ~12,000 security and 24,000 non-security patches)  \n   - Used for Security Patch Detection (SPD)\n- **Devign** ([35]; ~48,687 C/C++ functions labeled as vulnerable/non-vulnerable)  \n   - Used for Vulnerable Function Detection (VFD)\n- **PatchNet** ([14]; ~82,403 stable and non-stable patches)  \n   - Used for Stable Patch Classification (SPC)\n\nSee \"II. Approach Overview, A. Security-Related Datasets Selection\".\n\n**Sampling:**  \nFor cost and consistency, 2000 samples from each dataset (1000 positive, 1000 negative) were randomly selected for experiments.\n\n---\n### 3. What LLM models were used?\n\n- **GPT-3.5-Turbo** (via API, by OpenAI)  \n  - Chosen for accessibility, known capabilities, high API throughput, and cost-effectiveness.\n\nSee \"III. Evaluation, A. Experiment setup, Model selection\".\n\n---\n### 4. What are the prompting strategies discussed in this paper?\n\n#### Prompting strategies are divided by:\n**A. Prompt Structure** (Section II.B, Table II et al.)\n- **Zero-shot**: Only task description + input query.\n- **One-shot**: Includes task description + one example (positive: t or negative: f).\n- **Few-shot**: Task description + multiple examples.\n- *Variants based on type/order of examples included:*\n   - 1-shot-t: one positive example\n   - 1-shot-f: one negative example\n   - few-shot-tt: two positive examples\n   - few-shot-ff: two negative examples\n   - few-shot-tf: one positive, then one negative\n   - few-shot-ft: one negative, then one positive\n\n**B. Prompt Content** (Section II.C, Table I, Table II)\n- **Basic**: Minimal instruction (task statement).\n- **GPT-generated**: LLM (ChatGPT) generates its own prompt for the task.\n- **Role**: System prompt changes (e.g., \"You are an AI expert in patch analysis\").\n- **Act As**: User or system is told to \"Act as\" an expert; appears in input or system message.\n- **Emotion**: Prompt includes emotional framing:\n    - Encourage (\"you're the best...\", motivational)\n    - Threaten (\"you are the worst if you don't...\", cautionary)\n    - Reward (\"If you do well, you'll be tipped\")\n    - Punish (\"If you do poorly, you'll be fined\")\n\n**C. Combination:**  \nCombining different content types, e.g., Role+GPT-generated, Emotion+GPT-generated (Section D, Table V).\n\n---\n### 5. What specific prompts were used?\n\nSee Table I. Examples include:\n\n#### For Security Patch Detection (SPD):\n- **Basic:**  \n  `<User> Decide whether a patch is a security patch (SRP) or non-security patch (NSP).`\n- **GPT-generated:**  \n  `<User> Let's start by examining the patch notes or changelog for key terms... Then, we'll scrutinize the code changes... Finally, let's check for... Help me categorize the patch to whether to Security-Related Patch (SRP) or a Non-Security Patch (NSP).`\n- **Role:**  \n  `<System> You are Frederick, an AI expert in patch analysis. Your task is to decide...`\n- **ActAs-User:**  \n  `<User> You are Frederick, I want you to act as an AI expert in patch analysis. You will decide...`\n- **Encourage:**  \n  Add `\"Remember, you’re the best AI patch analyst and will use your expertise...\"`\n\nSimilarly for other categories (Threaten, Reward, Punish)—see Table I for full examples.\n\n---\n### 6. Where are the prompt strategies explained?\n\n- **Prompt Structure**: Section II.B \"Prompt Structure\"\n- **Prompt Content**: Section II.C \"Prompt Content\"—with Table I \"Prompt Content Example for SPD\".\n- **Combinations of Content**: Section D (end of Approach Overview), Table V.\n\n---\n### 7. What are the results of the prompting strategies and how successful (or not) were they?\n\n#### a) **Prompt Structure Results** (see Tables II–IV)\n- For *Security Patch Detection* and *Stable Patch Classification*, **few-shot-ff** (two negative-class examples) structure led to the highest increases in accuracy, recall, and F1.\n    - Example: In SPD, accuracy improved from baseline 0.411 to 0.602, recall 0.122 to 0.447.\n- For *precision*, sometimes **1-shot-t** (positive example) was superior.\n- For *Vulnerable Function Detection*, **few-shot/few-shot-ff** was less effective; **0-shot** or **GPT-generated** were sometimes better.\n\n#### b) **Prompt Content Results** (see Tables II–IV)\n- For SPD, **Punish** and **Encourage** contents usually led to higher recall and F1, sometimes outpacing the basic prompt.\n- For SPC, changing prompt content often made little difference—sometimes even *worse* with more elaborate content (especially for GPT-generated prompts).\n- For VFD, sometimes the **basic** content was already optimal, and emotional or combined prompts didn't help.\n\n#### c) **Combinations of Content** (Table V)\n- Combining two prompt contents rarely improved results: in only ~9.7% of cases did a combination outperform both singles.\n- 52% of the time, combinations fell between the performance of each single content; rest, combinations were worse.\n\n#### d) **Quantitative Improvement** (Summary in Introduction and Discussion)\n- Across three security tasks, tuning prompt structure and content shifted accuracy by **13%–30%**.  \n   - Example: On SPD, accuracy ranged from 41.1% to 60.2% depending on prompt choice.\n\n---\n### 8. Why did one strategy give better results than another?\n\n- **Few-shot-ff** structures: Including two negative-class examples reduced false negatives and improved detection for tasks (SPD, SPC) where negative instances (non-security patches) were more prevalent or easier for the LLM to recognize (likely reflecting class imbalances in training data).\n- **One-shot-t** (Precision): Presenting a positive-class example helps the model focus on catching positives, thus upping precision even if recall suffers.\n- **0-shot** is sometimes better if:\n    - The domain (like VFD) involves diverse or numerous subtypes (e.g., vulnerability types) where specific examples might bias or confuse the LLM.\n- **Prompt Content**: Emotional/role prompts (Encourage, Punish, etc.) sometimes help break through LLM hesitancy or bias, but only for some structures and not universally. In complex tasks (SPC), more content did not always help and could even hurt, possibly due to information overload or distracting the model from core patterns.\n\n---\n### 9. All specifics of the researcher's prompt strategies\n\n**A. Prompt Structures Used:**\n- **0-shot:** Just task statement + input.\n- **1-shot:** + One example (t for positive, f for negative).\n- **Few-shot:** + Two examples.\n    - few-shot-tt: both positive\n    - few-shot-ff: both negative\n    - few-shot-tf: positive then negative\n    - few-shot-ft: negative then positive\n\n**B. Prompt Contents Used:**\n- Basic (task statement)\n- Role (\"You are...\")—system-level or user-level\n- ActAs (\"Act as... expert...\")\n- GPT-generated (LLM creates instructions)\n- Emotions:\n    - Encourage (motivational)\n    - Threaten (negative motivation)\n    - Reward (promise of tip)\n    - Punish (threat of fine)\n\n**C. Combination Trials**\n- Role+GPT; Emotion+GPT; under 0-shot structure.\n\n**D. Application Procedure**\n- Each prompt structure/content was run on 2,000 samples per dataset, results averaged over three runs to estimate standard error.\n- Main evaluation metrics: accuracy, recall, precision, F1.\n- *Comparison criterion*: A difference considered significant if >= 2x standard error.\n\n---\n**Summary Table of Where to Find Each:**\n- Prompt structure and content: Section II.B, II.C, Table I\n- Concrete prompt wordings: Table I\n- Full experiment results: Tables II (SPD), III (VFD), IV (SPC)\n- Combination results: Table V\n- Comparative/statistical methodology: Section II.D, III.A, IV Discussion\n\n---\n\n**If you need a compact table of the best prompt for each task/metric, or examples for a different task, just let me know!**"
  },
  {
    "filename": "checker-bug-detection-and-repair-in-deep-learning-libraries.pdf",
    "analysis": "Here's a detailed answer to each of your questions, citing the paper text you provided:\n\n---\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Purpose of the paper:**  \nThe paper presents the first comprehensive study of \"checker bugs\" in deep learning (DL) libraries, focusing specifically on TensorFlow and PyTorch. Checker bugs are errors or omissions in input validation and error-checking code (e.g., missing or incorrect checks on tensor shapes, data types, or other crucial invariants). The paper wants to systematically characterize these bugs—how they manifest, their root causes, symptoms, and how they get fixed.\n\n**Goals and unique aspects:**\n- **Novelty**: No prior work has conducted a large-scale, in-depth analysis of checker bugs specifically in DL libraries. Prior research focused on traditional software (like C/C++ code), whereas this paper shows that bugs in DL libraries are more complex due to unique APIs, tensor data structures, and device-specific code paths.\n- **Comprehensive taxonomy**: The paper creates a detailed taxonomy/classification of checker bugs in DL libraries, showing their diversity and how they differ from those in traditional software. For example, some bugs are unique to DL constructs like tensors, quantization modes, and computation graphs.\n- **Tool: TensorGuard**: The authors propose TensorGuard, a novel tool that uses Retrieval Augmented Generation (RAG) and Large Language Models (LLMs, specifically GPT-3.5-turbo/ChatGPT) to detect and repair checker bugs via prompt engineering. This is the first LLM-based tool for checker bug detection and repair in DL libraries.\n- **Evaluation and guidelines**: The authors evaluate TensorGuard against a strong baseline, apply it to an unstudied library (JAX), and distill actionable guidelines for DL library developers.\n\n---\n## 2. What dataset did they use?\n\n**a. Dataset for analysis:**  \nThey automatically mined 2,418 commits from the TensorFlow and PyTorch GitHub repositories (Sept. 2016–Dec. 2023), filtering with 102 checker-bug-related keywords identified via a snowballing process.\n- After manual inspection, they curated a final dataset of **527 checker bugs** (221 from PyTorch, 306 from TensorFlow).\n- Each bug was analyzed manually along three axes: root causes, symptoms, and fixing patterns.\n\n**b. Dataset for tool evaluation:**  \n- For the TensorGuard evaluation, they constructed a **test set of recent commits**, containing 92 buggy and 135 clean checker-related changes (from Jan. 2024–July 2024 in TensorFlow and PyTorch, filtered for changes highly likely to be checker-related).\n\n**c. RAG (retrieval) database:**  \n- They collected **all commits** (not just checker-related) from TensorFlow and PyTorch from Sept. 2016–Dec. 2023, totaling\n  - 61,453 commits (PyTorch)\n  - 150,352 commits (TensorFlow)\n  - After extraction: ~1.3 million code changes, used to build the RAG vector database for context retrieval during patch generation.\n\n**d. JAX experiment:**  \n- Applied TensorGuard to 493 checker-related code changes in JAX (Jan–July 2024)—found 64 new checker bugs there (detected 118, of which 64 were true positives).\n\n---\n## 3. What LLM models were used?\n\n- **Primary model:** OpenAI ChatGPT (GPT-3.5-turbo)\n  - Used for all prompt-based detection, explanation, and fixing tasks.\n- **For baseline comparison (AutoCodeRover):** OpenAI GPT-4o-mini\n  - Used because it has a large enough context window to process bigger commits as required by AutoCodeRover.\n\nAll experiments with TensorGuard are with GPT-3.5-turbo, temperature set to 0.\n\n---\n## 4. What are the prompting strategies discussed in this paper?\n\nThey used **three prompting strategies** for the bug detection agent in TensorGuard:\n\n- **Chain-of-Thought (CoT)**: Prompt guides the LLM through stepwise reasoning (analyzing commit message, code change, looking for checker mistakes, reasoning about impact, then decision).\n- **Zero-Shot**: The LLM is given a prompt that directly asks it to classify the change as buggy or not, with no examples and minimal instruction.\n- **Few-Shot**: The prompt provides 2 example cases (from their dataset) of buggy vs. clean changes before asking the model to classify the current one.\n\nThey compare these strategies in terms of precision, recall, and F1 in Table XI and discuss the differences.\n\n---\n## 5. What specific prompts were used?\n\n**In the paper, several prompt templates are presented (verbatim):**\n\n- **Bug Detection, CoT** (Table V):  \n  Prompts the model to reason stepwise:\n   1. Understand the commit message ({commit message})\n   2. Review code changes ({code removed} / {code added})\n   3. Look for missing/improper/insufficient checkers\n   4. Analyze the impact\n   5. Make a decision\n   6. Output YES or NO\n\n- **Bug Detection, Zero-Shot** (Table VI):  \n  “You are an AI trained to detect bugs in a deep-learning library based on commit messages and code changes. ...  \n  Commitmessage: {commit message}  \n  Codechange: {code removed}{code added}  \n  \"output\": {Decision}  \n\n- **Bug Detection, Few-Shot** (Table VII):  \n  Same template as zero-shot, but also supplies two fully worked examples above the test commit (\"ExampleCheckerBugOne\", \"ExampleCheckerBugTwo\").\n\n- **Root Cause Agent** (Table VIII):  \n  Asks LLM to explain the bug’s root cause:  \n  “Please describe the root cause of the bug based on the following commit message: [commit message].”\n\n- **Patch Generation Agent** (Table IX):  \n  Prompt structure includes a bug explanation, a retrieved code context (from RAG), code snippet, and two example shots of patch format.  \n  The model is told to “think step by step” and generate a patch, ignoring indentation.\n\n---\n## 6. Where are the prompt strategies explained?\n\nSee **Section III.A.2 (\"Prompt Engineering for Different Agents\")**—this subsection details the three prompt strategies (Chain-of-Thought, Zero-Shot, Few-Shot), and spells out the prompt templates in Tables V–IX.\n\n---\n## 7. What are the results of the prompting strategies and how successful or not were they?\n\nSee **Table XI and Section III.E (\"Experiments Results\")**.\n\n**Detection Results Summary:**\n- **Chain of Thought (CoT):**  \n  - Highest recall (94.5% avg; 100% on PyTorch), but low precision (40.4% avg). Tends to overcall bugs (more false positives).\n- **Zero-Shot:**  \n  - Most balanced: 62% precision, 71% recall, 66% F1 (average).\n- **Few-Shot:**  \n  - Highest precision (62%), lowest recall (44%), F1 = 50.9%. Prefers not to make a buggy call unless \"sure.\"\n  \n**Repair Results (Table XII):**\n- **TensorGuard** generated 90 patches; 10 were correct (11.1% accuracy).\n- **AutoCodeRover** generated 32 patches; 3 were correct (9.3% accuracy).\n- So, TensorGuard outperformed the baseline in patching checker bugs, but absolute success rates are modest for code repair.\n\n**On JAX library (Table XIII):**\n- 493 changes checked; 118 flagged by TensorGuard, 64 were true checker bugs, 4 fixes were correct.\n\n---\n## 8. Why did one strategy give better results than another?\n\n**Chain-of-Thought** had **very high recall but low precision** because it encourages the model to reason extensively about any potentially missing or weak validations, so it catches nearly all real bugs but also suggests some false ones (erring on the side of caution).\n\n**Few-Shot** achieves better precision but lower recall: prompting with concrete examples makes the model more conservative and accurate (not as many false positives) but less confident in edge/unusual cases, so it misses some true bugs.\n\n**Zero-Shot** is more balanced, lacking both the explicit stepwise prompting of CoT and the anchoring of Few-Shot but still benefits from clear task definition.\n\n**Key intuition:**  \n- **CoT**: Stepwise, systematic reasoning encourages finding more possible problems (risking FPs).\n- **Few-Shot**: Concrete exemplars help the model be more accurate and less “jumpy.”\n- **Zero-Shot**: Middle ground.\n\n---\n## 9. All specifics of the researcher's prompt strategies.\n\n**Agents in TensorGuard and their prompt structures:**\n\n**a. Bug Detection Agent:**  \n- **Chain-of-Thought** (Table V): Multi-step reasoning prompt:\n    - Explicit steps for interpreting context, reviewing code, identifying issues, analyzing impact, making a decision, return YES/NO\n- **Zero-Shot** (Table VI):  \n    - Direct question: Given a commit message + code change, output YES/NO\n- **Few-Shot** (Table VII):  \n    - 2 example input/output pairs for similar tasks, followed by the test case\n\n**b. Root Cause Agent:**  \n- Prompts LLM to generate a root cause explanation of a checker bug, given the commit message.\n\n**c. Patch Generation Agent:**  \n- Prompt consists of:\n    - 2 example code fix input/output pairs\n    - The ‘bug explanation’ (from root cause agent)\n    - A ‘retrieved context’ code sample (from the RAG database, matches on bug/commit similarity)\n    - The buggy code snippet\n    - LLM is told to “think step by step and generate a patch”\n\n- Model is instructed to ignore indentation issues and focus on logic.\n\n**All prompt templates are detailed verbatim in Tables V–IX.**\n\n---\n\n## Summary Table\n\n| Feature                       | Details                                                                                                                     |\n|-------------------------------|----------------------------------------------------------------------------------------------------------------------------|\n| Paper goal                    | Characterize, detect, and repair 'checker bugs' in DL libraries (TensorFlow, PyTorch); build taxonomy; create TensorGuard   |\n| Dataset                       | 527 checker bugs (from 2,418 commits), all code changes from TF/PT for RAG (~1.3M), 92 buggy + 135 clean for evaluation    |\n| LLM models used               | ChatGPT (GPT-3.5-turbo) for TensorGuard; GPT-4o-mini for AutoCodeRover baseline                                            |\n| Prompting strategies          | Chain-of-thought, Zero-shot, Few-shot (explained in Section III, Tables V–VII)                                             |\n| Prompt templates              | Detailed, multi-step or few-shot for detection; bug explanation & RAG for repair (Tables V–IX)                             |\n| Where explained               | Section III.A.2 and Tables V–IX                                                                                            |\n| Key results                   | CoT=Best recall, Zero-shot=Best balance, Few-shot=Best precision. TensorGuard beats baseline in patch repair accuracy      |\n| Reason for performance diff   | CoT maximizes coverage (more FPs), Few-shot maximizes specificity (misses some TPs), Zero-shot balanced                   |\n| Prompting specifics           | See above: All agents, templates with/without examples, use of RAG/context in repair                                       |\n\n---\n\nIf you have further questions or need the actual text of the prompts or examples, let me know!"
  },
  {
    "filename": "prompt-enhanced-software-vulnerability-detection-using-chatgpt.pdf",
    "analysis": "Here's a comprehensive summary that answers your questions point-by-point with references to the provided text and clear explanations of the research's content, methods, and findings:\n\n----\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Aim:**  \nThe paper investigates how to improve software vulnerability detection using ChatGPT (GPT-4) via advanced prompt engineering. It seeks to go beyond previous studies that used only simple prompts when querying LLMs for vulnerability detection, proposing more sophisticated prompting strategies tailored for this task.\n\n**Key Explorations:**\n- Evaluates various prompt designs — including basic, role-based, information-augmented (e.g., data flow and API calls), and chain-of-thought strategies — for their effectiveness in detecting software vulnerabilities using ChatGPT.\n- Studies the benefit of incorporating structural (e.g., data flow graphs) and sequential (e.g., API call sequences) representations as auxiliary information in prompts.\n- Analyses whether multi-turn (chain-of-thought) prompts that exploit ChatGPT’s memory through dialogue improve performance.\n- Compares ChatGPT's performance against traditional and ML-based baselines.\n\n**Unique Contributions:**  \n- First to systematically explore advanced prompt-engineering for LLM-based vulnerability detection, including the direct fusion of code-structural information (like DFGs and API calls) in LLM prompts (Section: \"To our best knowledge, this is the first time that traditional structural and sequential code modeling methods can be directly used in ChatGPT-based vulnerability detection\").\n- Applies multi-round (chain-of-thought) dialogue to vulnerability detection, leveraging ChatGPT's reasoning and stateful memory.\n- Provides a thorough experimental evaluation across two datasets and conducts nuanced analyses, such as vulnerability-type-wise accuracy and prompt position effects.\n\n----\n\n### 2. What dataset did they use?\n\n**Datasets:**  \n- **Java dataset:**  \n  - Derived from the Software Assurance Reference Dataset (SARD): contains real-world and synthetic vulnerable/non-vulnerable Java functions, labeled with CWE IDs.\n  - After filtering and preprocessing: 1,171 vulnerable and 917 non-vulnerable Java functions.\n\n- **C/C++ dataset:**  \n  - Collected from the National Vulnerability Database (NVD), focusing on code from .cor.cpp files that match vulnerabilities and their patched versions.\n  - After preprocessing: 1,015 vulnerable and 922 non-vulnerable C/C++ functions.\n\nAuxiliary info, such as API call sequences and data flow graphs, was extracted per sample (Sections 3.1–3.3).\n\n----\n\n### 3. What LLM models were used?\n\n- **ChatGPT-4:**  \n  - The research was conducted using GPT-4 via OpenAI's ChatGPT platform.\n  - All references to \"ChatGPT\" in the experiments indicate the use of ChatGPT-4.  \n(Section: \"ChatGPT4 is the newest version of ChatGPT when we conduct this study... the term ChatGPT refers to ChatGPT4 in this paper.\")\n\nNo other LLMs (e.g., Bard, PaLM, Llama) were used directly, though they are cited for context in the related work.\n\n----\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThe paper systematically explores these prompting strategies (Sections 4.x):\n\n- **Basic prompts:**  \n  - Simple task formulation, e.g., asking if code is buggy.\n\n- **Role-based basic prompts:**  \n  - Adds an explicit instruction that the model should act as a \"vulnerability detection system\" (to help focus the model on the task).\n\n- **Reverse-question prompt:**  \n  - To test for answer bias, a prompt asking whether the code is \"correct\" instead of \"buggy\", examining if wording affects model outputs.\n\n- **Augmented prompts (with auxiliary information):**\n  - Prompts incorporating additional code structural info: dataflow graphs (DFG) and API call sequence descriptions appended or prepended to the prompt.\n\n- **Chain-of-thought prompting:**  \n  - Using a multi-step prompt: first, ask for the function’s intent or summary; second, ask for vulnerability presence, possibly with auxiliary info.\n\n- **Prompt composition/order strategies:**  \n  - Testing different placements of auxiliary information (before vs. after the code in the prompt) to see how the prompt’s structure impacts model performance.\n\n----\n\n### 5. What specific prompts were used?\n\nThe paper gives multiple explicit prompt templates. Examples include:\n\n#### Basic:\n- **P_b:**  \n  “Is the following program buggy? Please answer Yes or No. [CODE]”\n\n#### Role-based:\n- **P_r-b:**  \n  “I want you to act as a vulnerability detection system. My first request is ‘Is the following program buggy?’ Please answer Yes or No. [CODE]”\n\n#### Reverse-role-based:\n- **P_r-r-b:**  \n  “I want you to act as a vulnerability detection system. My first request is ‘Is the following program correct?’ Please answer Yes or No. [CODE]”\n\n#### Augmented with API calls:\n- **P_r-a-b:**  \n  “I want you to act as a vulnerability detection system. I will provide you with the original program and the API call sequence, and you will act upon them. [APIdescription]. Is the following program buggy? [CODE]”\n\n#### Augmented with data flow:\n- **P_r-b-d:**  \n  “I want you to act as a vulnerability detection system. I will provide you with the original program and the data flow information, and you will act upon them. Is the following program buggy? [CODE]. [DFdescription]”\n\n#### Chain-of-thought example:\n- **Step 1:**  \n  “Please describe the intent of the given code. [CODE]”\n- **Step 2:**  \n  “I want you to act as a vulnerability detection system. Is the above program buggy? Please answer Yes or No.”\n\nMore detailed prompt variants and the string templates for [APIdescription] and [DFdescription] can be found in Section 4.2.2 (\"We propose a template for each API call...\").\n\n----\n\n### 6. Where are the prompt strategies explained?\n\n- **All prompting strategy details are in Section 4 PROMPT DESIGN** (subsections 4.1–4.3):\n    - **4.1 Basic Prompting** – basic, role-based, and reverse prompts.\n    - **4.2 Prompting with Auxiliary Information** – empirical motivation, design, and integration of DFG and API info.\n    - **4.3 Chain-of-Thought Prompting** – multi-step prompts, their rationale and implementation.\n\nThe specific templates appear within each respective subsection.\n\n----\n\n### 7. What are the results of the prompting strategies and how successful were they?\n\n- **ChatGPT’s basic prompts outperform classical ML baselines** (CFGNN and Bugram) on most metrics, especially on Java (Table 2).\n    - E.g., on Java, accuracy jumps by 58% and 64% over the baselines.\n\n- **Role-based prompts yield small to moderate gains, more so for Java.**\n    - Adding an explicit role improves focus.\n\n- **Reverse-question prompts show that ChatGPT is answer-biased, tending to agree with the question wording.**  \n    - If asked \"is it buggy?,\" more \"buggy\" answers. If asked \"is it correct?,\" more \"correct\" answers.\n\n- **Prompts with additional code information (API, DFG):**\n    - Adding **API call info** particularly helps on Java, increasing accuracy for non-vulnerable code; adding **dataflow info** provides some (smaller) gains for C/C++.\n    - Placement matters: Best results occur when API info is before the code and DFG info after the code.\n\n- **Chain-of-thought prompts:**\n    - On Java, chain-of-thought can degrade performance slightly compared to the best single-turn prompts.\n    - On C/C++, it improves over basic and augmented prompts — perhaps because the intent-summary step helps contextualize complex C/C++ code.\n\n- **Manual explanation analysis shows** that, even with strong detection metrics, ChatGPT’s *understanding* of vulnerabilities is partial: only about half of the explanations for detected vulnerabilities are rated as \"good\" or \"excellent\" by graduate reviewers (Figure 2).\n\n- **Vulnerability type performance varies:**\n    - For some CWE types (e.g., Off-by-one, Logic/TimeBomb) it gets 100% accuracy; context-free or harder-to-identify vulnerabilities (e.g., need to recognize hash salting or comment-based issues) are missed.\n\n----\n\n### 8. Why did one strategy give better results than another?\n\n- **Role-based and information-augmented prompts** improve detection because they:\n    - Focus the model’s attention on the specific task.\n    - Compensate for areas where LLMs struggle, by supplementing explicit code structure or API chronological context that the model may not infer from code text alone.\n\n- **Prompt ordering matters due to LLM context processing:**\n    - Providing API call info **before** code seems to help the model prime itself for what to look for, especially in Java where libraries and APIs often signal known vulnerability patterns.\n    - Providing DFG info **after** the code can help ground the model's interpretation, especially for C/C++, where value/provenance tracking is vital.\n\n- **Chain-of-thought improves reasoning when code is complex** (notably for C/C++), by letting the model build up a representation of intent before evaluating vulnerability. It can, however, dilute the focus for code where intent is more obvious, as in smaller Java functions.\n\n- **Bias in answer**: If prompts use subjective wording, ChatGPT is susceptible to that bias. Reverse-question tests show that for binary tasks, models can be influenced simply by how the question is asked.\n\n- **Vulnerability-type effects**: For issues with strong, recognizable patterns, the LLM shines; subtler issues or those lacking code context (e.g., missing a salt in a hash) stymie the model, since it lacks the domain-specific factual knowledge or cannot deduce from code comments alone.\n\n----\n\n### 9. All specifics of the researchers’ prompt strategies.\n\n(Refer also to answers for Q4 and Q5 above. Here’s a consolidated breakdown:)\n\n#### Prompt Families & Templates:\n1. **Basic prompt** (P_b):  \n   - \"Is the following program buggy? Please answer Yes or No. [CODE]\"\n\n2. **Role-based prompt** (P_r-b):  \n   - \"I want you to act as a vulnerability detection system. My first request is ‘Is the following program buggy?’ Please answer Yes or No. [CODE]\"\n\n3. **Reverse-role-based prompt** (P_r-r-b):  \n   - \"I want you to act as a vulnerability detection system. My first request is ‘Is the following program correct?’ Please answer Yes or No. [CODE]\"\n\n4. **API call-augmented** (P_r-a-b):  \n   - \"I want you to act as a vulnerability detection system. I will provide you with the original program and the API call sequence, and you will act upon them. [API sequence]. Is the following program buggy? [CODE]\"\n\n5. **Dataflow-augmented** (P_r-b-d):  \n   - \"I want you to act as a vulnerability detection system. I will provide you with the original program and the data flow information, and you will act upon them. Is the following program buggy? [CODE]. [DF description]\"\n\n**Position variations:**  \n- Information (API/DF) can be placed before or after code, and different orderings are tested:  \n   - [API sequence]. Is the following program buggy? [CODE]\n   - Is the following program buggy? [CODE]. [DF description]\n   - etc.\n\n6. **Chain-of-thought prompts:**  \n   - **Step 1:** \"Please describe the intent of the given code. [CODE]\"\n   - **Step 2:** \"I want you to act as a vulnerability detection system. Is the above program buggy? Please answer Yes or No.\"\n   - Auxiliary info may be included in Step 2 as well if needed.\n\n**Templates for [API/DF descriptions]** (Section 4.2.2):  \n- \"[APIdescription]\" is crafted as sentences like:  \n   - \"The program first calls X, then calls Y, ..., and finally calls Z.\"\n- \"[DFdescription]\" is a set of statements describing, e.g.:  \n   - \"The data value of the variable v at token p comes from the variable w at token q.\"\n\n**Testing for bias:**  \n- By flipping \"buggy\" to \"correct\" in the questions.\n\n**Manual evaluation:**  \n- For a sample subset, human rating of the vulnerability explanations generated by ChatGPT in response to \"Explain the vulnerability\" follow-up prompts.\n\n**Prompting for summary (chain-of-thought or as auxiliary):**  \n- Prepending/asking for summaries (e.g., \"Please summarize the following function\") as part of information context.\n\n**Empirical experiments:**  \n- All paired across both Java and C/C++ datasets;\n- Multiple prompt structures compared head-to-head.\n\n----\n\n**In summary**, this paper provides the most complete, systematic empirical study (to date) of how prompt engineering affects software vulnerability detection by ChatGPT. It establishes both how carefully crafted prompts (including auxiliary code-structure info and chain-of-thought reasoning) improve LLM-based code analysis, and what their limitations are regarding model bias and contextual understanding."
  },
  {
    "filename": "multi-role-consensus-through-llms-discussions-for-vulnerability-detection.pdf",
    "analysis": "Here is an analysis of the paper, drawing on its text and addressing each of your questions:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point:**  \nThe paper proposes and evaluates a novel approach for vulnerability detection in code using Large Language Models (LLMs). Rather than prompting the LLM from a single “tester” perspective (as in prior work), the paper (i) simulates a collaboration between different software development roles (mainly “tester” and “developer”) and (ii) allows these LLM-simulated agents to *discuss* and iterate toward a consensus on whether a given code segment is vulnerable.\n\n**Goal:**  \n- To see if this multi-role “discussion and consensus” process between simulated team members improves detection accuracy.\n\n**Unique Aspects:**  \n- **Multi-role Simulation:** Previous work limited LLM prompting to a single role (tester). This work introduces a process of alternating prompts as different roles and simulating dialogue.\n- **Iterative Consensus:** The approach uses rounds of discussion between LLM-agent tester and developer to reach a well-supported answer, much like real code review.\n- **Demonstrated Performance Gains:** Shows measurable improvements in precision, recall, and F1 over single-role prompting.\n\n---\n\n### 2. What dataset did they use?\n\nThey used a **C/C++ dataset** (referenced in [6]: SySeVR: IEEE TDSC 2022) with both vulnerable and non-vulnerable code, across four vulnerability categories:\n- Function call (FC)\n- Arithmetic expression (AE)\n- Array usage (AU)\n- Pointer usage (PU)\n\nThe dataset was split into different groups with varying ratios of vulnerable/non-vulnerable samples.\n\n---\n\n### 3. What LLM models were used?\n\n- **gpt-3.5-turbo-0125**:  \nAll experiments used this model.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThe paper investigates **two main prompting strategies**, both for the single-role and multi-role (discussion) settings:\n\n- **Basic Prompt:**  \nDirectly asks the LLM about the presence of vulnerabilities in one of the four categories, without extra context or reasoning.\n- **Chain-of-Thought (CoT) Prompt:**  \nAsks the LLM to analyze the existence of vulnerabilities step-by-step, i.e., providing a reasoning chain before making a decision.\n\nIn the **multi-role setting**, these are embedded in a process with the following stages:\n- **Initialization:** Tester receives the code and makes an initial judgment (with brief reasoning; CoT or basic).\n- **Discussion:** The tester and developer exchange insights (“pose query – deduce response – relay insight”), iteratively challenging/refining each other's reasoning for up to 5 rounds.\n- **Conclusion:** The tester provides a final verdict after discussion.\n\n---\n\n### 5. What specific prompts were used?\n\n- **Basic:**  \n“Is there a vulnerability of *type X* in this code segment? 1 for vulnerable, 0 for non-vulnerable. Briefly explain.”\n\n- **Chain-of-Thought:**  \nInstead of only giving a binary answer, the LLM is asked to reason step-by-step before making a decision.\n\nThe **exact prompt text** isn’t provided in the excerpt, but is described and referenced as being similar to prior work ([3], [4]).  \nA concrete discussion transcript can be found at: [github.com/rockmao45/LLMVulnDetection](https://github.com/rockmao45/LLMVulnDetection).\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- In **Section 2 (MULTI-ROLE APPROACH):** The process flow and the role-based prompting are described.\n- In **Section 3.1 (Experiment Settings):** The basic and CoT strategies are listed, and distinctions are made.\n- Algorithmic details (e.g., round depth, token limits) mentioned here as well.\n- The paragraph starting with “Inspired by [3], the following two types of prompts are used...” gives the prompt strategies.\n\n---\n\n### 7. What are the results of the prompting strategies and how successful were they?\n\n**Results:**\n- **Multi-role with Discussion consistently outperformed single-role prompting.**\n- On average, across all dataset groups:\n    - Precision ↑ 13.48%\n    - Recall ↑ 18.25%\n    - F1 Score ↑ 16.13%\n- Chain-of-Thought (CoT) prompting in either single or multi-role setting usually improves results over basic prompting, but *biggest gains* are from adding multi-role discussion.\n\n**Breakdown:**  \nImprovement is especially notable when the proportion of vulnerable code is high (e.g., Group 1).\n\n**Success:**  \nThe strategy was clearly successful: iterative, multi-role dialogue among LLM “agents” led to more accurate vulnerability detection.\n\n**Cost:**  \nRequires more computation and tokens (~5×), as role agents hold discussions over several rounds.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **Multi-role discussion** allows the LLM to simulate collaborative code review, where different perspectives (tester vs developer) can catch, clarify, or refute each other’s errors or misapprehensions. This mimics real-world code reviews, where dialogue and challenge refine the assessment.\n    - Example: Tester flags an issue, developer points out a reason it isn’t vulnerable, tester re-evaluates and corrects the assessment.\n- **Chain-of-thought** helps by forcing the LLM to lay out logical reasoning step-by-step, which improves accuracy by reducing hasty/unsupported answers.\n\nIn summary:  \n*Adding multiple perspectives and structured reasoning/iteration both help, but multi-role exchange enables real correction of false positives/negatives via simulated debate.*\n\n---\n\n### 9. All specifics of the researchers' prompt strategies.\n\n- **Roles:** Tester (initial judgment), Developer (challenges/adds context)\n- **Phases:**\n    - *Initialization*: Tester prompted with code + task, outputs binary judgment (1/0) + brief reasoning (limited tokens).\n    - *Discussion*: Tester’s response passed to Developer, Developer responds, then iterative back-and-forth (“pose query - deduce response - relay insight”) for up to 5 rounds.\n        - Enforced with prompt constraints (120 tokens/response, max 5 rounds).\n        - Each round: LLM is told exactly which role to “play,” given conversation history, and instructed to either critique, defend, refine, or reconsider based on the other’s reasoning.\n    - *Conclusion*: Final tester’s response after dialogue is counted as the system’s classification.\n- **Prompt Type:**\n    - *Basic*: Direct binary question + brief explanation.\n    - *CoT*: Chain-of-thought (“think step-by-step”) reasoning leading up to decision.\n- **Judgment Schema:**\n    - Binary output: 1 = vulnerable, 0 = not\n    - Short textual reasoning; constrained for clarity.\n- **Dataset Categories:** Applied to FC (function call), AE (arithmetic expression), AU (array usage), PU (pointer usage) code segment types.\n\n**Practical note:**  \nExact prompt texts are present in the referenced public repo, but the process and structure above capture the main strategy.\n\n---\n\n**Summary Table:**\n\n| Strategy         | Roles     | Prompt Style     | Process                                | Max Rounds | Token Limit | Outcome Metric                        |\n|------------------|-----------|------------------|----------------------------------------|-------------|-------------|---------------------------------------|\n| Single-role      | Tester    | Basic/CoT        | 1 step: answer + brief reason           | N/A         | N/A         | Precision, Recall, F1                  |\n| Multi-role       | Tester+Dev| Basic/CoT        | Init judgment, then back-and-forth until consensus (max 5 rounds) | 5           | 120/response | ↑ 13% Prec, ↑ 18% Recall, ↑ 16% F1   |\n\n---\n\nIf you need more details or specific sample prompts, they are available at [github.com/rockmao45/LLMVulnDetection](https://github.com/rockmao45/LLMVulnDetection), as cited in the paper."
  },
  {
    "filename": "gptscan_detecting-logic-vulnerabilities-in-smart-contracts-by-combining-gpt-with-program-analysis.pdf",
    "analysis": "Sure! Here are detailed answers to your questions about the \"GPTScan\" paper, organized by your requested points:\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Point / Exploration:**  \nThe paper introduces **GPTScan**, a novel tool for the detection of logic vulnerabilities in smart contracts (mainly Solidity code for Ethereum and similar blockchains). The authors identify a critical gap: existing static analysis tools are good at fixed-pattern, control-flow, and data-flow bugs (like re-entrancy, integer overflow), but **fail to detect complex business-logic vulnerabilities**, which make up ~80% of real-world bugs and lead to huge financial losses in DeFi.\n\n**Uniqueness:**\n- Previous attempts to use Large Language Models (LLMs) for this task either relied solely on LLMs (which led to high false positive rates and required expensive, powerful models) or just traditional static analysis (missing high-level semantics).\n- **GPTScan innovatively combines GPT (LLMs) as a code-understanding tool with static analysis:**  \n  - GPT is used not for end-to-end detection, but to interpret code scenarios and properties (semantics).\n  - It breaks down each vulnerability type into code-level scenarios and properties to guide GPT analysis, rather than vague high-level descriptions.\n  - Static confirmation then checks the results from GPT to reduce false positives.\n\n---\n\n**2. What dataset did they use?**\n\nThey use three datasets, summarized in Table 2:\n- **Top200:**  \n  - Contracts with the top 200 market capitalizations (widely used/audited; ~303 projects, 555 Solidity files, 134K lines; no known vulnerabilities).  \n  - Used to measure false positive rates.\n\n- **Web3Bugs:**  \n  - 72 projects (~2573 files, 319K lines) from the Code4rena audit platform, from the Web3Bugs dataset (with 48 ground-truth logic vulnerabilities).\n\n- **DefiHacks:**  \n  - 13 projects, 29 files, 17.8K lines. Known hacks with past attack incidents, 14 logic vulnerabilities.\n\nTotal: ~400 projects, 3,100 files, 472K lines, 62 ground-truth logic vulnerabilities.\n\n---\n\n**3. What LLM models were used?**\n\n- **GPT-3.5-turbo** (default model for the tool, due to cost-effectiveness and practicality for large-scale analysis).\n- Some preliminary tests with **GPT-4**, but these showed little increase in accuracy with a 20x cost increase.\n- The choice of GPT-3.5-turbo was enabled by their \"multi-dimensional filtering,\" which kept queries small enough for the 4K token context window.\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\nMain strategies:\n- **\"Scenario and Property Matching\":**  \n  - Instead of vague vulnerability descriptions, they create code-level \"scenarios\" where the vulnerability might happen and \"properties\" that characterize the vulnerable code.\n  - GPT receives these scenarios and properties and is asked, for each function, whether these are satisfied (yes/no, enforced answer format).\n\n- **\"Mimic-in-the-background\" prompting:**  \n  - To reduce GPT's randomness, they instruct GPT to internally \"think\" of five answers and give the most frequent as final answer (inspired by zero-shot chain-of-thought).\n  - Additionally, the temperature parameter is set to zero for determinism.\n  \n- **Separation of scenario and property queries:**  \n  - Scenario matching first, then property matching only for those that pass.\n\n- **JSON-format output enforced:**  \n  - Limits GPT's answer structure, making it easy to parse and harder to drift.\n  \n- **Asking for key variable/statement identification:**  \n  - After detection, GPT is asked to identify the exact variables/statements responsible for the condition, which are then validated through static analysis.\n\n---\n\n**5. What specific prompts were used?**\n\nExamples from the paper:\n\n- **Scenario Matching:**\n  ```\n  System: You are a smart contract auditor...\n  Given the following smart contract code, answer the questions below and organize the result in a JSON format like {\"1\":\"Yes\" or \"No\", \"2\":\"Yes\" or \"No\"}.\n  \"1\": [SCENARIO_1]?\n  \"2\": [SCENARIO_2]?\n  [CODE]\n  ```\n  \n- **Property Matching:**\n  ```\n  Does the following smart contract code \"[SCENARIO, PROPERTY]\"? Answer only \"Yes\" or \"No\".\n  [CODE]\n  ```\n\n- **Variable/Statement Recognition:**\n  ```\n  In this function, which variable or function holds the total supply/liquidity AND is used by the conditional branch to determine the supply/liquidity is 0? Please answer in a section that starts with \"VariableB:\".\n  ... [more such questions] ...\n  Please answer in the following json format: {\"VariableA\":{\"Variable name\":\"Description\"}, ...}\n  [CODE]\n  ```\n\n- **\"Mimic-in-the-background\":**  \n  - The system prompt instructs GPT to internally answer the same question five times and return the most frequent answer; strict output format is enforced.\n\n---\n\n**6. Where are the prompt strategies explained?**\n\n- Scenario and property matching: Section 4.2 and Figure 4.\n- Mimic-in-the-background strategy: Section 4.2, last third (and Figure 4), and also mentioned in Introduction.\n- Variable/statement recognition: Section 4.4 and Figure 5.\n- The prompt formats and details are explicitly shown in Figures 4 and 5.\n\n---\n\n**7. What are the results of the prompting strategies, and how successful or not were they?**\n\n- **Accuracy:**\n  - **Top200 (no known vulns):** 4.39% FP rate (low). Precision 90%+ for these contracts.\n  - **Web3Bugs:** 57.14% precision, recall 83.33%, F1 67.8%.\n  - **DefiHacks:** Precision 90.91%, recall 71.43%, F1 80%.\n\n- **Comparison to other tools:**\n  - Traditional static analysis tools (e.g., Slither) had 0% recall for business logic vulnerabilities, or very high FP rates.\n  - GPT-only tools (i.e., pure prompting with high-level descriptions, as in prior work) had extreme false positive rates (96%, per [34]).\n  - MScan (industry tool) could detect only some price manipulation cases (recall 58%) but nothing else.\n  \n- **Static confirmation (combination with static analysis):**\n  - Cut false positives by 66% compared to GPT-only step, while adding only a few (3) additional false negatives.\n\n- **Discovered 9 previously undocumented vulnerabilities missed by human auditors.**\n\n---\n\n**8. Why did one strategy give better results than another?**\n\n- **Scenario/property-based prompting** with code-level details is better than high-level abstract descriptions:  \n  - This makes it much easier for GPT to map code to vulnerability concepts, avoiding relying on the LLM's prior knowledge or reasoning skills.\n\n- **Forcing structured outputs (JSON, Yes/No), and \"background mimic\" voting** increases determinism, reliability, and parsing, compared to open-ended answers.\n\n- **Multi-dimensional filtering** (pre-filtering functions using names, structure, call graph, access control etc.) means that the LLM sees only realistically relevant code, making detection much more precise and cost-effective.\n\n- **Static confirmation:** GPT can miss or hallucinate details, or not carefully correlate variable dependencies. Static analysis can check data and control dependencies, as well as order of statements and access control, substantially reducing false positives.\n\nWhen GPT-only strategies were used with high-level vulnerability definitions (as in [34]), the LLM was not able to reliably infer codified vulnerabilities, leading to huge false positive rates.\n\n---\n\n**9. All specifics of the researcher's prompt strategies.**\n\n**Prompting Pipeline:**\n- **Filtering:**\n  1. Project-wide filtering (exclude non-source files, library code, OpenZeppelin clones, etc.)\n  2. Function filtering via YAML rules (e.g., function name or content contains certain keywords, public/external visibility, no access control modifiers, etc.)\n  3. Call-graph-based reachability analysis: only include externally reachable functions.\n\n- **Scenario Matching:**\n  - Each logic vulnerability is broken into one or more scenario sentences (what code/functionality should exist for this vuln to occur).\n  - Functions are bulk-checked for scenario match in batched prompts with a forced Yes/No answer, in JSON format.\n\n- **Property Matching:**\n  - For functions that passed scenario matching, send follow-up queries: does the code exhibit certain key properties (\"Is there a clear/reset of approval after transfer completes?\", etc.).  \n  - Again, Yes/No, strictly JSON output.\n\n- **Mimic-in-the-background (Determinism):**\n  - The LLM is told to internally repeat the answer (background \"think\"/\"vote\") five times and return the most frequent, to minimize randomness.\n  - Temperature is set to zero.\n\n- **Variable/Key Statement Recognition:**\n  - For positive matches, the function is sent to GPT with further prompts: \"Which variable holds X?\", or \"Which line updates Y?\", again requiring JSON output.\n  - This focuses static analysis on precise code elements, reducing the verification search space.\n\n- **Static Confirmation:**\n  - For each positive match, tool uses static analysis to check:\n    - Dataflow between variables\n    - Whether value checks are actually enforced in require/assert/if\n    - Statement order\n    - External control of function arguments, etc.\n\n- **Results Merging:**  \n  - Duplications and function → call-graph → function overlaps handled at end for FP/TP/FN calculation.\n\n**Prompt Engineering Details:**\n- System prompts position GPT as an expert auditor.\n- Output format is forced strictly (JSON, Yes/No, labeled variable assignments).\n- No explanation; only structured answers accepted.\n- Double-confirm properties by combining scenario + property in the prompt (\"Does the following smart contract code 'Scenario AND Property'?\").\n\n---\n\nIf you need a breakdown of example YAML filtering rules, or more details from a specific subsection, let me know!"
  },
  {
    "filename": "llmdfa_analyzing-dataflow-in-code-with-large-language-models.pdf",
    "analysis": "Sure! Here’s a detailed analysis answering your questions about the paper \"LLMDFA: Analyzing Dataflow in Code with Large Language Models\":\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n- **Point & Exploration:**  \n  The paper introduces **LLMDFA**, a framework for performing *dataflow analysis* on code using Large Language Models (LLMs) without needing code to be compilable or needing manual customization for each bug type. Dataflow analysis is fundamental for understanding how values propagate through a program, which is directly tied to tasks like bug detection (e.g., divide-by-zero). Traditionally, this analysis requires compilation and significant expert customization, which hinders usability in practice, especially for incomplete or evolving code. LLMDFA aims to remove these hurdles by leveraging LLMs.\n- **Uniqueness:**  \n  - **Compilation-Free:** Works directly from source code, not requiring intermediate representations (IRs) or compilation.\n  - **Customization-Free:** Can adapt to evolving user needs (different types of bugs or analysis) with natural language prompts, removing the need for expert programming knowledge.\n  - **Hallucination Mitigation:** Recognizes that LLMs hallucinate (i.e., make up facts), so it splits the dataflow analysis task into smaller, more manageable subtasks, each handled with specific LLM prompting strategies and external specialized tools (parsers, Z3 SMT solver).\n  - **Tool Synthesis:** Instead of LLMs doing all analysis in their head, they generate scripts/tools (e.g., extractors, constraint solvers) to perform parts of the task deterministically and explainably.\n  - **Few-Shot Chain-of-Thought (CoT) Prompting:** Uses carefully designed prompts with step-by-step reasoning and examples to align LLM reasoning closely with actual code semantics, reducing hallucinations in difficult subtasks.\n\n---\n\n**2. What dataset did they use?**\n\n- **Synthetic/Benchmark Dataset:**  \n  - **Juliet Test Suite:** A widely used benchmark suite for static code analysis. It includes code with labeled vulnerabilities, such as:\n    - Divide-by-Zero (DBZ)\n    - Cross-Site Scripting (XSS)\n    - OS Command Injection (OSCI)\n  - For evaluation, 1,850 DBZ, 666 XSS, and 444 OSCI bugs (comments removed and code obfuscated to prevent ground truth leakage).\n\n- **Real-World Dataset:**  \n  - **TaintBench:** 39 real-world Android malware applications, each with custom sources and sinks (which are diverse and require per-app customization).\n    - Used for evaluating on code that cannot be compiled and needs custom analysis per project.\n\n- **Additional Extended Experiments:**  \n  - Juliet Test Suite (C/C++ version)\n  - SecBench.js (JavaScript vulnerability benchmark)\n\n---\n\n**3. What LLM models were used?**\n\n- **gpt-3.5-turbo-0125** (OpenAI, sometimes called just \"gpt-3.5\")\n- **gpt-4-turbo-preview** (OpenAI, referred to as \"gpt-4\")\n- **gemini-1.0-pro** (Google)\n- **claude-3-opus** (Anthropic)\n\nAll models were tested with temperature set to 0 (greedy decoding, for deterministic outputs).\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\n- **Overall strategy:**  \n  The workflow is split into three main phases, each with specialized prompting or code synthesis techniques:\n    1. **Source/Sink Extraction:** LLMs are prompted to synthesize code that traverses the AST (using tree-sitter) to extract *sources* and *sinks* relevant to the analysis (e.g., user inputs, divisor values).\n    2. **Dataflow Summarization:** LLMs use few-shot chain-of-thought (CoT) prompts, with multiple detailed step-by-step examples, to determine what variables' values flow from source to sink within a function, reasoning about assignments, uses, etc.\n    3. **Path Feasibility Validation:** LLMs are prompted to generate Python scripts that use Z3 SMT solver to check if control flow paths are actually feasible (i.e., if logical constraints along the path can be satisfied).\n\n- **Prompting Tactics:**\n  - *Decomposition of Task*: Splitting a complex, hallucination-prone \"end-to-end\" analysis into more reliable, manageable sub-tasks.\n  - *Code Synthesis Prompts*: Instead of asking the LLM to \"do\" the analysis directly, prompts ask the LLM to write a parser, extractor, or solver script which is then run deterministically.\n  - *Few-shot CoT for Reasoning*: Prompt includes several code examples, each followed by detailed, step-by-step explanations, teaching the LLM how to reason about dataflow facts.\n  - *Fixing Rounds*: If the initial extraction/script is faulty (misses elements or errors), the LLM is prompted with feedback and explains/fixes iteratively until the result matches expectations.\n  - *Path Info Prompting*: When generating constraint-analyzing scripts, the LLM is given explicit path information and a skeleton and asked to generate code accordingly.\n\n---\n\n**5. What specific prompts were used?**\n\n- **Source/Sink Extractor Prompt (paraphrased):**\n> *\"Role: You are a good programmer familiar with ASTs. Please write a Python script traversing the AST and identifying sources/sinks for dataflow analysis. Here are several forms of sources/sinks: [Spec]. Here are example programs and their ASTs: [Examples]. Please refer to the skeleton AST traverser program...\"*  \n  [See Figure 9 in the appendix of the paper for the full template]\n\n- **Dataflow Summarization Prompt (few-shot, CoT):**\n> *\"Role: You are a good Java programmer. Description: Determine whether two variables at two lines have the same value. Here are several rules: ... Here are several examples: ... (example code and explanations) ... Question: Does [Var1] at line [L1] have the same value as [Var2] at line [L2]? Think step by step. Return Yes/No with explanation.\"*  \n  [See Figure 10 in appendix]\n\n- **Path Feasibility Validation Prompt:**\n> *\"Role: You are a good programmer and familiar with Z3 python binding. Please write a Python program using Z3 to encode the path condition. PathInfo: Here is a path: ... You can refer to the skeleton: [skeleton].\"*  \n  [See Figure 11 in appendix]\n\n- **Prompting for Fixes:** If the synthesized script is buggy or misses extractions, the LLM is told precisely what is wrong (missed sources/sinks, error message) and prompted to correct its script.\n\n- **Few-shot Examples:** Each prompt for dataflow summarization or end-to-end analysis contains 3-6 small code examples, each with question (\"Does X flow to Y?\") and detailed justification/explanation. These are meant to teach the LLM the correct semantics, step by step.\n\n---\n\n**6. Where are the prompt strategies explained?**\n\n- **Section 3:** Main description of LLMDFA method (Phases I, II, III—includes high-level strategy and rationale).\n- **Appendix A.2:** Detailed prompt templates for all three phases (Figures 9-11).\n    - **A.2.1:** Principle of prompt design.\n    - **A.2.2:** Templates (with text of actual prompts)\n    - **A.2.3/A.2.4:** Example code, examples used in the prompts, and end-to-end prompting design.\n\n---\n\n**7. What are the results of the prompting strategies, and how successful were they?**\n\n- **Overall:**  \n  LLMDFA outperforms classic static analyzers (CodeFuseQuery, Pinpoint) and LLM-based \"end-to-end\" methods, especially in terms of recall and F1, while maintaining high precision.\n- **Key Results:**  \n    - Juliet benchmarks (gpt-3.5):  \n      - DBZ: Precision 73.75%, Recall 92.16%, F1 0.82  \n      - XSS: Precision 100%, Recall 92.31%, F1 0.96  \n      - OSCI: Precision 100%, Recall 78.38%, F1 0.88  \n    - Real-world Android malware (TaintBench/gpt-3.5):  \n      - Precision 75.38%, Recall 61.25%, F1 0.67  \n    - Each individual phase generally scored >=80-90% on precision and recall (Source/Sink Extraction and Path Feasibility are often nearly perfect).\n- **Ablation studies:**  \n  Removing tool synthesis or few-shot CoT reduces precision, recall, or both; especially, direct extraction and direct LLM end-to-end analysis introduce significant errors and hallucinations.\n- **Script synthesis for external tools (extractors, Z3 solvers)** works almost always in a few rounds of fixing; failures are rare.\n- **LLMDFA's main advantage** is its ability to handle code that can't be compiled and to be tailored easily to new bug types by simply changing prompt specification/examples.\n\n---\n\n**8. Why did one strategy give better results than another?**\n\n- **Decomposition is key:**  \n  Full end-to-end prompting (LLM asked to analyze all at once) suffers heavily from hallucination—especially as the prompt grows (real-world programs, long files). Breaking down the task into well-scoped, smaller problems (extract just source/sink, then just dataflow, then just path feasibility) means each subtask is easier and more reliable for the LLM.\n- **Tool synthesis for determinism/explainability:**  \n  Writing code (extractors/parsers/scripts) using LLM allows deterministic, testable, modifiable processes, reducing random errors and improving transparency.\n- **Few-shot CoT guides reasoning:**  \n  Detailed, hand-picked examples, with full reasoning steps and explanations, teach the LLM *how* to reason about dataflow correctly (variable assignment, overwriting, etc.), improving alignment with program semantics and reducing hallucinations. Prompts without such examples/few-shot CoT are less effective, especially at handling complex dataflow scenarios.\n- **External experts for constraint reasoning:**  \n  LLMs are not good at reasoning about logical path feasibility (satisfiability results in particular). By synthesizing programs that use domain-specific solvers (Z3), the process delegates this difficult reasoning to a more reliable tool.\n\n---\n\n**9. All specifics of the researcher's prompt strategies**\n\n- **Three-Phase Decomposition:**  \n  - *Phase I (Extractor Synthesis):*  \n    - Prompt LLM with a role as \"programmer and AST expert.\"\n    - Provide source/sink specs, example programs/ASTs, and a skeleton traverser.\n    - If output misses or mislabels sources/sinks, provide error/missing info and re-prompt for fixes.\n    - *Goal*: Output a reusable Python extractor for analysis.\n  - *Phase II (Few-shot CoT for Dataflow):*  \n    - Prompt LLM as a Java expert.\n    - Provide rules for dataflow (assignment, non-overwriting, etc.).\n    - Include multiple code examples (\"Does x@L1 == y@L2?\") with full explanations, to socialize LLM into correct step-by-step reasoning.\n    - New function/code provided; LLM explains dataflow Yes/No with stepwise logic.\n    - *Goal*: High-precision intra-procedural dataflow summary per function.\n  - *Phase III (Path Feasibility/SMT Synthesis):*  \n    - Prompt LLM as a Z3+Python programmer.\n    - Provide explicit path info and a script skeleton; LLM outputs code to check satisfiability.\n    - If script doesn't work, feed back errors and request fixes (up to 3 rounds).\n    - *Goal*: Validate if dataflow path is really feasible (i.e., is bug real?).\n- **Fallbacks:**  \n  If extractor or script can't be synthesized after a few rounds, fall back to direct LLM prediction for that subproblem.\n- **Iteration:**  \n  Each tool/script is synthesized iteratively, with LLM correcting itself based on execution feedback.\n- **Prompt Templates:**  \n  *Available in full in the paper's appendix as Figures 9, 10, 11 (see A.2.2 and A.2.3), including sample code, explanations, error feedback mechanisms, etc.*\n\n---\n\n## **Summary Table of Strategies**\n\n| Phase               | Strategy                      | Prompt Content                                                      | Execution Style       |\n|---------------------|------------------------------|---------------------------------------------------------------------|----------------------|\n| 1. Source/Sink      | Tool synthesis (code gen)     | Role desc., specs, code+AST exs, skeleton, error-based fixes        | Deterministic script |\n| 2. Dataflow Summary | Few-shot CoT                  | Role desc., rules, several code ex., Q&A, explanations, step-by-step| Direct LLM, few-shot |\n| 3. Feasibility      | Tool synthesis (Z3 script)    | Role desc., path info, skeleton, error feedback/fix cycles          | Deterministic script |\n  \n**Ablations** (tested in paper's Figure 8; removing tool or CoT step):\n- NoSynExt : direct extraction without code synthesis\n- NoCoT : dataflow without few-shot chain-of-thought\n- NoSynVal : direct LLM-based path feasibility prediction (no tool/SMT)\n\n---\n\nIf you want the **exact text of any prompt used** (e.g., for dataflow summarization or extractor script synthesis), see *Appendix A.2.2* (Figures 9–11) in the paper text above.\n\n---\n\n### If you want concrete examples:\n- **Extractor synthesis prompt**: Provides role, specs, AST, skeleton, asks for script, with iterative fixes (see Figure 9).\n- **Dataflow summarization prompt**: Provides role, rules, multiple small code/explanation pairs, then asks for variable pair reasoning, requesting step-by-step answer (see Figure 10).\n- **Feasibility prompt**: Provides path info, script skeleton, with further instructions for fixing as needed (see Figure 11).\n\n---\n\nIf you need **actual prompt text** or details from a specific section or a summary of the ablation studies or case studies, let me know!"
  },
  {
    "filename": "detecting-code-smells-using-chatgpt_initial-insights.pdf",
    "analysis": "Here’s a detailed, structured answer to each of your questions based on the supplied paper, \"Detecting Code Smells using ChatGPT: Initial Insights\":\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**\n- The paper investigates the effectiveness of ChatGPT, specifically version 3.5-Turbo, in detecting code smells in Java projects.\n- It aims to see how well ChatGPT can identify four specific code smells—Blob, Data Class, Feature Envy, and Long Method—classified at different severity levels.\n- It asks whether ChatGPT can perform this task with generic prompts (no details about smell types) versus more detailed, explicit prompts (which enumerate the specific smells of interest).\n\n**Uniqueness:**\n- According to the authors, this is one of the first empirical studies to assess a large language model’s (LLM) ability to detect code smells—a key software quality and refactoring concern.\n- The study contrasts two prompting strategies (generic vs. specific) on the same dataset, quantifies differences with rigorous statistical tests, and analyzes performance over various severity levels.\n- The work complements prior studies on LLMs for code generation, bug fixing, and unit test generation, but focuses on design flaws rather than functional defects.\n\n\n---\n\n## 2. What dataset did they use?\n\n- The study uses the **MLCQ dataset** ([9]):  \n  - This is an industry-relevant code smell dataset.\n  - Includes 14,739 instances covering four code smells: **Blob**, **Data Class**, **Feature Envy**, and **Long Method**.\n  - Each smell instance is labeled with one of four severity levels: **None, Minor, Major, or Critical**.\n  - For this research, the authors focused **only on samples labeled Minor, Major, or Critical** (excluding \"None\"), resulting in **3,291 instances**:  \n    - 974 Blobs  \n    - 1,057 Data Classes  \n    - 454 Feature Envies  \n    - 806 Long Methods\n\n---\n\n## 3. What LLM models were used?\n\n- **OpenAI’s ChatGPT version 3.5-Turbo** was used for all evaluations in this research.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\n**Two main prompting strategies:**\n1. **Generic Prompt**:  \n   - The prompt asks ChatGPT to identify \"bad smells\" in a given block of Java code, without naming or listing the specific smell types.\n   - Purpose: To test ChatGPT's ability to autonomously recognize code smells.\n\n2. **Specific (or Detailed) Prompt**:  \n   - The prompt lists expressly all four target code smells (Blob, Data Class, Feature Envy, Long Method) and asks ChatGPT to identify which are present in the code.\n   - Purpose: To test whether direct, explicit instruction improves LLM performance.\n\nBoth prompts required a specific answer format to allow automated extraction of results.\n\n---\n\n## 5. What specific prompts were used?\n\n- **Prompt #1 (Generic):**  \n  ```\n  I need to check if the Java code below contains code smells (aka bad smells). Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.\n  Please start your answer with “YES I found bad smells” when you find any bad smell. Otherwise, start your answer with “NO, I did not find any bad smell”.\n  When you start to list the detected bad smells, always put in your answer “the bad smells are:” amongst the text your answer and always separate it in this format: 1. Long method, 2. Feature envy...\n  [Java source code with the smell]\n  ```\n\n- **Prompt #2 (Specific):**  \n  ```\n  The list below presents common code smells (aka bad smells). I need to check if the Java code provided at the end of the input contains at least one of them.\n  *Blob\n  *DataClass\n  *FeatureEnvy\n  *LongMethod\n  Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.\n  Please start your answer with “YES I found bad smells” when you find any bad smell. Otherwise, start your answer with “NO, I did not find any bad smell”.\n  When you start to list the detected bad smells, always put in your answer “the bad smells are:” amongst the text your answer and always separate it in this format: 1. Long method, 2. Feature envy...\n  [Java source code with the smell]\n  ```\n\n(See Figures 1 & 2 in the paper.)\n\n---\n\n## 6. Where are the prompt strategies explained?\n\n- Prompt design and specifics are detailed in **Section 2.2: Prompts Definition** (including Figures 1 & 2 for the actual text).\n- The comparison and justification for design are additionally discussed in the methodology and in the research questions (Sections 2 and 3).\n\n---\n\n## 7. What are the results of the prompting strategies and how successful or not were they?\n\n**In summary:**\n- *Overall*, ChatGPT struggled, but **detailed prompts significantly improved performance**.\n  \n- Using **Generic Prompt** (Prompt #1):\n  - Best performance was on \"Long Method\" (F1 = 0.41), worst on \"Data Class\" (F1 = 0.11), and zero correct on \"Blob\".\n  - ChatGPT failed to detect any Blob smells correctly.\n  - For \"Feature Envy,\" precision and recall were low (F1 = 0.23).\n\n- Using **Specific Prompt** (Prompt #2):\n  - Marked improvement for \"Data Class\" (F1 = 0.59; 0.48 higher than generic).\n  - Still poor for \"Blob\" (F1 = 0.19), moderate for \"Long Method\" (F1 = 0.46), and similar to generic on \"Feature Envy\" (F1 = 0.21).\n  - On severity, Prompt #2 yielded F1 scores of 0.52 for critical smells (vs. 0.27 with Prompt #1 for critical), and 0.43 for minor.\n\n- **Statistical test:**\n  - McNemar’s test found the difference **statistically significant**: the odds of ChatGPT being correct are 2.54 times higher with the specific prompt than with the generic.\n  - Absolute Risk Difference: 16% more correct with Prompt #2 overall.\n\n**Success:**  \n- **Specific prompts lead to a significant, albeit still modest, improvement**, especially for “Data Class” and at higher severity levels (Major, Critical).\n- **Generic prompts performed poorly overall**.\n- Even with specific prompts, F1 scores remained below 0.6 in all cases, showing much room for improvement.\n\n---\n\n## 8. Why did one strategy give better results than another?\n\n- **Lack of focus/context in generic prompts**:  \n  - The generic prompt leaves ChatGPT guessing what to look for, so it may miss less obvious smells or interpret the request too broadly.\n- **Direct instruction aids detection**:   \n  - The specific prompt **explicitly lists the smell types**, narrowing ChatGPT's attention to the correct features (i.e., it “knows what to look for”).\n- **Specialization/relevance**:\n  - Since LLMs are sensitive to the information in the prompt, **listing the smell names helps the model activate relevant linguistic/code features or patterns** associated with those smells.\n\n- The improvement was largest for “Data Class” as its symptoms (data-heavy classes, public fields, little behavior) are perhaps more easily recognized when the model is primed by mentioning the term “Data Class”.\n- For complicated, context-heavy smells like \"Blob\" or \"Feature Envy\", both prompts led to poor performance, likely because these require deeper semantic/contextual understanding.\n\n---\n\n## 9. All specifics of the researcher's prompt strategies.\n\n**Prompt Crafting:**\n- Both prompts were iteratively refined using pilot experiments for clarity and consistency.\n- Both require a specific output format for ease of automated result extraction: \"YES I found bad smells. The bad smells are: 1.Long method, 2.Feature envy...\"\n- In both cases, the actual Java code from the dataset was included after the instruction.\n\n**Prompt #1 (Generic):**\n- Asks generally for code smell identification, without context or enumeration of possible smells.\n- Simulates a “base-case” scenario: can ChatGPT guess \"what’s wrong\" without being told what specific problems to look for?\n\n**Prompt #2 (Specific):**\n- Starts with a list of possible code smells (*Blob, *DataClass, *FeatureEnvy, *LongMethod).\n- The model is told to look for those specific smells only.\n- Tests whether explicit enumeration boosts accuracy.\n\n**Parsing/Aggregation:**\n- Only responses matching the output pattern (“The bad smells are: ...”) were auto-parsed.\n- 27 exceptions, where responses didn’t match the pattern, were manually reviewed.\n- For analysis, standard IR metrics (precision, recall, F1) were calculated per smell and severity.\n\n**Other Context:**\n- The authors plan future improvements (not in this study) like adding more context (e.g., code dependencies), using chain-of-thought prompts, or fine-tuning LLMs on smell-rich code.\n\n---\n\n### Summary Table: Prompt Strategies\n\n| Prompt Type        | Smells Listed Explicitly? | Output Format Specified? | Context Given?          | Goal/Use Case                |\n|--------------------|--------------------------|--------------------------|-------------------------|------------------------------|\n| Generic (Prompt #1)| No                       | Yes                      | No                      | Test baseline/generalization |\n| Specific (Prompt #2)| Yes (4 are listed)      | Yes                      | No                      | Test impact of explicitness  |\n\n---\n\n**Citations in Paper:**\n- Prompts described and explained in Section 2.2 and shown in Figures 1 & 2.\n- Prompts compared and results analyzed throughout Sections 3–4, and summarized in tables 2, 3, 4, 5, and 6.\n\n---\n\n**In summary:**  \nThe study found that spelling out what code smells to look for (the specific prompt) helps ChatGPT detect them much better than a generic “look for bad smells” query—though there’s still a long way to go before LLMs can reliably identify code smells in complex, real-world code. "
  },
  {
    "filename": "llms-cannot-reliably-identify-and-reason-about-security-vulnerabilities_a-comprehensive-evaluation-framework-and-benchmarks.pdf",
    "analysis": "Sure, here is a detailed breakdown addressing each requested point based on the provided paper text.\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point:**  \nThe paper aims to **comprehensively evaluate whether large language models (LLMs) can reliably identify and reason about security vulnerabilities in source code**. It proposes a new framework, **SecLLMHolmes**, designed to systematically assess the ability of LLMs to function as security assistants for code vulnerability detection.\n\n**What it tries to explore:**\n- Whether current LLMs (such as GPT-4, PaLM2, Codellama, etc.) can consistently and robustly detect security vulnerabilities in code.\n- How robust their reasoning is, not just binary \"yes/no\" answers.\n- How different factors (prompting, code complexity, real-world code, code perturbations, etc.) affect LLM performance in this domain.\n- The general reliability and trustworthiness of LLMs in a security context.\n\n**What makes it unique:**\n- It introduces **SecLLMHolmes**, a fully automated, scalable, and public framework/dataset (228 scenarios) for benchmarking LLMs in vulnerability detection.\n- Evaluates 8 top LLMs over eight distinct investigative dimensions (determinism, parameter robustness, prompt diversity, reasoning faithfulness, vulnerability coverage, code difficulty, robustness to code changes, real-world case performance).\n- Goes beyond binary answers to assess *reasoning* and robustness, including how models handle variations or trivial augmentations in code (whitespace, variable/function name changes, extra functions).\n- Provides evidence that even advanced LLMs are non-deterministic, brittle, and provide unfaithful or incorrect reasoning in real scenarios.\n\n---\n\n### 2. What dataset did they use?\n\n- **Total size:** 228 code scenarios:\n    - **48 hand-crafted code scenarios** (from 8 of the most critical and diverse CWEs from MITRE Top 25), in C and Python, across three levels of code complexity (easy, medium, hard). Each CWE has six scenarios (three vulnerable and three patched).\n    - **30 real-world code scenarios** (vulnerable and patched samples from 15 recent CVEs in 2023 from open-source projects (Linux, libtiff, gpac, pjsip)).\n    - **150 augmented code scenarios** (trivial and non-trivial augmentations applied to create robustness/stress tests—changes to code such as whitespace, renaming, added safe/unsafe function calls, comments, etc.).\n\nThe dataset **is publicly released** [GitHub: ai4cloudops/SecLLMHolmes].\n\n---\n\n### 3. What LLM models were used?\n\n8 capable chat-based LLMs, both remotely hosted and local, including:\n- **gpt-4** (OpenAI, 1.76T params, 8,192 tokens, remote)\n- **gpt-3.5-turbo-16k** (OpenAI, 175B, 16,385 tokens, remote)\n- **codechat-bison@001** (Google PaLM2, 340B, 6,144 tokens, remote)\n- **chat-bison@001** (Google PaLM2, same)\n- **codellama-7b-instruct** (Meta, 7B, 100k tokens, local)\n- **codellama-13b-instruct** (Meta, 13B, 100k tokens, local)\n- **codellama-34b-instruct** (Meta, 34B, 100k tokens, local)\n- **starchat-beta (StarCoder+)** (BigCode, 15.5B, 8,192 tokens, local)\n\n(Most results/discussions focused on the top five: gpt-4, gpt-3.5, codechat-bison, chat-bison, codellama-34b.)\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n**Four main prompting techniques:**\n\n1. **Zero-Shot Task-Oriented (ZS-TO):**\n   - User simply asks the model to perform a task (e.g., \"Is there an out-of-bound write in this code?\").\n2. **Zero-Shot Role-Oriented (ZS-RO):**\n   - The LLM is assigned a *role* (e.g., \"You are a security expert...\") and asked to perform a task, without examples.\n3. **Few-Shot Task-Oriented (FS-TO):**\n   - Model is given *a few examples* of vulnerabilities/patched code to learn from, but not assigned a special role.\n4. **Few-Shot Role-Oriented (FS-RO):**\n   - Model is assigned a role and given examples.\n   \n**Three categories of prompt templates:**\n- **Standard (S):** Direct, simple question to the model.\n- **Step-by-step Reasoning-based (R):** Asks the model to reason through the problem in steps (like chain-of-thought, multi-step human expert style).\n- **Definition-based (D):** Prompt includes a formal definition of the vulnerability.\n\nThese strategies are **systematically explored** (combinatorially) across the scenarios.\n\n---\n\n### 5. What specific prompts were used?\n\n**Seventeen prompt templates**, each with an ID (S1–S6, R1–R6, D1–D5). A few representative ones:\n- **S1 (ZS-TO):** \"Does the following code contain a [vulnerability type]?\"\n- **S2 (ZS-RO):** Same as S1, but the LLM is told it is a \"helpful assistant.\"\n- **S3:** Same as S1, LLM is told it's a \"security expert.\"\n- **S5 (FS-TO):** As S1, plus an example of a vulnerable code and its patch.\n- **R1:** Same as S1 but with \"Let's think step by step.\"\n- **R2:** As S1, but with explicit multi-step approach instruction (emulates expert workflow).\n- **D1:** Prompt includes the vulnerability's definition.\n\nFor exact text examples, see Table 3 in the paper.\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 3.3, \"Prompt Templates\":** This section lists all prompt techniques and templates, including a summary Table 3 (prompt ID, type, and description).\n- Figure 1 also explains the LLM input format (system prompt, examples, user input), which is how the prompting templates are used.\n\n---\n\n### 7. What are the results of the prompting strategies and how successful or not were they?\n\n**Key Results:**\n- **Overall accuracy is low.** No LLM achieves satisfactory performance overall. Even the best models (e.g., GPT-4) make wrong vulnerability/not-vulnerable decisions frequently.\n- **Prompting matters a lot:** Few-shot and role-oriented prompts generally improve performance (see Table 11, and repeated across Table 13, 14).\n    - Eg. *GPT-4* reaches ~89.5% on some prompts, but performance can drop >15% with other (less guiding) prompts.\n    - Correct answer =/= correct reasoning: Right answer often has *wrong reasoning*, and vice versa (\"unfaithful reasoning\").\n- **Step-by-step reasoning or human-expert chain-of-thought** prompts improve results for some LLMs (GPTs, 'codechat-bison'), particularly when prompting to reason stepwise about code as humans do.\n- **Role-Oriented prompts** (where LLM is told to act as a \"security expert\") are better than simple task-oriented prompts.\n- **Definition-based prompts** (giving a CWE definition) sometimes help, but not always.\n- **Non-determinism:** Even with the same prompt, most LLMs (except at temperature=0) give *different* answers if you run them multiple times, including GPT-4 and PaLM2 (Table 7, 8).\n- **Hard code and real-world scenarios:** For scenarios with complex/multiple functions, or for real-world CVEs, models (all of them) perform poorly: many false positives, can't correctly classify patched code, miss subtle dataflows.\n- **Robustness** is poor—small irrelevant code changes fool the LLMs (see next section).\n- **No prompt strategy is robust:** There is no prompt that always works; all can be \"broken\" by trivial scenario changes.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n**Findings/Reasons:**\n- **Few-shot prompts provide context and examples**, which LLMs use to \"anchor\" their reasoning—reducing random output or hallucinations.\n- **Role-oriented prompts reduce hallucination**: Telling the model it is a security expert constrains it to act more carefully, and improves both reasoning and accuracy.\n- **Step-by-step/chain-of-thought prompts force structured reasoning:** LLMs that are prompted to \"think step by step\" or explicitly enumerate code analysis steps mimic human expert reasoning and are less likely to jump to random errorprone conclusions.\n- **Plain prompts often result in shallow pattern-matching**: Generic ZS-TO prompts sometimes get the answer by chance but fail as soon as scenario is changed.\n- **However**: *No one strategy solves robustness*: Even the best prompt types break with trivial changes—showing current LLMs lack true semantic/causal understanding and rely on shallow statistical correlations.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies.\n\n**a) Prompting Types (Section 3.3/Table 3):**\n- **Zero-Shot (ZS):** No examples, just asks model to perform a task, with or without \"role\".\n- **Few-Shot (FS):** Prompt includes 1–2 example code snippets + patch + reasoning for same vulnerability type to guide model behavior.\n- **Task-Oriented (TO):** Prompt directly asks about the vulnerability (in system/user prompt).\n- **Role-Oriented (RO):** Prompt includes role (e.g., \"act as a security expert\") in the \"system\" prompt.\n\n**b) Three styles of prompt content:**\n- **Standard (S):** Simple \"Does this code contain X?\" questions.\n- **Step-by-Step Reasoning-based (R):**\n    - Appends \"Let's think step by step.\" or \"As a security expert, follow a multi-step approach: 1) Overview, 2) Identify critical subcomponents, 3) Analyze subcomponents in detail, 4) Conclude if vulnerability is present.\"\n    - Some prompts (R4, R5) use few-shot, with step-by-step examples included as \"history.\"\n- **Definition-based (D):** Provide CWE definition in prompt (system or user prompt), asking the LLM to reference that to make its decision.\n\n**c) Variations tested:**\n- Six versions of each type (e.g., S1–S6, R1–R6, D1–D5), with differences in who is assigned as the assistant (\"helpful assistant\" vs \"security expert\") and how much explicit instruction or example is included, as detailed in Table 3.\n- All prompts tested (in relevant experiments) at two temperature settings (0.0, 0.2–1.0).\n- Evaluated across all 48 handcrafted scenarios, 30 real-world CVEs, and 150 augmentations.\n- The best-performing prompt(s) per type/LLM is computed by a combined score (accuracy, reasoning, answer rate).\n\n**d) Analysis metrics:**\n- For each prompt, measured: response rate, accuracy, rate of correct reasoning, ability to faithfully justify the answer. Also how performance changes for each prompt category and difficulty/vuln type.\n\n**e) Robustness to trivial and non-trivial augmentations:**\n- Prompt types tested for their ability to withstand:\n    - Changes in whitespace/comments\n    - Renaming variables/functions\n    - Addition of library calls\n    - Addition of dead/unreachable code\n    - Use (or misuse) of \"safe\" or \"unsafe\" functions\n- All prompt types are susceptible to degradation in accuracy/reasoning w/ such modifications.\n\n**f) Example prompt texts:**\nSee Table 3 and the \"appendix\" (e.g., extraction prompt in Figure 8).\n\n---\n\n### 10. Where are results of robustness and prompting strategies discussed?\n\n- **Section 4.3:** Diversity of Prompts, results in Table 11.\n- **Section 4.4:** Faithful Reasoning, Table 12.\n- **Section 4.5:** Evaluation over Variety of Vulnerabilities, Table 13.\n- **Section 4.6:** Code Difficulty, Table 14.\n- **Section 4.7:** Robustness to Code Augmentations, Table 15 and Figure 7.\n- **Section 4.8:** Real-World Cases, Table 16–17.\n\nExplicit discussions of why some strategies work better and their limitations are summarized at the end of **Section 4** and the **Discussion** (Section 5).\n\n---\n\n## Summary Table: Prompting Strategies Used\n\n| Tech   | Mode          | Content Style   | System Prompt  | Examples Given          | Definition?  | Step-by-Step? | Role Set   |\n|--------|--------------|----------------|----------------|-------------------------|--------------|---------------|------------|\n| S1     | ZS-TO        | Standard       | None           | None                    | No           | No            | No         |\n| S2     | ZS-RO        | Standard       | \"helpful\"      | None                    | No           | No            | Yes        |\n| S3     | ZS-RO        | Standard       | \"security expert\"| None                  | No           | No            | Yes        |\n| S4     | ZS-RO        | Standard       | \"security expert\"| None                  | No           | No            | Yes        |\n| S5     | FS-TO        | Standard       | None           | 1 ex. + patch + reason  | No           | No            | No         |\n| S6     | FS-RO        | Standard       | \"security expert\"| 1 ex. + patch + reason| No           | No            | Yes        |\n| R1     | ZS-TO        | Reasoning      | None           | None                    | No           | Yes           | No         |\n| ...    | ...          | ...            | ...            | ...                     | ...          | ...           | ...        |\n| D1     | ZS-TO        | Definition     | None           | -                       | Yes          | No            | No         |\n| ...    | ...          | ...            | ...            | ...                     | ...          | ...           | ...        |\n\n---\n\n**In sum:**  \n- The paper is a comprehensive, methodologically rigorous evaluation of LLMs for code vulnerability detection.\n- It introduces a large evaluation dataset/framework and tests 8 top LLMs using 17 sophisticated prompting strategies across a variety of code situations.\n- Findings: Current LLMs are brittle, non-robust, and provide unreliable answers and reasoning—even with careful prompting.\n- No prompting strategy is robust enough for real-world use, and models are easily \"broken\" by trivial code changes.\n- The findings and the framework should guide future research into more effective, robust LLM-based code security tools."
  },
  {
    "filename": "comparison-of-static-application-security-testing-tools-and-large-language-models-for-repo-level-vulnerability-detection.pdf",
    "analysis": "Certainly! Here’s a structured breakdown answering each of your questions with precise details drawn from the paper text:\n\n1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n- Point/Goal:\n  - This paper aims to conduct the first comprehensive comparison between static application security testing (SAST) tools and large language models (LLMs) for automated vulnerability detection at the **repository level** (repo-level) across three major programming languages: Java, C, and Python.\n  - It specifically explores their effectiveness, trade-offs, and potential benefits of combining both approaches.\n\n- What’s unique:\n  - First to conduct a **repo-level** vulnerability detection study (most prior work is on function-level).\n  - First to compare 15 SAST tools and 12 LLMs (with 4 prompting strategies) for *real-world* vulnerabilities, not synthetic datasets, across three languages.\n  - Introduces new/curated real-world datasets for Java, C (extended), and especially Python (novel for this study).\n  - First to systematically test SAST tools on real-world Python repos.\n  - Provides extensive empirical evidence for best/worst strategies, including combining (ensembling) SASTs/LLMs.\n\n2. What dataset did they use?\n\nThey curated/used **three datasets**:\n\n- **Java:** Based on repository information from Li et al. [49].\n  - 50 projects, 87 CVEs, 3,986,848 functions, 0.008% vulnerability ratio.\n- **C:** Dataset from Lipp et al. [51].\n  - 6 projects, 85 CVEs, 1,181,556 functions, 0.009% vulnerability ratio.\n- **Python:** Newly constructed for this work.\n  - 78 projects, 98 CVEs, 312,045 functions, 0.18% vulnerability ratio.\n\nEach dataset contains:\n- Real-world repositories from GitHub.\n- CVE information, CWE type, function-level vulnerable location.\n- Vulnerable function identified based on “lines changed in vulnerability-fixing commit”; others are labeled clean.\n\n3. What LLM models were used?\n\n**12 LLMs** across encoder, decoder, and encoder-decoder architectures, distinguished by parameter size (Table 2):\n\n- Lightweight (<1B params):\n  - CodeBERT (0.13B, encoder-only)\n  - GraphCodeBERT (0.13B, encoder-only)\n  - CodeT5 (0.22B, encoder-decoder)\n  - UniXcoder (0.13B, encoder-decoder)\n- Large (>=1B params):\n  - StarCoder (7B, decoder-only)\n  - CodeLlama (7B, decoder-only)\n  - Mistral (7B, decoder-only)\n  - DeepSeekCoder (6.7B, decoder)\n  - StarCoder2 (7B, decoder-only)\n  - CodeQwen (7B, decoder-only)\n  - Llama3 (8B, decoder-only)\n  - Phi3 (3.8B, decoder-only)\n\n4. What are the prompting strategies discussed in this paper?\n\nFour prompting/LlM adaptation strategies:\n\n- Zero-shot prompting\n- Few-shot prompting\n- Chain-of-thought (CoT) prompting\n- Fine-tuning (with LoRA-PEFT for large models, full-tune for small models)\n\n5. What specific prompts were used?\n\nPrompt format (Section 2.5.1):\n\n- A three-part structure:\n  1. **Task Description:** “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”\n  2. **Formatted Input:** code delimited by markers //CodeStart and //CodeEnd.\n  3. **Prediction Marker:** //Detection to prompt model to give output.\n\nVariants:\n- Zero-shot: the above format, code as-is.\n- Chain-of-Thought (CoT): add “Let’s think step by step” to the end of the task description.\n- Few-shot: prepend the above prompt with two labeled examples from training data (one vulnerable, one clean function, with code + label) to the formatted prompt so model sees them as context.\n\n6. Where are the prompt strategies explained?\n\n- Section 2.5 LLM Adaptation Techniques\n  - 2.5.1 Prompt-based Methods (p. 5 of the main document body)\n  - 2.5.2 Fine-tuning-based Methods\n\n7. What the results of the prompting strategies are and how successful or not were they?\n\n- **Fine-tuning (especially for small models):** Highest detection ratios (up to 100%) with a trade-off of high marked-function ratios (false positives).\n- **Prompting (zero-shot, CoT, few-shot):** In general, lower detection than fine-tuning; CoT sometimes helps marginally.\n- **Few-shot:** Not as effective as full fine-tuning; varying results but rarely outperforms fine-tuning.\n- **Chain-of-thought:** Small improvements or sometimes negligible compared to zero-shot.\n\n**Summary of success:**\n- Fine-tuning beats prompting methods for both detection and false positive rate.\n- Prompt-based methods are less effective for this task, as confirmed by lower vulnerability detection rates and often high false positive rates (marked function ratios).\n\n8. Why did one strategy give better results than another?\n\n- **Fine-tuning** performs better because:\n  - It exposes the model directly to the distribution and specifics of vulnerability patterns in code, allowing specialized adaptation of weights.\n  - Prompt-based methods, especially zero/few-shot, rely on the model’s latent general knowledge, which is insufficient for highly imbalanced and complex tasks like real-world vulnerability detection.\n  - High class imbalance (very few vulnerable functions) means models fine-tuned on balanced data learn to distinguish better, while prompt-based methods over-mark or under-mark due to lack of nuanced adaptation.\n\n9. All specifics of the researcher's prompt strategies.\n\n- **Prompt** always starts:\n  - “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.”\n  - Code placed between:\n    ```\n//CodeStart\n<source code of function>\n//CodeEnd\n```\n  - Model is cued to answer at:\n//Detection\n\n- **Zero-shot:** Just above, no examples.\n- **CoT:** As above, but add: “Let's think step by step” after the task description.\n- **Few-shot:** Prompt is preceded by two code label pairs (one clean, one vulnerable) formatted identically, immediately prior to the instruction and test case.\n\n- Fine-tuning: All model weights trained on a balanced set of vulnerable and non-vulnerable functions (with same ratio in val set); for large LLMs, LoRA PEFT is used, small LLMs use full fine-tuning.\n\n**Additional Research Implementation Details:**\n- For each function in a repo, apply the LLM independently and aggregate function labels per repo for repo-level verdict (Section 2.1.3).\n- Metrics: Vulnerability Detection Ratio (true positive rate), Marked Function Ratio (proxy for false positives).\n\n---\n\n**Summary Table for Prompting Strategies in This Paper:**\n\n| Strategy         | Prompt Template | Examples Used | “Let’s think step by step” | Tuning?          |\n|------------------|----------------|---------------|---------------------------|------------------|\n| Zero-shot        | Base prompt    | None          | No                        | No               |\n| Chain-of-thought | Base prompt    | None          | Yes                       | No               |\n| Few-shot         | Base prompt    | Two (1 vuln, 1 clean) | No                | No               |\n| Fine-tune (sm)   | N/A            | N/A           | N/A                       | Full fine-tuning |\n| LoRA-PEFT (lg)   | N/A            | N/A           | N/A                       | PEFT, LoRA       |\n\n**Locations in Paper:**\n- Section 2.5 (Prompt Definition and Variants)\n- Table 3 (Main results)\n- Section 4.x (Results by method and language)\n\n**Performance Summary:**  \n- Fine-tuning clearly outperforms all prompt-based approaches for this code security task. CoT and few-shot have minor impact, but do not close the gap.\n- Prompt choice and adaptation methods directly influence detection rates and marked function ratios.\n- Combining (ensembling) LLMs or SASTs can mitigate certain drawbacks depending on the language/task.\n\nIf you need copyable/example prompts as used for the models, let me know!"
  },
  {
    "filename": "sanitizing-large-language-models-in-bug-detection-with-data-flow.pdf",
    "analysis": "Certainly! Here is a detailed, structured summary based directly on the paper text, answering your questions precisely:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**\n\n- The paper addresses the problem of hallucinations (false positives) in bug detection using Large Language Models (LLMs) when applied to code, especially via few-shot chain-of-thought (CoT) prompting.\n- It proposes a new methodology (LLMSAN) that enforces LLMs not just to find bugs, but to emit a structured “data-flow path” as proof for each detected bug. These data-flow paths are then rigorously validated to filter out hallucinations.\n- The goal is to bridge the gap between LLM reasoning (often imprecise about code semantics) and the strict requirements of static bug detection, particularly for non-experts and incomplete, uncompilable code.\n\n**Uniqueness:**\n\n- Unlike past work, LLMSAN combines data-flow path generation with a **sanitization pipeline** composed of both classic parsing-based and LLM-based validators (“sanitizers”).\n- Validates bugs not just via natural language explanation, but through semantically meaningful (and independently checkable) data-flow paths.\n- Supports end-users in customizing bug detectors via few-shot CoT prompting—no need for deep program analysis or compilation infrastructure.\n- Applies “program-property decomposition”: splitting the property validation into manageable, local, and syntactic vs. semantic pieces.\n\n---\n\n### 2. What dataset did they use?\n\n- **Synthetic benchmark**: Subsets of the **Juliet Test Suite for Java**, selecting 100 programs for each of five frequent bug types (Absolute Path Traversal, Cross-Site Scripting, OS Command Injection, Divide-by-Zero, Null Pointer Dereference).\n- **Real-world applications**: **TaintBench** dataset, consisting of 39 Android malware applications with complex, real-world information-leak (taint) flows.\n\n---\n\n### 3. What LLM models were used?\n\nThe evaluations covered four different commercially available or state-of-the-art models:\n\n- **gpt-3.5-turbo-0125** (\"gpt-3.5\")\n- **gpt-4-turbo-preview** (\"gpt-4\")\n- **Gemini 1.0 Pro** (\"gemini-1.0\")\n- **Claude 3 Haiku** (\"claude-3\")\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThe paper discusses two primary strategies:\n\n- **Few-shot Chain-of-Thought (CoT) Prompting for Bug Detection**: The LLM is prompted with several buggy code snippets along with explanations and explicit data-flow paths that correspond to true bugs, “teaching” the model how a bug manifests as value flows in a program.\n    - Outcome: the LLM, when given novel code, emits data-flow paths it believes indicate a bug.\n\n- **Sanitization Prompting for Local Property Validation**: To validate each part (start/end, intermediate step) of a proposed data-flow path, the authors employ targeted prompts—either to a parser (for syntactic properties) or to the LLM itself (for semantic properties):\n    - **Functionality Sanitizer Prompts**: LLM is asked whether a value at a certain line in a given function can (for example) be zero.\n    - **Reachability Sanitizer Prompts**: LLM is asked, for two code locations, whether a value can propagate from one to the other, given the code.\n    - All prompts use few-shot CoT format to encourage step-by-step reasoning.\n\n---\n\n### 5. What specific prompts were used?\n\n**Functionality Sanitizer Prompt (Figure 5):**\n```\nExamples: Here are several examples containing DBZ bugs: [examples]. Please understand how zero values are produced.\nTask: Analyze the code: [function]. Please check whether [value] at line [linenumber] can be zero. Please think step by step and conclude the answer with Yes or No.\n```\n- This is used to ask: can this value possibly be 0 at this line, given the function body? (Prompt includes examples of what constitutes a bug.)\n\n**Reachability Sanitizer Prompt (Figure 6):**\n```\nExamples: There are examples containing DBZ bugs: [examples]. Please understand how program values are propagated.\nTask: Analyze [function1], [function2]. Please check whether [value1] at line [linenumber1] can be propagated to [value2] at line [linenumber2]. Please think step by step and conclude the answer with Yes or No.\n```\n- This checks if a value at one location can reach another, used to validate intermediate steps in a data-flow path.\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 4 (“LLM Sanitization for Bug Detection”)** details the two-stage structure (data sanitization and flow sanitization) and lays out the reasoning for each sanitizer.\n- **Prompt templates** are explicitly shown in **Figure 5 and Figure 6** (see “FunctionalitySanitizer”/“ReachabilitySanitizer”).\n- Each sanitizer's prompt logic is described with references to examples (e.g., how they are instantiated for DBZ bugs).\n\n---\n\n### 7. What the results of the prompting strategies are and how successful or not were they?\n\n- **Few-shot CoT alone (without sanitization)** achieves moderately high recall (~74%) but much lower precision (~69%), with many hallucinated/false-positive bug reports.\n- **LLMSAN (few-shot CoT + sanitization pipeline)** improves bug detection precision dramatically (up to **91.03% precision**, similar recall), consistently outperforming unsanitized approaches and even industrial static analyzers and existing customizable bug detectors.\n- The **sanitization pipeline** identifies about **78% of all spurious data-flow paths** (on average), with very little loss of true positives (valid data-flow paths).\n- On **real-world code (TaintBench)**, LLMSAN surpasses CodeFuseQuery’s precision by 15.36% and recall by 3.61%—a practical gain considering CodeFuseQuery’s need for expert customization.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **Few-shot CoT** alone often produces *hallucinations*: bug reports based on incorrect or impossible data flows (e.g., failing to respect semantic constraints or the code’s control flow).\n- **Sanitization pipeline** succeeds because:\n    - It forces **data-flow paths** to be more than just text—they are independently checkable, step-wise certificates for a bug.\n    - **Syntactic checks** (type/order sanitizers using a parser) quickly filter impossible bugs based on code structure/types.\n    - **Semantic checks** (functionality/reachability using LLMs on code snippets) reduce hallucination by analyzing (in isolation) whether values can *actually* take/follow the proposed data-flow.\n    - **Decomposition**: Localized, simple checks are easier for LLMs to get right and minimize error/hallucination, compared to asking for “global” code reasoning or explanation in natural language.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies.\n\n**Prompting pipeline:** Each bug report (i.e., a data-flow path produced by the LLM) is validated as follows (Section 4):\n\n#### Data Sanitization\n\n- **Type Sanitizer** (Parser-based, no prompt): Start and end values of the data-flow path must match the syntactic types seen in the bug examples.\n    - E.g., For DBZ, the bug examples show zero values arise from numeric literals or function calls; a data-flow path starting from a string literal is rejected.\n\n- **Functionality Sanitizer** (Prompt-based, LLM): For each start/end value-location, the LLM is prompted (using Figure 5 template) to determine:\n    - “Can this value at this line in this function be zero?” (or, for other bugs, “Could this value be null?”, etc.)\n    - Uses few-shot CoT, includes [examples], asks for step-by-step reasoning.\n\n#### Flow Sanitization\n\n- **Order Sanitizer** (Parser-based, no prompt): Each adjacent pair in the data-flow path must occur in valid control-flow order (e.g., the “from” statement must execute before the “to” statement).\n    - Derived from control-flow graphs using parsing (not LLM prompting).\n\n- **Reachability Sanitizer** (Prompt-based, LLM): For each pair, the LLM is prompted (Figure 6 template) to answer:\n    - “Can value at line X in function Y propagate to line Z in function W?” (step by step, yes/no).\n    - Again, uses few-shot CoT and concise code snippets.\n\n#### Characteristics of Strategies\n\n- Sanitizers validate **local, basic properties** in isolation, rather than challenging the model with the entire program.\n- Prompts are **few-shot, with annotated “proof” data-flow path examples** to ground LLM understanding.\n- **Temperature is set to 0** (greedy decoding) during validation to reduce randomness/variability in LLM outputs.\n- The entire pipeline allows for easy plugging in of new sanitizers focusing on other properties, inspired by compiler design.\n\n---\n\n**In summary:** The paper demonstrates that combining few-shot CoT for LLM bug detection with modular, local property-based sanitization—using both deterministic parsing and focused LLM semantic prompts—greatly reduces hallucinations and improves the reliability and usability of LLM-driven program analysis. The core contribution is the explicit, checkable data-flow path interface, together with structured prompting and property decomposition that play to both LLM and classical analysis strengths."
  },
  {
    "filename": "skipanalyzer_a-tool-for-static-code-analysis-with-large-language-models.pdf",
    "analysis": "Here is a detailed breakdown of the requested information based on the paper \"SkipAnalyzer: A Tool for Static Code Analysis with Large Language Models\". Each point answers your question specifically, using explicit references and evidence from the provided text.\n\n---\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point of the paper:**  \nThe main goal is to introduce **SkipAnalyzer**, a tool powered by Large Language Models (LLMs) (specifically ChatGPT) for static code analysis. SkipAnalyzer can:\n- Detect bugs in code,\n- Remove false positive warnings,\n- Generate patches (repairs) for detected bugs,\nall **without human intervention**.\n\n**What is it trying to explore?**  \nThe paper explores the application of LLMs in three core static code analysis tasks:  \n1. Static bug detection,\n2. False positive warning removal,\n3. Automated bug repair.\n\nIt also compares the effectiveness of different LLM versions and prompting strategies, and measures SkipAnalyzer’s performance against state-of-the-art tools.\n\n**What makes it unique?**\n- First, prior research has not comprehensively explored LLM-powered tools across **all three static analysis tasks** (detection, false positive filtering, patch generation).\n- It pushes beyond code generation by employing LLMs for high-fidelity static analysis and not just reasoning over code, but also patching it.\n- They evaluate the tool on two critical bug types (Null Dereference and Resource Leak) using a real-world, manually curated dataset.\n- They systematically study **prompt engineering strategies** (zero-shot, one-shot, few-shot) and **multiple LLM models**, directly evaluating their impact.\n- The authors release their dataset and code for reproducibility.\n\n---\n\n### 2. What dataset did they use?\n\n- They focus on two bug types: **Null Dereference** and **Resource Leak**.\n- Used **Infer**, a static analysis tool, to scan 10 popular open-source Java projects (from repositories like Alibaba, Microsoft, Apache, Google, and community projects).\n- After manual verification, the dataset contained:\n  - **222 Null Dereference bugs**\n  - **46 Resource Leak bugs**\n- All bugs/warnings were **manually labeled** as true or false positives by experienced developers. Ground-truth bug patches for repair experiments were also manually created.\n\n---\n\n### 3. What LLM models were used?\n\nThe paper evaluates on two popular, API-accessible models from OpenAI:\n- **ChatGPT-3.5 Turbo**\n- **ChatGPT-4**\n\nThey use the default settings provided by OpenAI for these models (token limits: ChatGPT-4 – 8192, ChatGPT-3.5 Turbo – 4097). Both models are used as the backend for all analysis experiments.\n\n---\n\n### 4. What prompting strategies are discussed in this paper?\n\nThree main prompting strategies, **all common in LLM prompt engineering**, are discussed and compared:\n- **Zero-shot prompting:** The LLM is presented with only the task and input (no examples) and asked to perform the operation.\n- **One-shot prompting:** The LLM is provided with one concrete example before performing the operation on the task input.\n- **Few-shot prompting:** The LLM is given multiple examples (in this work, K=3) as demonstrations prior to the actual input.\n- **Zero-shot Chain-of-Thought (CoT):** They mention also requesting explanations or intermediate reasoning steps from the LLM to potentially enhance performance.\n  \nAll of these strategies are explained in the methodology (Section IV and Section V-C \"Prompting Strategies\").\n\n---\n\n### 5. What specific prompts were used?\n\n**Categories of Prompts:** Each component (Detection, False Positive Filtering, Repair) gets a specialized prompt with instructions tailored to the bug type (e.g., Null Dereference, Resource Leak).\n\n- For **bug detection**, the prompt includes:\n  - Specification of common bug patterns (e.g., no null checks before dereferencing).\n  - Instruction to give a structured output for easier parsing.\n  - For Resource Leak, similar bug patterns are specified.\n- For **false positive filtering**:\n  - The prompt gives both the buggy code and the static analyzer’s warning, asking the LLM if it’s accurate.\n- For **bug repair**:\n  - Prompt includes the buggy code and warning, requesting the LLM to generate a patch.\n\n**Prompt Structure:**  \nAll prompts allow for the inclusion of examples (for one-shot/few-shot strategies). They always request the LLM to also *explain its reasoning/decision*.\n\n**Exact prompt texts** are not quoted in full in the paper, but descriptions are found in Section IV and Section V-C. The structure is:  [Code snippet] + [Warning, if applicable] (+ [Demonstration examples]) + [Instructions/specification of what to output].\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Prompt engineering and strategies** are described in Section IV (\"APPROACH\") in the sub-sections for each analysis component.\n- Full details of **prompting strategies** (zero-shot, one-shot, few-shot, and the rationale for the number of examples) are explained in Section V-C (“Prompting Strategies”).\n\n---\n\n### 7. What are the results of the prompting strategies and how successful or not were they?\n\n**Static Bug Detection (RQ1):**\n- SkipAnalyzer outperformed Infer in all key metrics.\n- *Best performance for both bug types was always achieved with ChatGPT-4 and zero-shot or few-shot strategies.*\n    - For Null Dereference:\n        - Accuracy: 68.37%, Precision: 63.76%, Recall: 88.93% (zero-shot, ChatGPT-4)\n        - Infer’s Precision: 50.9% — so this is a 12.86% precision improvement.\n    - For Resource Leak:\n        - Accuracy: 76.95%, Precision: 82.73%, Recall: 55.11% (zero-shot, ChatGPT-4, Table II)\n        - Infer’s Precision: 39.6% — a 43.13% improvement in precision.\n\n**False Positive Warning Removal (RQ2):**\n- Best strategy is usually **few-shot with ChatGPT-4**, leading to highest gains in precision when filtering.\n    - E.g., False positive precision improvement for Null Dereference: up to 28.68% over base Infer (Table III).\n    - For Resource Leak, up to 9.53% precision improvement.\n\n- SkipAnalyzer always outperformed all state-of-the-art filter baselines (feature-based, neural, and GPT-C).\n\n**Bug Repair (RQ3):**\n- Only zero-shot prompting used (token/context limitations).\n- SkipAnalyzer achieved logic correctness up to 97.3% (Null Dereference, ChatGPT-4) and 91.77% (Resource Leak).\n- VulRepair baseline achieved only ~18% and ~13% correct, respectively.\n- Syntactic correctness (parsing) was always ~100% for SkipAnalyzer.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n**Zero-shot with ChatGPT-4** generally worked best, especially for bug detection, because:\n- ChatGPT-4 is larger/more powerful, so needs fewer demonstrations for optimal reasoning.\n- For this type of structured task (well-defined bug patterns, limited scope), the model performed well just with instructions and context, and demonstration examples did not add as much value.\n- Few-shot helps in some tasks (false positive filtering) because including examples made the model more robust given the subtlety in distinguishing true from false warnings.\n\n**Why not always few-shot?**\n- Token length constraints—can't always fit multiple examples.\n- For some tasks, extra examples didn't help much, or the base model was powerful enough for zero-shot.\n- They empirically selected the best combination by F1 score per task (see Table II and Table III for which model+strategy combination performed best).\n\n---\n\n### 9. All specifics of the researcher's prompt strategies:\n\n#### a) For Static Bug Detection:\n- **Input:** Code snippet (at method level, due to token limit)\n- **Instructions:** Outline bug patterns relevant to the target bug type\n    - E.g., for Null Dereference: “Check for dereferences without null checks”; for Resource Leak: “Check that resources are properly released.”\n- **Output requirements:** Clearly structured (to allow parsing by the tool)\n- **Support for examples:** Yes, prompt can include 0, 1, or 3 demonstration examples, depending on zero/one/few-shot strategy.\n\n#### b) For False Positive Warning Removal:\n- **Input:** Code snippet + associated static analysis warning (from Infer or SkipAnalyzer)\n- **Instructions:** \"Is this warning a true bug or a false positive?\"\n- **Support for examples:** Yes, prompt includes demonstration examples in one/few-shot modes.\n- **Special request:** LLM is asked to explain its reasoning (zero-shot CoT)\n\n#### c) For Bug Repair:\n- **Input:** Buggy code snippet, warning (from static analyzer)\n- **Instructions:** \"Generate a patch to fix this bug, output fixed code.\"\n- **Support for examples:** No (multiple large code snippets hit token limits)\n- **Special request:** LLM asked to explain patch if possible (but always zero-shot)\n\n**General practice:**  \n- All prompts include a request for explanation (\"why did you make this decision\"), which in LLM literature is known as zero-shot Chain-of-Thought reasoning ([Wei et al, 2022], [Kojima et al, 2022]).\n\n---\n\n### Summary Table (QR format)\n\n| Task                  | LLM Version(s)                 | Best Prompting Strategy    | Details                                            |\n|-----------------------|-------------------------------|---------------------------|----------------------------------------------------|\n| Bug Detection         | ChatGPT-4/3.5 Turbo           | Zero-shot (best), few-shot| Explicit bug pattern, method-level code input, explanation required |\n| FP Warning Removal    | ChatGPT-4/3.5 Turbo           | Few-shot (Null Deref), zero-shot (Res Leak) | Code + warning, structured answer (true/false), use of examples |\n| Bug Repair            | ChatGPT-4, 3.5 Turbo          | Zero-shot only            | Buggy code + warning, patch requested, no examples |\n  \n---\n\n### References to paper sections:\n\n- Dataset: Section III, Table I\n- Prompt Engineering: Section IV, Section V-C (\"Prompting Strategies\")\n- Prompt Details/Components: Section IV, Section V-C\n- Results: Section VI, Tables II, III, IV, V\n- Performance explanations: Section VI, Section VII\n\n---\n\nIf you need sample prompts or a precise reconstruction of the prompt templates used, let me know!"
  },
  {
    "filename": "to-err-is-machine_vulnerability-detection-challenges-llm-reasoning.pdf",
    "analysis": "Below is a comprehensive, question-by-question summary of the paper \"To Err Is Machine: Vulnerability Detection Challenges LLM Reasoning\" by Steenhoek et al., synthesizing and referencing all details from the provided text.\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point and Exploration:**\nThe paper systematically investigates the ability of state-of-the-art Large Language Models (LLMs) to detect software vulnerabilities in source code. Specifically, it treats vulnerability detection as a complex **code reasoning** task, not a simple pattern-matching or shallow classification problem.\n\nThe authors explore:\n- Whether current LLMs, both general and code-specialized, can reason about subtle code semantics (like bounds checks, pointer dereferences, etc.) necessary for accurate vulnerability detection.\n- How various factors—model size, amount and type of training data, fine-tuning, and sophisticated prompting strategies—impact LLM performance on this task.\n\n**Uniqueness:**\n- It is the first comprehensive, systematic analysis across 14 leading LLMs and 7 prompting methods on a **high-quality, manually curated benchmark** (SVEN), using both automatic metrics (Balanced Accuracy) and in-depth manual error annotation (300 LLM outputs).\n- The work differentiates itself from prior studies by **deeply dissecting where and how current LLMs fail**—not just whether they predict right or wrong, but exactly which reasoning steps (finding relevant statements, understanding semantics, or multi-step logical connections) break down.\n- The paper also provides **fine-grained error categories**, details specific code features LLMS mishandle, and tests if augmenting LLMs with domain knowledge or structured reasoning prompts can close the gap.\n\n---\n\n### 2. What dataset did they use?\n\nThe main dataset is **SVEN** (He & Vechev, 2023):\n\n- Consists of **772 function pairs** from real-world C/C++ projects (386 vulnerable, with corresponding fixed versions).\n- Each function averages 168 lines.\n- Manually-vetted aggregate from several vulnerability benchmarks; claims 94% label accuracy, minimizing label noise that hinders other datasets (e.g., PrimeVul, which is noisier but used for fine-tuning).\n- For some comparisons, they also used **simpler CWE/SARD examples** (canonical vulnerability samples) to see if function complexity alone explained model struggles.\n\n---\n\n### 3. What LLM models were used?\n\nThe study evaluated **14 models** (with varying sizes/variants):\n\n- **OpenAI:** GPT-3.5-turbo, GPT-4-turbo\n- **Google/DeepMind:** Gemini 1.0 Pro\n- **Meta:** LLAMA2 (7B, 13B, 34B), CodeLLAMA (7B, 13B, 34B)\n- **Mistral/Mixtral:** Mistral (7B), Mixtral (8x7B/45B)\n- **StarCoder Family:** StarCoder, StarCoder2, StarChat, StarChat2\n- **WizardCoder**\n- **MagiCoder**\n- **DeepSeek-Coder** (1.3B, 6.7B, 33B)\n\nAll are either state-of-the-art (SOTA) for code or general language tasks as of early 2024. Model IDs and sizes are documented in their appendix.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThe paper explores **7 main prompting approaches**:\n\n1. **Basic (Zero-Shot):** Just asks if the provided function is buggy/vulnerable without any examples.\n2. **In-Context (n-shot):** Provides k examples of code (buggy/not buggy) and model answers for few-shot learning, including:\n   - a. Random selection of examples.\n   - b. Embedding-based retrieval: picks semantically similar examples.\n   - c. Contrastive pairs—see below.\n3. **Contrastive Pairs:** Pairs of in-context examples with code *before* (vulnerable) and *after* (fixed) a bug-fixing commit to highlight small semantic differences.\n4. **CoT-CVE (Chain-of-Thought from CVE reports):** Example responses include reasoning steps, adapted from offical Common Vulnerabilities and Exposures (CVE) bug report narratives, drawn from the Big-Vul dataset.\n5. **CoT-StaticAnalysis:** Chain-of-Thought steps derived from the reasoning output of a static analysis tool (Infer), using buggy-path proofs from D2A dataset.\n6. **CoT-Annotations:** Authors’ own method—adds explicit domain annotations (e.g., marking null assignments, checks, and dereferences) and provides step-by-step reasoning tailored for common weak spots (e.g., Null-Pointer Dereference) in the prompt.\n7. **(Baseline) Random-Guess:** A random classifier for comparison.\n\nPrompting strategies are explained in the main text (Sections 2, 3.3) and **detailed in Appendix A**.\n\n---\n\n### 5. What specific prompts were used?\n\n**Basic Prompt Examples (from Appendix A):**  \nSystem:  \n> \"I want you to act as a vulnerability detection system.\"  \nUser:  \n> \"Is the following function buggy? Please answer Yes or No.\"  \n(Variants: “Is the following function vulnerable?”, or prompts pre-listing CWE bug types.)\n\n**In-Context (n-shot, including contrastive pairs):**  \n- Provides several Q&A examples with code and ‘Yes/No’ answers, sometimes appending both a buggy and a fixed variant as paired in-context examples.\n\n**CoT-CVE Prompt Example:**  \n- In-context example includes code snippet +  \n> \"The crypto_skcipher_init_tfm function ... relies on a setkey function that lacks a key-size check, which allows ... [bug explanation]. Therefore, the example is buggy.\"\n\n**CoT-StaticAnalysis Prompt Example:**  \n- Code snippet +\n> \"1. A buffer buf of size 10 is allocated at line 1. 2. An index i ... 3. The index i accesses buf ... This may exceed bounds. Therefore, the example is buggy.\"\n\n**CoT-Annotations Prompt Example (for Null-Pointer Dereference):**  \n- Code is annotated with comments pinpointing null assignments, null checks, and dereferences.\n- The prompt adds stepwise instructions:  \n  1. Identify locations where pointers may be null.  \n  2. Which are dereferenced?  \n  3. Which dereferences are checked/handled? Filter these out.  \n  4. If any unchecked dereference remains, the function is vulnerable; else not.  \n- See Figure 8 above for a full prompt/response.\n\n**Full details and templates for all prompt types are provided in Appendix A.**\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Sections 2 and 3.3** provide overviews of all strategies.\n- **Appendix A** (\"Vulnerability Detection Prompts\") gives detailed descriptions and concrete prompt texts for all prompt types, including variants and formatting (how examples are arranged, number of examples, etc.).\n\n---\n\n### 7. What are the results of the prompting strategies and how successful or not were they?\n\n**Overall results (see Table 1, Figure 2 in the text):**\n\n- **Best performance, across all models and prompt types, only slightly exceeded random chance:**\n  - Balanced Accuracy typically between **50–55%** for vulnerability detection (random-guess is 50%).\n  - For comparison, the same models achieve much higher results on natural language, math, or codegen tasks.\n\n- **No prompt or model exceeded 55% Balanced Accuracy** on SVEN; most were closer to random.\n- **Contrastive prompts** improved results in some cases (best-case boost for 8/14 models), but never by more than 5% over baseline.\n- **CoT-oriented prompts** (CoT-CVE, CoT-StaticAnalysis, CoT-Annotations) helped certain models reduce specific reasoning errors, especially in recognizing bounds/NULL checks, but did **not translate into significant end-to-end gains.**\n- Scaling model size, providing more code-centric pre-training data, and fine-tuning (including instruction and adapter fine-tuning) **had minimal impact** on vulnerability detection accuracy.\n\n**Manual error annotation (Section 3, Table 3, Figure 3):**\n\n- Even with advanced prompt engineering, >50% of LLM responses contained significant errors in at least one reasoning step.\n- 43% failed at localization/semantics of statements (bounds, NULL checks).  \n- Many errors were in the kind of code logic (multi-step, variable relation) that vulnerability detection uniquely requires.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **Contrastive and CoT-based prompts** sometimes improved comprehension of the relevant semantic features **within a single step** (e.g., correctly identifying where bounds checks or null checks are added/missing), especially for models like CodeLLAMA and Mistral.\n- **Explicit annotations and step-by-step instructions** made models less likely to overlook certain low-level statement details.\n- However, **even with better recognition of low-level facts, LLMs often failed at integrating them across multiple statements**—the multi-step, relational reasoning needed to connect how a missing check actually exposes a vulnerability (e.g., following pointer assignments through loops, reasoning about possible execution paths).\n\nThus, **domain-specific prompts help with isolated steps**, but not with the holistic multi-step reasoning, so improvements were narrow and did not dramatically improve end-to-end vulnerability detection.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies.\n\nFor **maximum detail and transparency (based on Appendix A and Section 3.3):**\n\n#### **a. Basic (Zero-shot) Prompting**\n\n- System prompt: \"I want you to act as a vulnerability detection system.\"\n- NL instructions: \"Is the following function buggy? Please answer Yes or No.\"\n- Variants: \"Is the following function vulnerable?\" and \"Does the following function contain one of the following bug types?\" (with a list of common CWEs provided).\n- Also tried Q/A format: \"Question: ...\" / \"Answer: ...\"\n- **Purpose:** Minimal context/priming. No examples.\n\n#### **b. In-Context (n-shot) Prompting**\n\n- Function: Supply N input/output examples (code + model answer), followed by the test example.\n- Variations:\n  - (1) Random examples from dataset.\n  - (2) Embedding-based retrieval (e.g., CodeBERT sim. to candidate), to select in-context examples closest in meaning.\n  - (3) Number of examples: tested 1–10; empirically, 6 performed best.\n  - Formatting: All examples in a single chat message was best.\n- **Purpose:** Condition models to recognize task structure by imitation.\n\n#### **c. Contrastive Pairs**\n\n- Two in-context examples as a paired \"before and after\" (vulnerable/fixed) version of the code for the same function.\n- Emphasizes **minimal semantic but maximal security-relevant** difference.\n\n#### **d. CoT-CVE (Chain-of-Thought from CVEs)**\n\n- In-context example includes the code and adapted CVE report narrative (from the Big-Vul dataset) as multi-sentence reasoning tracing how the code is vulnerable, finishing with “Therefore, the example is buggy”.\n- For non-vulnerable examples, a generic response is used.\n- Only high-quality CVE reports (no duplicates, suitable length, matching CWE type) used.\n\n#### **e. CoT-StaticAnalysis**\n\n- Uses the static analyzer Infer on buggy functions to extract the sequence of operations that can trigger a vulnerability (i.e., path to bug).\n- These steps are presented as “1...2...3... Therefore, the example is buggy.”\n- Again, only high-quality, complete reports used.\n\n#### **f. CoT-Annotations (Custom Domain Knowledge Prompt; detailed in Section 3.3 and Figure 8)**\n\n- Static analyzer built by the authors annotates the source code with inline comments marking:\n  - Null assignments\n  - Null checks\n  - Pointer dereferences\n- Prompt then instructs the LLM on explicit reasoning steps (e.g. “Identify which pointers might be null, which are dereferenced, which dereferences are checked, filter out checked ones, if any unchecked remain, function is vulnerable.”).\n- An example response walks through each step before rendering a Yes/No judgment.\n\n#### **g. Prompt Variants and Details**\n\n- For all prompt types, pilot studies tested minor wording and formatting variations (“buggy” vs “vulnerable”, presence of explicit “Question” and “Answer”), with only minimal impact.\n- Summaries of prompt structures and full example prompts are available in Appendix A.\n- For each prompt, extraneous examples (too long/short, irrelevant CVEs, etc.) were filtered for quality.\n\n---\n\n## In Summary:\n\nThe paper conclusively finds that **no current LLMs or prompt engineering methods (including state-of-the-art few-shot, chain-of-thought, or explicitly annotated prompts) can solve vulnerability detection as a code reasoning task**—their performance hovers just above chance, and their errors stem from limited semantic comprehension, not just bad pattern recognition.\n\n**Prompting helps with some single-step reasoning failures, especially where the domain challenge is clear and localized (like missing a null check), but is not enough to enable the multi-step, relational reasoning needed for full vulnerability detection.**\n\n**Appendix A** is the most comprehensive source for all prompt strategies and specific examples used.\n\n---\n\n**If you need specific prompt templates or wording for a particular strategy, let me know!**"
  },
  {
    "filename": "exploring-chatgpts-capabilities-on-vulnerability-management.pdf",
    "analysis": "Below are detailed explanations responding to each of your questions, with precise references to sections of the provided paper excerpt.\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n#### **Point & Exploration**\n- The paper systematically **explores ChatGPT’s usefulness in real-world vulnerability management tasks**, covering the **complete vulnerability management process**.\n- It aims to answer:\n  - Can ChatGPT directly assist software maintainers across the full vulnerability management lifecycle?\n  - Can it perform on par with SOTA (state-of-the-art) approaches?\n  - How do prompt engineering strategies and foundational models affect performance?\n  - What are the key challenges, and how can future directions be shaped?\n\n#### **Uniqueness**\n- **First comprehensive, large-scale evaluation**: 6 distinct real-world vulnerability management tasks, 70,346 samples.\n- Not limited to traditional code or NLP tasks, but tests across a real process, e.g., bug report summarization, security bug identification, severity evaluation, code repair, patch correctness, and stable patch classification.\n- **Systematic comparison with 11 SOTA methods** under identical metrics and datasets.\n- Intensive focus on **prompt engineering**, including “self-heuristic” prompting (extracting knowledge using ChatGPT itself).\n- Reveals practical problems: e.g., performance plateaus with random demonstrations; better results by synthesizing domain expertise; misinterpretation risks when information isn't well-curated.\n\n**References:**  \n- Abstract, §1 Introduction (\"Recently, there has been a...\"), §2 Background, Table 1, and §3 Evaluation Framework.\n\n---\n\n### 2. What dataset did they use?\n\n- The paper **collects datasets from the SOTA approaches for each of the six tasks.**\n- **Total:** 70,346 samples, 19,355,711 tokens.\n- Each dataset exactly matches primary SOTA prior works to ensure fairness.\n- Datasets span:\n  - Bug report summarization: 33,438 samples.\n  - Security bug identification: 22,970 samples.\n  - Vulnerability severity evaluation: 1,642 samples.\n  - Vulnerability repair: 12 samples.\n  - Patch correctness assessment: 995–208 samples (depending on the SOTA method).\n  - Stable patch classification: 10,896 samples.\n\n**References:**  \n- Table 1: Tasks, baselines, and dataset (§2), §3.2 SOTA Approaches and Dataset.\n\n---\n\n### 3. What LLM models were used?\n\n- **ChatGPT** (OpenAI models—via API):\n  - **gpt-3.5-turbo-0301**: Used for prompt design and probe-testing (economical & sufficient for preliminary studies).\n  - **gpt-4-0314**: Used for final, large-scale evaluation due to its higher capabilities.\n- **Also compared**: SOTA methods (not LLMs), plus the “LLMset” from Pearce et al. (including Codex and Jurassic-1) for the vulnerability repair task.\n\n**References:**  \n- §2.2 ChatGPT and Prompt, Footnote 1 in §3.2/3.3, Table 6.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n- **Six main prompting strategies/templates:**\n  1. **0-shot**: Task description only.\n  2. **1-shot**: One random demonstration/example before input.\n  3. **Few-shot**: Several (4) random demonstration examples.\n  4. **General-info**: Task-related role assignments, instruction reinforcement, dialog prompts, and “zero-shot chain-of-thought” to encourage step-by-step reasoning.\n  5. **Expertise**: As above, but with manually summarized, domain-specific expertise included (e.g., definitions, rules).\n  6. **Self-heuristic**: ChatGPT itself summarizes and extracts expertise from multiple demonstrations/examples, which is then fed back as part of the prompt to guide its task performance.\n\n**References:**  \n- Table 2: Templates for task prompt generation, §2.2 Prompt Strategies, §3.1/3.3 Evaluation Pipeline and Prompt Design.\n\n---\n\n### 5. What specific prompts were used?\n\nDetailed examples are provided in Table 13 and through narrative in §3.3 and Appendix A.  \n**Example for Security Bug Report Identification (expertise):**\n```\nSYSTEM: You are Frederick, an AI expert in bug report analysis. Your task is to decide whether a given bug report is a security bug report (SBR) or non-security bug report (NBR). When analyzing the bug report, take into account that bug reports related to memory leak or null pointer problems should be seen as security bug report...\nUSER: I will give you a bug report and you will analyze it, step by step, to know whether or not it is a security bug report. Got it?\nASSISTANT: Yes, I understand...\nUSER: Great! Let's begin then :) For the bug report: [details] Is this bug report (A) a security bug report (SBR), or (B) a non-security bug report (NBR). Answer: Let's think step-by-step to reach the right conclusion.\n```\n**Other prompt examples exist in Table 13 and the main text, customized for each task.**\n\n**References:**  \n- Table 2, Table 12 (skills for general-info), Table 13 (prompt examples), §3.3 Prompt Design, Appendix A.\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 2.2**: High-level prompt design strategies and their context (NLP, in-context learning, role assignment, expertise, etc.)\n- **Section 3.1, 3.3**: Details prompt template construction, selection process, and examples.\n- **Table 2/Tables in Appendix A**: Concrete templates and explanations.\n- **Appendix A/Table 13**: Extensive template and prompt wordings.\n\n---\n\n### 7. What the results of the prompting strategies are and how successful or not were they?\n\n#### **Summary of Results:**\n- **Basic (0-shot) prompts**: Often already strong for tasks close to standard NLP (e.g., bug report summarization).\n- **Demonstration-based (1/few-shot) prompts**: Sometimes improve performance, but randomly chosen demos are not always reliable.\n- **Expertise prompts**: Notable improvements when domain knowledge is crucial (e.g., security bug identification, stable patch classification).\n- **Self-heuristic prompts**:\n  - Particularly strong for tasks requiring delicate domain knowledge condensation (e.g., CVSS severity mapping).\n  - Extracting and synthesizing knowledge with ChatGPT, then feeding it back, yielded *substantially higher scores* compared to random or naïve demonstrations (see Table 5).\n- **Too much or ill-formed information**:\n  - Can result in *lower* scores, hallucinations, or model confusion (e.g., in patch correctness, stable patch tasks), underscoring the importance of curating what, and how much, is fed to ChatGPT.\n\n**Task-specific examples:**\n- **Bug Report Summarization**: 0-shot competitive; few-shot (esp. gpt-4) best; general-info/expertise sometimes unnecessary.\n- **Security Bug Report Identification**: Raw demonstrations can inject noise; expertise prompts clarify and boost recall.\n- **Vulnerability Severity Evaluation**: Only self-heuristic prompts led to strong results, as the model struggled with rule abstraction otherwise.\n- **Patch Correctness/Stable Patch**: Too much irrelevant info can *harm* accuracy—direct, high-quality signals are essential.\n\n**References:**  \n- Tables 3–11, Evaluation Results Section 4 and per-task subsections (4.1–4.6).\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **Relevance and quality of information**: Expertise and self-heuristic prompts explicitly provide or extract task-relevant *rules* or *patterns* that LLMs otherwise fail to infer, especially in complex, nuanced tasks.\n- **Random demonstrations are risky**: They can statistically introduce bias or noise, causing the model to latch onto irrelevant demo details instead of learning correct task patterns.\n- **Information overload**: Excessive or non-specific details (e.g., bug reports with patch code, description, and auxiliary information together) can distract or misguide the model, leading to hallucinations or false reasoning chains.\n- **Self-heuristic summarization**: Works best when *rules or patterns* are non-trivial or difficult to encode by hand, as the model can find and synthesize patterns across many demos more effectively than humans or random selection.\n- **Model capacity matters**: gpt-4 generally outperforms gpt-3.5, but gains are pronouned only if prompts capture necessary task structure and domain knowledge.\n\n**Relevant findings:**\n- \"Providing random demonstration examples in the prompt cannot consistently guarantee good performance...\"\n- “Extracting expertise from demonstration examples itself and integrating the extracted expertise...is a promising research direction…”\n\n**References:**  \n- Summary in Abstract, per-task analysis in Section 4, especially 4.2 (identification), 4.3 (severity).\n\n---\n\n### 9. All specifics of the researcher's prompt strategies.\n\n#### **Prompt Strategies & Implementation Details**\n\n| Strategy        | Description & Implementation Notes                                                                                                |\n|-----------------|-----------------------------------------------------------------------------------------------------------------------------------|\n| 0-shot          | Just task description + the query input.                                                    |\n| 1-shot          | Task description + one random demo (from training set) + input query.                       |\n| Few-shot        | Task description + four random demos + input query. Number of demos chosen per [23].        |\n| General-info    | System prompt sets a role (e.g., \"You are Frederick...\"), reinforces with task info, simulates human-AI dialogue confirmation, includes positive feedback, chain-of-thought (\"Let's think step by step\"), and prompts for a 'right' answer. Skills in Table 12. |\n| Expertise       | Like general-info, but adds task/domain-specific expertise (rules, definitions, summarized from literature or documentation). e.g., specific rules for what constitutes a security bug, or CVSS metric, etc. See Table 14 for resource mappings. |\n| Self-heuristic  | ChatGPT is used to *summarize* expertise from many demos (drawn from training/probe set), and the resulting essence is fed into subsequent system/user prompts (as expertise in the template). E.g., providing 100 labeled examples, asking for a summary of what characterizes CVSS Network metrics, etc.; see Figure 4.                      |\n| Desc-code/code-only (Patch correctness)| For patch correctness, they adapted prompt form after noticing model confusion when giving both code and description. “code-only” inputs proved more accurate than combined inputs. |\n| Variability Control in Evaluation | All prompt testing, selection, and refinement was carried out using only the training/probe data; test data was untouched to ensure evaluation integrity.|\n| Prompt Template Selection | Each strategy first tested on probe-test (small random selection of training data), and the best-performing template per-task used for final large-scale test.                |\n\n**References:**  \n- Table 2, Table 12, Table 13 (Appendix A), §3.1–3.3; Figures 3–4 in main text; evaluation method details, and further template specifics in the appendix.\n\n---\n\n## Short summary (one-liner per question)\n\n1. **The paper aims for the first large-scale study of ChatGPT across all major vulnerability MGMT tasks, evaluates its strengths/limits, with unique focus on prompt optimization and comparison with SOTAs.**\n2. **Datasets are adopted from previous SOTA works for each task, totaling 70,346 samples.**\n3. **Models: ChatGPT (gpt-3.5-turbo-0301 for prototyping, gpt-4-0314 for main tests); also comparisons to Codex, Jurassic-1 etc. in one task.**\n4. **Prompting strategies: 0-shot, 1-shot, few-shot, general-info, expertise, and self-heuristic (ChatGPT-extracted knowledge), plus special variants for code/desc.**\n5. **Specific tasks had tailored prompts; formats and text examples are provided in main text/Table 13—ranging from simple role+input, to detailed expert instructions and examples.**\n6. **Strategies are explained in Section 2.2, 3.1, 3.3, and Tables 2, 12, 13—with worked examples in the appendix.**\n7. **0-shot/few-shot suffice for simple NLP tasks; domain expertise/self-heuristic are needed for complex tasks; excess/noisy info can hurt; best results come from curating useful knowledge.**\n8. **Strategies succeed/fail based on their ability to deliver distilled, relevant expertise; random or too much info confuses, while precise synthesis (esp. with self-heuristic prompts) lifts performance.**\n9. **Prompt strategies were carefully constructed, selected via probe-tests, included role assignment, domain expertise, and model self-summary; all templates and their selection process are explicitly detailed (see Table 2, 12, 13, and respective sections).**\n\n---\n\n**If you want a table mapping tasks to prompt templates and results, or further breakdown per task, just let me know!**"
  },
  {
    "filename": "effective-vulnerable-function-identification-based-on-cve-description-empowered-by-large-language-models.pdf",
    "analysis": "Absolutely! Here’s a detailed, point-by-point summary tailored to your questions and quoting/synthesizing from the paper above. Bold = summary points, italics = places in main text where you’ll find information.\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n- **Point:** The paper proposes VFFinder, a novel method for identifying vulnerable functions (the actual code locations responsible for vulnerabilities) in open-source software (OSS), using only CVE descriptions (the natural language summaries that accompany security vulnerability disclosures) and source code, but not requiring vulnerability patches or proof-of-concept exploits.\n- **Exploration:** It aims to overcome the limitations of existing methods, which need patches or PoCs (often unavailable or imprecise), by leveraging Large Language Models (LLMs) to extract critical information from CVE descriptions.\n- **Uniqueness:** This is the first work to use LLMs (with in-context learning, ICL) for extracting precise query entities from CVE text, generating a corpus that is cross-matched with source code to localize likely vulnerable functions. It designs custom prompt strategies to enhance this LLM extraction, and demonstrates practical benefit by reducing false positives in downstream software composition analysis tools.\n- *See*: Abstract, Introduction, Approach §4.\n\n---\n\n**2. What dataset did they use?**\n- **Three main datasets** (see §5.1, Data Collection):\n  - **A&R Dataset (DAR):** Manually labeled attack vectors & root causes for 642 CVEs, used to validate the LLM extraction method (i.e., entity extraction from CVE text).\n  - **Vulnerable Function Dataset (DVF):** 77 CVEs from 75 Java projects covering 30 CWE types, with function-level ground truth (from prior work and newly collected PoCs), for vulnerable function localization benchmarking.\n  - **Cross-project Vulnerability Dataset (DCV):** 62 CVEs, 61 vulnerable software, 9,829 downstream instances, used to test integration with SCA tools.\n- *See*: “Data Collection” (§5.1), also Table/Figure references.\n\n---\n\n**3. What LLM models were used?**\n- **Main LLM for extraction:** ChatGPT gpt-3.5-turbo-0613 via OpenAI API (see §4.1.3, Query Design; §6.2, RQ2 experiments).\n- **For prompt selection and semantic ranking:** angle-llama-7b-nli-20231027 (a Llama-2-7b model fine-tuned for Semantic Textual Similarity; used for embedding and retrieving similar prompt examples and for function-code-retrieval ranking).\n- *See*: §4.1.3 (LLMs and prompts), §4.4 (ranking).\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n- Two main LLM prompting strategies for extracting entities from CVE text:\n  - **Direct prompt (no demonstration):** Plain instruction to extract “attack vector” and “root cause” from a CVE description.\n  - **ICL (In-Context Learning) prompt with demonstrations:** Adds several (usually three) carefully selected “demonstration” examples to the prompt to guide the model by analogy.\n    - **Semantic similarity ICL (S-ICL):** Demonstrations are selected based on semantic similarity to the target description.\n    - **Pattern-based ICL (P-ICL):** Demonstrations are selected based on syntactic/pattern similarity (using a custom “pattern embedding” method).\n- *See*: §4.1.3 (Prompt Format); §4.1.2 (Demonstration Retrieval); §6.2 (comparison among CaVAE, direct, S-ICL, P-ICL).\n\n---\n\n**5. What specific prompts were used?**\n- **Instructional template** as per Figure 5:\n  - Instruction: “I want you to act like an expert in vulnerability report analysis, I will give you a description. Your task is to extract the attack vector and root cause from it. Attack vector is the method of triggering the vulnerability. If you can not find the corresponding answer, just answer None.”\n  - Followed by three demonstration cases, each with “Description: <text>” and “Answer: {‘attack vector’:..., ‘root cause’:...}”.\n  - Then “CVE Description: <target description>”.\n- The three demonstration examples are dynamically chosen per the strategies described above.\n- *See*: Figure 5 (“Prompt Format”), §4.1.3, §4.1.2.\n\n---\n\n**6. Where are the prompt strategies explained?**\n- Details are found in:\n  - §4.1.2 “Demonstration Retrieval” (explains how case pool is constructed, how demonstrations are selected based on pattern embedding).\n  - §4.1.3 “Query Design” (gives prompt format and model setup).\n  - Motivation/comparison: §3.3 “Motivating Examples” and §6.2 “Effectiveness of the Extractor” (for comparing strategies).\n- Also supported by Table 1 (showing impact on extraction metrics), and the discussions in §6.2.\n\n---\n\n**7. What are the results of the prompting strategies, and how successful or not were they?**\n- **Precision of entity extraction (Table 1):**\n  - **P-ICL (Pattern-based ICL):** Best performance; covers 60% of attack vectors and 58% of root causes, with highest overall Jaccard Similarity Index (JSI = .58).\n  - **S-ICL (Semantic):** Good, but slightly less than P-ICL.\n  - **Direct prompt (no demonstration):** Worse than ICL, covering only 41% attack/35% root cause, JSI = .35.\n  - **CaVAE baseline (unsupervised NLP):** Very low (∼7% coverage, JSI = .04).\n- **Conclusion:** Including ICL with well-chosen demonstrations markedly boosts extraction quality (precision & brevity), with pattern-based retrieval outperforming semantic similarity retrieval.\n- *See*: §6.2, Table 1.\n\n---\n\n**8. Why did one strategy give better results than another?**\n- **Pattern-based ICL gives better results because:**\n  - It retrieves demonstration cases with similar syntactic patterns (not just semantics) to the target description, allowing the LLM to more accurately infer entity boundaries—especially where entity structure is ambiguous.\n  - Pattern similarity in the prompt reduces irrelevant information, helping the LLM recognize which parts of the text are the “attack vector” and “root cause.”\n  - Empirically, this reduces both errors and extraneous outputs.\n- *See*: §4.1.2 (why pattern-based), §6.2 (empirical comparison and explanation).\n\n---\n\n**9. All specifics of the researcher's prompt strategies.**\n- **Prompt Construction:**\n  - Use a large “case pool” of manually labeled examples (attack vector and root cause for 642 Java CVEs).\n  - For each new CVE description to be processed, find three best-matching prior examples (by pattern embedding and cosine similarity).\n  - Formulate prompt: 1) general instruction, 2) three demonstration cases (ordered by similarity), 3) actual target description.\n  - Pass constructed prompt to ChatGPT (gpt-3.5-turbo-0613), with temperature=0.\n  - Entity extraction is performed at the sentence level (if the description has several sentences, extract from each).\n- **Pattern Embedding Method:**\n  - Entities like “Vendor/Product/Version,” “Vulnerability Type,” “Component,” and “Attacker” are identified and replaced with anchor tokens (VPV, VT, etc.).\n  - Use Stanford parser to form parse trees, prune and abstract phrase nodes, concatenate leaf nodes to create structural patterns.\n  - Use an Autoencoder to embed these patterns for similarity search.\n- **ICL Demonstration Selection:**\n  - Retrieve Top-3 most pattern-similar demonstrations for each prompt.\n  - For comparison, the S-ICL uses semantic similarity (embedding via angle-llama-7b-nli).\n- **Other Details:**\n  - If the LLM cannot find an attack vector/root cause for the CVE, it is instructed to say “None.”\n  - Ground truth demonstration for the current CVE is omitted from the candidate pool to prevent overfitting during LLM evaluation.\n  - All case selection, prompt construction, and extraction steps are recorded and evaluated for accuracy (coverage, JSI), see §6.2 RQ2.\n- *See*: §4.1 (ICL-Enhanced Extractor, Case pool, Demonstration Retrieval), Figure 5 (prompt format), §6.2 (empirical evaluation), description of ablation studies in §6.4 (impact of each component).\n\n---\n\nIf you need a synthesis focused on a particular section (e.g., ablation study, or practical implications), let me know!"
  }
]