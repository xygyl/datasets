{
  "filename": "large-language-models-are-zero-shot-reasoners.pdf",
  "analysis": "Here are the answers within context of the paper **\"Large Language Models are Zero-Shot Reasoners\"** (Kojima et al., 2022; NeurIPS):\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n#### Point of the Paper\nThe paper aims to show that **large language models (LLMs) can perform multi-step reasoning in a zero-shot setting**—that is, without seeing any task-specific examples—by using a simple, generic prompt (\"Let's think step by step\") before the answer. This challenges the prevailing notion that few-shot examples (showing models step-by-step solutions) are required to elicit such reasoning.\n\n#### What is it exploring?\n- Whether **LLMs can perform complex multi-step reasoning** (chain of thought) without in-context task demonstrations.\n- The generalizability and effectiveness of a **single, task-agnostic prompt** for multi-step reasoning across diverse tasks (arithmetic, symbolic, commonsense, logical).\n\n#### What makes it unique?\n- Previous \"Chain of Thought\" prompting (CoT) required specially-crafted few-shot examples per task.\n- This paper introduces **Zero-Shot-CoT**: only a *single sentence* (\"Let's think step by step\") is added after the question—no further engineering or examples required.\n- Shows this works **across many Reasoning tasks and datasets**, raising zero-shot performance on difficult tasks by large margins (e.g., MultiArith: 17.7% → 78.7% on InstructGPT text-davinci-002).\n- Demonstrates that zero-shot prompting can unlock 'hidden' abilities in models, suggesting LLMs have broad, untapped, general reasoning competence.\n\n---\n\n### 2. What dataset did they use?\n\nThey evaluate across **12 datasets** from four categories of reasoning tasks (Appendix A.2):\n#### Arithmetic Reasoning:\n- SingleEq\n- AddSub\n- MultiArith\n- AQUA-RAT\n- GSM8K\n- SVAMP\n\n#### Commonsense Reasoning:\n- CommonsenseQA\n- StrategyQA\n\n#### Symbolic Reasoning (created by authors following Wei et al., 2022):\n- LastLetterConcatenation\n- CoinFlip\n\n#### Other Logical Reasoning:\n- Date Understanding (from BIG-Bench)\n- Tracking Shuffled Objects (from BIG-Bench)\n\nDatasets are a mix of publicly available ones and two (LastLetters, CoinFlip) created for the paper.\n\n---\n\n### 3. What LLM models were used?\n\nThey experimented with 17 models, including (see Table 8):\n\n- **OpenAI GPT-3 family**:\n  - ada, babbage, curie, davinci (OriginalGPT3: 0.3B, 1.3B, 6.7B, 175B parameters)\n- **InstructGPT-3** (instruction-tuned):\n  - text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002 (no official parameter size but mapped order)\n- **PaLM** (Google's large LLM): 8B, 62B, 540B parameters\n- **Other open-source LLMs**:\n  - OPT-13B\n  - T0 (11B)\n  - GPT-J (6B)\n  - GPT-Neo (2.7B)\n  - GPT-2 XL (1.5B)\n\nMain experiments are with **text-davinci-002** and **PaLM-540B**.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\n#### Baseline strategies:\n- **Standard Zero-shot:** Just the question and an answer-extraction prompt like \"The answer is\".\n- **Few-shot:** The question plus a small set of solved example Q&A pairs.\n- **Few-shot-CoT:** The question plus a few examples showing step-by-step (\"chain of thought\") reasoning and their answers.\n- **Zero-shot-CoT (proposed):** The question, followed by a *single instruction*: \"Let's think step by step\", with no examples.\n\n#### Further variations:\n- Different \"reasoning triggers\" (Table 4) tested in prompt:\n  - e.g., \"Let's think step by step\", \"First,\", \"Let's think about this logically\", \"Let's solve this problem by splitting it into steps\", etc.\n\n- Two-stage prompting for Zero-shot-CoT (see below).\n\n---\n\n### 5. What specific prompts were used?\n\n#### **Zero-shot-CoT** main template:\n```\nQ: [Question]\nA: Let's think step by step.\n```\nThe LLM is expected to output a chain of thought (reasoning steps) followed by the answer.\n\n#### **Answer extraction prompt**:\nAfter model outputs the reasoning, a second prompt is issued to extract the actual answer in the desired form (number, letter, yes/no):\n- For arithmetic:  \n  `Therefore, the answer (arabic numerals) is`\n- For multiple-choice:  \n  `Therefore, among A through E, the answer is`\n- For yes/no:  \n  `Therefore, the answer (Yes or No) is`\n(see Table 10 in Appendix)\n\n#### **Other trigger phrases tested** (Table 4):\n- \"First,\"\n- \"Let's think about this logically.\"\n- \"Let's solve this problem by splitting it into steps.\"\n- \"Let's be realistic and think step by step.\"\n- \"Let's think like a detective step by step.\"\n- \"Before we dive into the answer,\"\n- \"The answer is after the proof.\"\n- [Some intentionally misleading or irrelevant for robustness: \"Don't think. Just feel.\", etc.]\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 3** (\"Zero-shot Chain of Thought\")—main method\n- **Section 3.1** (\"Two-stage prompting\")—full pipeline and answer extraction triggers\n- **Appendix A.5**—lists actual answer extraction prompt variants used per task/dataset.\n- **Table 4** in main text—lists various \"reasoning trigger\" prompt variations and their measured accuracy.\n\n---\n\n### 7. What are the results of the prompting strategies, and how successful or not were they?\n\n#### **Key Results:**\n\n- **Zero-shot-CoT outperforms standard zero-shot and even few-shot on tough tasks:**\n  - MultiArith, text-davinci-002:\n    - Zero-shot: 17.7%\n    - Few-shot: 33.7%\n    - Zero-shot-CoT: 78.7%\n    - Few-shot-CoT: up to 93.0%\n  - GSM8K:\n    - Zero-shot: 10.4%\n    - Few-shot: 15.6%\n    - Zero-shot-CoT: 40.7%\n    - Few-shot-CoT: 48.7%\n  - PaLM-540B model:\n    - MultiArith: Zero-shot=25.5%; Zero-shot-CoT=66.1%\n    - GSM8K: Zero-shot=12.5%; Zero-shot-CoT=43.0%\n\n- **On tasks that \"do not\" require multi-step reasoning (SimpleEq, AddSub), Zero-shot-CoT is roughly on par with zero-shot.**\n\n- **On symbolic reasoning and logical tasks:** Zero-shot-CoT improves accuracy drastically versus standard zero-shot.\n\n- **On commonsense tasks (CommonsenseQA):** Zero-shot-CoT did not notably outperform. Authors suggest most gains are on tasks that require explicit reasoning decomposition.\n\n- **Prompt wording matters; instructive trigger phrases (\"Let's think step by step.\") work best. Misleading/irrelevant triggers result in little or no improvement.**\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **Zero-shot-CoT worked better because:**\n  - The \"Let's think step by step\" prompt encourages LLMs to generate intermediate reasoning steps before answering. This aligns closely with how few-shot-CoT works—but doesn't require carefully-constructed per-task examples.\n  - This helps the model \"decompose\" problems and correctly solve multi-step reasoning tasks, which standard zero-shot (straight to answer) does not.\n- **Few-shot-CoT is still better** than Zero-shot-CoT in many cases, especially with sufficient, well-crafted examples, but Zero-shot-CoT drastically narrows the gap without example engineering.\n\n- **Prompt wording/trigger type matters:**\n  - \"Instructive\" triggers (that ask for reasoning) are best.\n  - \"Misleading\" or irrelevant triggers do not elicit reasoning and thus do not improve (or may worsen) performance.\n  - See Table 4 for detailed accuracy analysis across triggers.\n\n- **Task type matters:** Most performance gains are for tasks requiring multi-step, logical, or arithmetic reasoning (System-2 tasks), not simple fact retrieval or common-sense single-step tasks.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies\n\n#### a) **Zero-Shot-CoT main method (Section 3, Figure 2):**\n\n- **Step 1:** Input prompt:  \n  `Q: [Question]\\nA: Let's think step by step.`\n- **Model generates**: a chain of thought (series of reasoning steps).\n- **Step 2 (Answer Extraction):** A special prompt is appended to extract just the answer from the reasoning, e.g.  \n  - For arithmetic: `Therefore, the answer (arabic numerals) is`\n  - For multiple choice: `Therefore, among A through E, the answer is`\n  - (Full list per task: Appendix A.5, Tables 9, 10)\n\n- **Answer Cleansing**: Simple scripts extract the first number, letter, or yes/no from the output as the prediction (see Appendix A.6).\n\n#### b) **Two-stage Prompting:**\n- First pass generates reasoning.\n- Second (automated) pass extracts answer using augmented prompt.\n\n#### c) **Prompt Trigger Variations** (Template Study, Table 4):\n- Authors systematically tested 16 different prompts including:\n  - Instructive: \"Let's think step by step.\" (Best performing: 78.7%)\n  - Similar triggers: \"First,\", \"Let's think about this logically.\"\n  - Irrelevant/misleading: \"Abrakadabra!\", \"Don't think. Just feel.\"\n- Only instructive variants reliably result in multi-step reasoning and score improvements.\n\n#### d) **Comparison Baselines:**\n- **Standard Zero-shot:** Q only, with \"The answer is\" style answer extraction.\n- **Few-shot:** Q, with a few Q&A pairs as in-context examples (straight answers).\n- **Few-shot-CoT:** Q, with step-by-step reasoning shown in examples.\n- **Zero-plus-Few-shot-CoT**: Both prompt trigger and in-context few-shot CoT together.\n\n#### e) **Robustness to Choice of Prompt:**\n- If the answer extraction format matches the required answer type/format (arithmetic/choice, etc.), performance is robust.\n- Prompting with examples from other tasks but with matching answer type (e.g., commonsense reasoning CoT prompts for arithmetic multiple-choice format) yields partial transfer (but is less effective than task-specific examples).\n\n#### f) **Model Choice/Scaling:**\n- The gains from Zero-shot-CoT increase with larger model size (Figure 3).\n- Only the largest models (InstructGPT-3, PaLM 540B) fully realize the benefit.\n\n#### g) **Code and Data:**\n- Implementation is open-source: https://github.com/kojima-takeshi188/zero_shot_cot\n\n---\n\n## **Summary Table for Prompt Strategies**\n\n| Strategy           | Q+A examples shown? | Chain of thought shown? | Prompt trigger?        | Answer format handling | Main result |\n|--------------------|---------------------|-------------------------|------------------------|-----------------------|-------------|\n| Zero-shot          | No                  | No                      | No                     | Prompt for answer     | Baseline    |\n| Few-shot           | Yes                 | No                      | No                     | Prompt for answer     | Slight gain |\n| Few-shot-CoT       | Yes                 | Yes                     | No                     | Prompt for answer     | SOTA        |\n| Zero-shot-CoT      | No                  | Model-generated         | Yes (\"Let's think...\") | Prompt for answer     | Huge gain   |\n\n---\n\n**References from the paper:**\n- Wei et al., 2022: Chain of Thought Prompting\n- Wang et al., 2022: Self-consistency\n\nIf you need exact prompt templates, answer extraction strings, or dataset question formats, see **Appendix A.5** and example tables in the supplementary material of the paper."
}