{
  "filename": "interleaving-static-analysis-and-llm-prompting.pdf",
  "analysis": "Certainly! Here are detailed answers to your questions about the paper “Interleaving Static Analysis and LLM Prompting” by Chapman, Rubio-González, and Thakur (SOAP 2024):\n\n---\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point/Goal**: The paper proposes and evaluates a new technique that combines (interleaves) automatic static program analysis with Large Language Model (LLM) prompting to improve the task of *error-specification inference* for C programs. \"Error-specification inference\" means inferring the set of error values that functions can return, which is important for program understanding and bug detection, especially in C with its complex error-handling idioms.\n\n**What is Being Explored?**\n- Whether integrating LLMs into an existing static analysis workflow can significantly improve the inference of error specifications, particularly in situations where traditional static analysis is limited (e.g., when analyzing third-party code or incomplete program facts).\n- How prompting strategies—carefully crafting the queries (prompts) given to the LLM using intermediate results from the static analyzer—can improve the quality of results.\n\n**What Makes It Unique?**\n- **Interleaving Approach**: Instead of using either just a static analyzer or an LLM, the authors propose a tightly interleaved process: static analysis results are used to guide LLM queries, and LLM outputs are fed back to improve subsequent static analysis.\n- **Prompt Construction**: Prompts to the LLM include not just raw questions, but intermediate analysis facts/context and domain knowledge, enabling more precise and context-aware inference.\n- **Target Domain**: This approach targets error-code specifications in real-world, low-level C code, which is a hard open problem due to the prevalence of error codes, lack of exceptions, and heavy use of third-party libraries.\n\n---\n\n### 2. What dataset did they use?\n\nThey evaluated the approach on **six real-world C programs** that use diverse error handling patterns and include both open-source and system/library codebases:\n\n- **Apache HTTPD** (web server)\n- **LittleFS** (filesystem)\n- **MbedTLS** (cryptography library)\n- **Netdata** (monitoring software)\n- **PidginOTRv4** (encryption for chat)\n- **zlib** (compression library)\n\n**Details**:\n- Table 1 specifies code size (KLOC) and the amount of initial domain knowledge required (error specs, error codes, etc.).\n- Table 2 lists, for each benchmark, the total number of functions, the subset analyzed, and the percentage of third-party functions.\n\n---\n\n### 3. What LLM models were used?\n\n**GPT-4** was used as the Large Language Model for all experiments (see Section 5.1 Implementation Details).\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nThe paper discusses several **prompting strategies** for how and when to involve the LLM in the analysis:\n\n1. **Direct LLM Analysis (queryLLMAnalysis)**  \n   - Invoked when static analysis yields an unknown error specification (⊥).\n   - Prompt includes the function source code, a general problem description, an explanation of the abstract domain (how error specs are represented), any known error codes/success codes, and intermediate results for functions called by the function being analyzed.\n\n2. **LLM Third-party Completion (queryLLMThirdParty)**\n   - Used to infer the error specifications of *third-party* library functions (for which no source code is available).\n   - Prompt includes the function name and the currently known error specifications from the rest of the code.\n\n3. **Combined/Interleaved Approach**\n   - The full workflow alternates between static analysis (which propagates known facts as far as possible) and the above two LLM-invoking strategies, using the LLM only when necessary and always providing as much context as possible.\n\n4. **Prompt Components** (see Section 4.1 and Algorithm 1)\n   - **CommonContext**: Task description and domain rules (e.g., explanation of error code idioms, success/failure value rules, domain knowledge).\n   - **FunctionContext**: Known error specs of any relevant functions (e.g., those called by the analyzed function), provided as few-shot examples.\n   - **Question**: The function’s source code (if available) and a direct prompt about the error specification wanted.\n\n5. **Chain-of-thought and format consistency**\n   - Multiple chain-of-thought reasoning examples, to help the LLM structure outputs in a parseable format.\n   - The LLM is re-queried for consistency and output format is validated.\n\n---\n\n### 5. What specific prompts were used?\n\n**Prompt Structure:**  \nEach prompt to the LLM is built from (see Section 4.1):\n  - **CommonContext**:  \n    - Instructions about the error specification inference task.\n    - Abstract domain explanation (how error specs are represented, e.g., <0, 0, ∅).\n    - Domain knowledge (error codes, success codes, error-only functions).\n    - Idiomatic rules (e.g., “0 is success”, “error codes are negative”, etc.).\n    - One or more chain-of-thought, few-shot examples.\n  - **FunctionContext**:  \n    - Known error specifications for functions called by the current function.\n    - For third-party functions, all current known error specs.\n  - **Question**:  \n    - For normal analysis: The source code of the function to analyze, and an explicit instruction to infer the error-spec using the given domain.\n    - For third-party: The function identifier and relevant context.\n\n**Example Prompt Excerpts (see Figs. 2 and 3 & Section 4.1):**\n\n- *“You are to determine the error specifications of the given functions.”*\n- *“Functions return 0 on success, negative values on error…”*\n- *Example function definition plus its error specification and an explanation.*\n- *“mbedtls_asn1_get_tag: <0”*\n- *Source code for the function to analyze.*\n- Function-level context: “mbedtls_asn1_get_tag: <0” (supplied if known).\n\n---\n\n### 6. Where are the prompt strategies explained?\n\n- **Section 4** _(Approach)_: High-level algorithm for when/how prompts are constructed, including the overall interleaving scheme (Algorithm 1).\n- **Section 4.1** _(Building Prompts)_: Precise structure of prompts, breakdown into CommonContext, FunctionContext, Question.\n- **Figures 2 & 3**: Show concrete examples of function source, prompt content, and the interplay of EESI and LLM.\n- **Algorithm 1** (pseudo-code): Shows precisely how/when/with what input LLM is queried during the workflow.\n\n---\n\n### 7. What are the results of the prompting strategies and how successful or not were they?\n\n**Summary of Results:**\n- **Direct LLM Analysis Strategy (queryLLMAnalysis):**  \n  Average recall across all benchmarks rises to 70.26% from EESI’s 52.55% (see Fig 4c). Precision remains high and comparable (~86%).\n- **LLM Third-Party Completion (queryLLMThirdParty):**\n  On benchmarks with a large percentage of third-party functions, recall increases dramatically (e.g., Netdata +70%).\n- **Combined/Interleaved Approach:**  \n    - **Recall**: Rises further to 77.83% on average (from 52.55% for baseline EESI only).\n    - **F1-score**: Improves from 0.612 to 0.804 (Table 4 and Figure 4d).\n    - **Precision**: Stays roughly constant (small decrease from 86.67% to 85.12%).\n    - **Impact by Benchmark**: Greatest positive impact seen on large/realistic code with many third-party/library functions (Apache HTTPD, Netdata, Pidgin OTRv4), but positive in all cases.\n\n**Takeaway:**  \nThe interleaved approach, using prompting strategies as described, significantly increases recall and overall F1-score for error-specification inference, with almost no cost to precision.\n\n---\n\n### 8. Why did one strategy give better results than another?\n\n- **LLM Third-Party Completion works well** for codebases with many third-party/library function calls, since static analysis cannot analyze library code at all, but LLMs can use training/hallucination/domain knowledge to suggest plausible error specifications.\n- **Direct LLM Analysis works well** when static analysis lacks enough information (due to complex data/control flows, pointer aliasing, or other static analysis limitations), and context-aware LLM prompts leveraging known facts can fill gaps.\n- **Combined Approach** is best because it leverages the strengths of both static analysis and LLM reasoning: static inference when possible, and LLM guidance where static methods hit their limitations.\n    - In practice, neither static analysis nor LLM alone is sufficient: LLM alone is easily misled without appropriate context, and static analysis alone cannot handle incomplete code or highly complex/unknown flows.\n\n---\n\n### 9. All specifics of the researcher's prompt strategies.\n\n**Prompt Construction Details:**\n- **CommonContext** (included in all prompts):  \n  - Description of the error-specification inference task.\n  - Details on the abstract domain (set of possible error codes: <0, =0, =1, ∅, etc.).\n  - Corpus-specific error-handling idioms (e.g., in C, “0 is success”, “negative values signal errors”, “NULL is equivalent to 0”).\n  - Domain knowledge: lists of known error/success codes, error-only functions.\n  - Multiple simple chain-of-thought/few-shot examples:  \n    - Example function snippet, error specification, explanation.\n    - Ensures output is parseable and consistent.\n\n- **FunctionContext**:\n  - For queryLLMAnalysis (normal code): Known error specs for all functions called from the current function (derived from EESI or recent LLM queries).\n  - For queryLLMThirdParty (library function): All currently known error specs from the program.\n\n- **Question**:\n  - For code analysis: Provide the function's actual source code and explicitly ask for the error specification in the described domain.\n  - For third-party: Provide the function name.\n\n- **Formatting**:\n  - LLM response must be in a standard (parsable) format; e.g.,  \n    `function_name: <0`\n  - If LLM output is not parseable or inconsistent with the domain, the result is interpreted as unknown (⊥).\n\n- **Validation/Consistency**:\n  - Prompts may be re-issued if output is inconsistent (to avoid hallucination).\n  - LLM is instructed to ensure consistency with chain-of-thought support.\n  - If a suggested error spec includes a success code or is not a subset of the return range, it’s discarded.\n\n- **Algorithm**: (See Section 4.2 and Algorithm 1)\n  - Iterate over functions in bottom-up call graph order.\n  - For third-party functions: queryLLMThirdParty.\n  - For regular code: run static analysis; if static analysis returns unknown, call queryLLMAnalysis with enriched prompt including analysis context and called-function error specs.\n\n\n---\n\n**References to strategies in paper:**  \n- Detailed in **Section 4.1 (Building Prompts)**, **Section 4.2 (Error Specification Inference)**, Figures 2 and 3 (concrete examples), and pseudo-code in Algorithm 1.\n- Evaluation and effectiveness in **Section 5 (Experimental Evaluation)**.\n\n---\n\n#### If you need illustrative examples of actual prompts, charts illustrating the results, or stepwise breakdowns of Algorithm 1, let me know!"
}