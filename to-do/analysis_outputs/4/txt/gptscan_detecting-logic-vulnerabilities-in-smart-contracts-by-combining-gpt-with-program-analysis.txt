## 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point and Exploration:**  
The paper proposes GPTScan, a novel tool for automatically detecting *logic vulnerabilities* in smart contracts by combining large language models (LLMs: specifically GPT-3.5-turbo) with classic static program analysis. This focus on logic vulnerabilities distinguishes it from most existing static analysis tools, which primarily detect well-known syntactic issues (like re-entrancy and integer overflows). In particular, recent studies cited by the paper show that about 80% of real-world smart contract bugs are logic vulnerabilities (i.e., domain/business-logic errors) that traditional tools cannot detect due to the lack of code semantics and property descriptions.

**Uniqueness:**  
- GPTScan uses LLMs not to directly "detect vulnerabilities," but as a *code understanding engine* to match code against manually defined code-level logic vulnerability scenarios and properties.
- It breaks down each logic vulnerability type into concrete scenario/property sentences that describe how and where business logic might be violated in code.
- It uses sophisticated function filtering and asks the LLM to home in on specific variables/statements, which are then checked with static analysis for confirmation, substantially reducing false positives.
- The approach is the first to combine LLM “understanding” with automated static analysis for logic bug detection in smart contracts, achieving both high recall and notably lower false positives than prior LLM-based approaches.

---

## 2. What dataset did they use?

**Three real-world datasets:**
- **Top200:**  
  - 303 smart contract projects with high market capitalization; open-source, well-audited, widely used projects (mainly for measuring false positives).
  - 555 Solidity files (~134K LoC); assumed to be non-vulnerable.
- **Web3Bugs:**  
  - 72 projects (from the Code4rena audit platform); drawn from the Web3Bugs dataset.
  - 2,573 Solidity files, 319K LoC, 48 ground-truth logic vulnerabilities (audited by humans).
- **DefiHacks:**
  - 13 known-vulnerable token contract projects from the DeFi Hacks dataset (past attack incidents).
  - 29 Solidity files, 17.8K LoC, 14 ground-truth logic vulnerabilities.

Together: ~400 projects, 3,157 files, 472K lines of code, 62 logic vulnerabilities.

---

## 3. What LLM models were used?

- Main Model: **OpenAI GPT-3.5-turbo** (via public API).
   - Chosen for 20x lower cost than GPT-4 but still good performance.
   - Used with zero temperature (`temperature=0`) to reduce randomness and promote deterministic results.
- GPT-4 was tested but did not give significant improvements and was much more expensive.
- The technique is designed to be adaptable to other GPT-like LLMs.

---

## 4. What are the prompting strategies discussed in this paper?

**Prompting strategies:**
- **Scenario/Property Matching:**  
  - Each vulnerability type is defined by a “scenario” (the context or functionality where the bug might appear) and “properties” (the code attributes/operations indicative of the bug).
  - The LLM is queried ("prompted") with yes/no questions about whether a function matches these hand-crafted scenario/property descriptions.
- **“Mimic-in-the-background” Prompting:**  
  - To minimize randomness and inconsistency, the prompt instructs the model to “answer the question in the background five times and provide the most frequent result,” inspired by zero-shot chain-of-thought prompting.
- **Variable/Statement Extraction Prompts:**  
  - If a candidate function matches the scenario/property, GPT is asked to extract and identify relevant variables/statements for later static validation.
- **Strict Output Format Enforcement:**  
  - Prompts enforce responses in strict JSON or yes/no to eliminate ambiguity in LLM outputs.
- **Function Filtering via Multi-dimensional Rule-based Filtering:**  
  - Before reaching the LLM, code is pre-filtered (by function name keywords, code expressions, parameter types, access modifiers, etc.) to reduce the number of irrelevant queries sent to GPT.

---

## 5. What specific prompts were used?

### a. **Scenario/Property Matching Prompt** (see Figure 4 in the paper):

```
System: You are a smart contract auditor. You will be asked questions related to code properties. You can mimic answering them in the background five times and provide me with the most frequently appearing answer. Furthermore, please strictly adhere to the output formats specified in the question; there is no need to explain your answer.

ScenarioMatching:
Given the following smart contract code, answer the questions below and organize the result in a json format like {"1":"Yes" or "No","2":"Yes" or "No"}.
"1": [SCENARIO_1]?
"2": [SCENARIO_2]?
[CODE]

PropertyMatching:
Does the following smart contract code "[SCENARIO, PROPERTY]"? Answer only "Yes" or "No".
[CODE]
```

### b. **Variable/Statement Extraction Prompt** (see Figure 5):

```
System: (same as above)
In this function, which variable or function holds the total supply/liquidity AND is used by the conditional branch to determine the supply/liquidity is 0? Please answer in a section that starts with "VariableB:".
In this function, which variable or function holds the value of the deposit/mint/add amount? Please answer in a section that starts with "VariableC:".
Please answer in the following json format:
{"VariableA":{"Variable name":"Description"}, "VariableB":{"Variable name":"Description"}, "VariableC":{"Variable name":"Description"}}
[CODE]
```

**Additional specifics:**  
- All prompts strictly enforce output format for easy parsing.
- The prompts are constructed for yes/no answers or key-value output in JSON.
- For multiple scenario statements, prompts ask the model to answer all of them in one JSON output.
- All variable extraction prompts ask for both names and short descriptions.

---

## 6. Where are the prompt strategies explained?

- **Scenario/Property Prompting**:  
  - Described in detail in **Section 4.2 ('GPT-based Scenario and Property Matching')** (with Table 1 for vulnerability breakdown; Figure 4 for prompt template).
- **Variable Extraction Prompting**:
  - Detailed in **Section 4.4 ('From GPT Recognition to Static Confirmation')** and illustrated in Figure 5.
- **"Mimic-in-the-background" Prompting**:  
  - Explained in Section 4.2 ("Minimizing the impact of GPT output randomness").
- **Functional Filtering (pre-prompt) Strategies**:
  - Section 4.3.

---

## 7. What are the results of the prompting strategies? How successful were they?

- **Overall Performance:**
  - **Precision (i.e., % correct bug detections among all positives):**
    - Top200 (non-vulnerable contracts, token contracts): 95.61% (false positive rate: 4.39%)
    - DefiHacks (token contracts): 90.91%
    - Web3Bugs (large/more complex projects): 57.14%
  - **Recall (ability to detect all true vulnerabilities):**
    - Web3Bugs: 83.33%
    - DefiHacks: 71.43%
  - **F1 scores** (balanced measure):
    - Web3Bugs: 67.8%
    - DefiHacks: 80%
  - GPTScan also discovered **9 new logic vulnerabilities** missed by human auditors (demonstrating value on top of classical audits).
- **Efficiency:**
  - 14.39 seconds and $0.01 per 1,000 lines of code on average.
- **False Positives:**
  - Dramatically lower than plain-GPT prompting (as in David et al.), e.g., prior GPT-4 method had 4% precision, 96% FPR; GPTScan’s combination with static confirmation reduced false positives by 66% compared to GPT-alone.
- **Static Confirmation:**
  - Filtering on static validation filtered out ~66% of potential false positives from the initial LLM output (see Table 4).

---

## 8. Why did one strategy give better results than another?

**Key Insights:**
- **GPT Alone:** When just using LLMs as in prior work (e.g., feeding vulnerability descriptions and asking for yes/no), the model often "hallucinates" -- it gives plausible but often incorrect answers due to lack of code context, limited reasoning over complex semantics, and output randomness.
- **Combination Approach:**  
  - By first *manually* breaking down each logic vulnerability into scenario/property sentences (providing precise code-level patterns to look for), the LLM acts as a code-semantic matcher, not a direct "vulnerability detector," which improves precision.
  - The multi-stage process:
    - (a) strict filtering of candidate functions
    - (b) tightly scoped LLM questioning on specific properties
    - (c) static verification of variables/conditions/statements
    Together ensures that LLMs are used only where they excel (semantic code understanding), and deterministic program analysis is relied on for hard validation.
- **"Mimic-in-the-background":**  
  - Reduces variance/randomness in LLM outputs (by majority voting among 5 internal runs), further boosting answer reliability.
- **Why not just GPT-4?** The authors found that even with GPT-4 (even with 32k context), you still face the above flaws if you aren’t guiding the model and post-processing with static checks.

---

## 9. All specifics of the researcher's prompt strategies

**a. Scenario/Property breakdown:**  
- For each logic vulnerability, the team created:
  - A *scenario* (e.g., function handles deposit/mint/share operations and initial conditions for liquidity pool).
  - *Properties* (e.g., code sets share = deposit amount when total supply or liquidity is zero).
  - Filtering types (to pick the right functions).
  - Static check types (e.g., dataflow tracing, value comparison, order check, function call argument check).

**b. Pre-filtering Functions:**
- Project-wide file filtering (removing test/libs/irrelevant code).
- OpenZeppelin function signature filtering (excluding standard, battle-tested library code).
- Multi-dimensional function rules (YAML-based: function name keywords, code content expressions, parameters, modifiers/callers, etc.).

**c. Prompt Structure and Techniques:**
- Strict system prompt to minimize hallucination (“as a smart contract auditor, do not explain, output only specified format”).
- Multiple scenario/property statements combined into one prompt for efficiency.
- Output in yes/no or tightly structured JSON for every question.
- For variable/statement extraction: ask specifically for names and short code-context-based descriptions; if results don’t exist or fit, abort.
- Post-prompt static validation: only if relevant variables/statements are recognized and confirmed in their role/context does the function "pass".

**d. “Mimic-in-the-background” trick:**  
- GPT is instructed to internally answer the question 5 times and then report the most frequent answer ("majority voting in background"), stabilizing output.

**e. Parameters:**  
- All GPT runs with `temperature=0` (fully deterministic output).
- Each query is a fresh session (no history/context leakage).
- No few-shot examples; only zero-shot with domain-specific breakdowns.

---

## References to Key Sections in the Paper

- **Section 4.2:** Full description of scenario/property-based prompting, strict output format, and “mimic-in-the-background.”
- **Table 1:** Shows how each vulnerability type is mapped to code scenario/property statements.
- **Figure 4:** Complete system and user prompt templates for scenario/property matching.
- **Section 4.4 and Figure 5:** Variable/statement extraction prompt and its role.
- **Section 5:** Evaluation results on all datasets, including comparison against prior LLM and static analysis-only approaches.
- **Section 6:** Discussion of limitations and LLM choices.
