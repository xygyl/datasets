### 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point & Exploration:**
- The paper comprehensively evaluates ChatGPT’s capabilities for **real-world vulnerability management tasks**, spanning the full vulnerability management lifecycle: bug report summarization, security bug report identification, vulnerability severity evaluation, vulnerability repair, patch correctness assessment, and stable patch classification.
- It aims to answer three specific research questions:
  - **RQ1:** Can ChatGPT achieve performance comparable to state-of-the-art (SOTA) methods on these tasks?
  - **RQ2:** How do different prompt engineering strategies impact ChatGPT’s performance?
  - **RQ3:** What future research directions can improve ChatGPT's effectiveness on these tasks?

**What makes it unique:**
- It is the **first large-scale evaluation** (70,346 samples and 19 million+ tokens) of ChatGPT across the complete vulnerability management process, comparing against 11 SOTA baselines for six real-world tasks.
- The study deeply analyzes not only task outcomes but also the effects of **various prompt engineering strategies** (e.g., "self-heuristic" prompts), offering insight into both strengths and bottlenecks in applying ChatGPT to software security tasks.
- It explores **failures and limitations**, provides a user study with practitioners, and suggests directions for future research.

---

### 2. What dataset did they use?

- The evaluation **uses the same datasets from the SOTA papers** for each of the six vulnerability management tasks.
- **Total dataset:** 70,346 samples (19,355,711 tokens).
- **Tasks and sources:** (see Table 1 in the text)
  - **Bug report summarization:** iTAPE [18], 33,438 samples.
  - **Security bug report identification:** Farsec [49], DKG [57], CASMS [35], 22,970 samples.
  - **Vulnerability severity evaluation:** DiffCVSS [48], 1,642 samples.
  - **Vulnerability repair:** LLMset [37], ExtractFix [24], 12 samples (plus 600-350 responses per approach).
  - **Patch correctness assessment:** Quatrain [46], Invalidator [31], Panther [44], ~995–208 samples per dataset.
  - **Stable patch classification:** PatchNet [25], 10,896 samples.

**Note:** For probe-test (prompt testing), 10% random samples from the training set were separated.

---

### 3. What LLM models were used?

- ChatGPT models used:
  - **gpt-3.5-turbo-0301** (for phases 1 and 2: template design & probe-testing, for economy)
  - **gpt-4-0314** (for large-scale final evaluations, for performance)
- SOTA LLM models mentioned for comparison in vulnerability repair include:
  - **OpenAI Codex** (from LLMset [37])
  - **AI21 Jurassic-1** (LLMset [37])

---

### 4. What are the prompting strategies discussed in this paper?

Prompting (“prompt engineering”) strategies covered include:

**A. In-Context Learning:**  
- **0-shot prompting**: Task description + input only (no examples).
- **1-shot prompting**: Task description + one demo example + input.
- **Few-shot prompting**: Task description + several demo examples (4) + input.

**B. Enhanced/Refined Prompts:**
- **general-info**: Adds system instructions (role, reinforcement, confirmation, positive feedback, chain-of-thought, right-answer directive).
- **expertise**: Manually adds domain-specific task expertise into the prompt, e.g., rules learned from docs and research.
- **self-heuristic**: Prompts where ChatGPT *itself* is asked to summarize expertise/patterns from demos, which are then integrated into the main prompt.

(See Table 2 and Section 3.3; extensive details in Appendix.)

---

### 5. What specific prompts were used?

- **0-shot:**  
  - `USER <task description> <input>`

- **1-shot:**  
  - `USER <task description> <demo example> <input>`

- **Few-shot:**  
  - `USER <task description> <demo 1> <demo 2> <demo 3> <demo 4> <input>`

- **General-info:**  
  - Uses system and user messages for role, reinforcement, task confirmation, positive feedback, chain-of-thought (“Let’s think step-by-step…” etc.).
  - Example: *See Figure 3 in the text for the full prompt with role ("You are Frederick, an AI expert..."), step-by-step reasoning, and conclusion request.*

- **Expertise:**  
  - As in general-info, but with extra blocks stating domain rules ("Remember, bug reports related to memory leak or null pointer should be seen as security bug reports," etc.).

- **Self-heuristic:**
  - ChatGPT is first given examples and asked to summarize patterns.
  - These patterns/knowledge are then injected into the prompt (see Figure 4 and 6).

**Concrete prompt templates for every task and examples are provided in Table 13 and the Appendix.**

---

### 6. Where are the prompt strategies explained?

- **Main Text:**
  - Section 2.2 ("ChatGPT and Prompt")
  - Section 3.1 & 3.3 ("Evaluation Pipeline"; "Prompt Design and Implementation")
- **Detailed prompt templates and examples:**
  - **Table 2** (summary of templates)
  - **Table 13** in the Appendix (full templates and instantiations for each task)
- **Prompt engineering skills (system messages):**
  - Table 12 in Appendix

---

### 7. What are the results of the prompting strategies and how successful or not were they?

**Key results summarized:**

- No single prompting strategy is always best; effect is **task-dependent**.
- **0-shot**:
  - For tasks similar to general NLP, like bug report summarization, 0-shot already performs very well (often matching/exceeding SOTA).
- **Few-shot**:
  - Helps more with tasks where examples clarify the expected answer, e.g., bug report identification (big increase in recall in Table 4).
- **Expertise/self-heuristic**:
  - **Expertise**: Manually inserted domain rules boost performance, especially for tasks needing security domain knowledge (bug report identification, stable patch classification).
  - **Self-heuristic**: In complex tasks (e.g., vulnerability severity evaluation), having the model summarize rules from examples and feeding those rules back—**significantly improves recall, precision, and overall performance** (see Table 5).
  - For severity mapping (Table 5), self-heuristic + GPT-4 nearly matches or beats SOTA.
- In **patch correctness** and **stable patch** tasks, how information is supplied (code only vs. code+desc) can change outcomes; overloading with irrelevant info can hurt.
- In **more complex reasoning or highly specialized domains**, tailored expertise or self-heuristic prompts generally beat pure in-context example-based prompts.

---
**Numbers Examples:**
- For security bug report identification (Table 4): recall with 1-shot is 0.76 (vs. 0.35 for 0-shot); expertise prompt improves precision markedly.
- For vulnerability severity evaluation (Table 5): recall and precision for self-heuristic + GPT-4 is 0.98/0.77 (AV:Network); 0-shot is far worse.
- For bug report summarization (Table 3): few-shot and GPT-4 exceeds ROUGE-F1 of SOTA.
- For patch correctness (Table 8): code-only + GPT-4 approaches SOTA performance; using general-info or expertise sometimes made things worse (if ChatGPT got distracted).

---

### 8. Why did one strategy give better results than another?

- **Task dependence:**  
  If the task is similar to standard NLP/translation tasks, ChatGPT is capable out-of-the-box (pretraining covers much of what’s needed).
- **Demonstrations help learning specific mappings:**  
  In classification tasks with clear boundaries or rare categories, **few-shot or 1-shot** gives better "pattern awareness" for ChatGPT.
- **Expertise/self-heuristic prompts:**  
  Where task success requires precise domain knowledge (e.g., mapping CVSS scores, nuanced bug labels), **expertise-infused** and **self-heuristic** prompts work best, avoiding model hallucinations or missed domain-specific cues.
- **Too much or redundant/unfocused information:**  
  **Adding demonstrations or details randomly can hurt performance, if they include irrelevant or confusing contexts** (as in security bug report ID—see Table 4, and patch correctness). ChatGPT may latch onto spurious correlations.
- **Extracted knowledge focuses the model:**  
  **Self-heuristic prompts force ChatGPT to extract and focus on rules and characteristics relevant to the task**, improving generalization and accuracy.

---

### 9. All specifics of the researchers' prompt strategies.

**Prompt Types (Table 2/Table 13):**

- **0-shot**: Direct instruction and input only.
- **1-shot/few-shot**: Provide 1 or 4 demonstration examples (randomly sampled from train set).
- **General-info**:
  - Use system/user/assistant turn structure, inject role, step-by-step instructions, reinforcement, positive feedback, ask to 'think step by step', make the right decision.
- **Expertise**:
  - Add domain rules or guidance derived from official docs, literature (Table 14: e.g., precise definitions, required features, lists of indicators, etc.).
- **Self-heuristic**:
  - First, feed ChatGPT a set of labeled or categorized examples and ask it to summarize what defines each class or score.
  - Second, take ChatGPT’s own output (the summary) and feed it as explicit guidance into the test prompt.
  - Example (vulnerability severity evaluation): “Functions that involve network communication, socket handling, etc. qualify for AV:Network.” (ChatGPT itself summarizes this based on training examples; see Figure 4/6.)

**Implementation Details:**
- All prompts are "manually engineered," with templates refined via reviewing ChatGPT outputs on train/probe-test set.
- 10% of the dataset used as probe-test for prompt selection.
- For some tasks (e.g., code patching), prompts were adjusted to suit code input length constraints or to avoid ChatGPT hallucinations (avoiding unrelated or overloaded prompt content).

**Key skills in General-info template (Table 12):**
- Role assignment, reinforcement, task confirmation, positive feedback, explicit stepwise reasoning ("zero-CoT"), correct-answer directive.

**Shows how prompting can be:**
- *Minimalist* (0-shot), *example-based* (few-shot), *instructional/structured* (general-info), *domain-expert* (expertise), and *model-self-derived* (self-heuristic).
- All templates and sample instantiations provided in Table 13 (see especially two long prompt examples at the end).

---

**In summary:**  
The paper deeply investigates ChatGPT for vulnerability management using diverse prompt strategies. It finds that while ChatGPT is already strong for some tasks, judicious prompt engineering—especially leveraging domain expertise or "knowledge distillation" techniques (self-heuristic)—is critical for more complex or niche vulnerability management scenarios. The exact best prompt varies by task, and the study provides a granular, empirical mapping of what works, when, and (often) why.
