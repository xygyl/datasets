## 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point/Problem**:  
The paper addresses the challenge of **identifying the specific functions in open-source software that are vulnerable based on public CVE (Common Vulnerabilities and Exposures) descriptions**. Existing tools and datasets rarely pinpoint the actual vulnerable functions (VFs), just the libraries or versions, which leads to high false-positive rates in security tools and makes it difficult to perform accurate vulnerability assessments or reachability analyses.

**What it tries to explore**:  
- How to automatically and accurately **localize vulnerable functions** in code given only (i) the CVE description, and (ii) the corresponding source code, even when patches and PoCs are unavailable.
- How **Large Language Models (LLMs)**, when equipped with tailored in-context learning (ICL) strategies, can extract key vulnerability information (e.g., "attack vector" and "root cause") from free-text CVE descriptions and use it to improve code search and matching.
- How this method (called **VFFinder**) can be used to reduce false alarms in software composition analysis (SCA) tools.

**What makes it unique**:  
- It’s **the first work to leverage LLMs and in-context learning** for the task of **vulnerable function identification based solely on natural language vulnerability descriptions**.
- Proposes an **ICL-enhanced extractor**: a prompt- and demonstration-driven method tailored for extracting entities from technical vulnerability text.
- Combines IR (Information Retrieval) techniques with LLM-enhanced query representation, semantic retrieval, and code corpus augmentation to substantially outperform prior baselines.
- Provides an open-source artifact (https://github.com/CGCL-codes/VFFinder) to benefit further research.

---

## 2. What dataset did they use?

They used several key datasets:

1. **A&R Dataset (DAR)**:
   - Human-labeled attack vector and root cause entities for 642 CVEs (drawn from 833 Java-related CVEs in the Cross-Project Vulnerability Dataset, CPVD [53]).
   - Used to train and evaluate the ICL-based extractor.

2. **Vulnerable Function Dataset (DVF)**:
   - Ground-truth mappings of 77 CVEs to their actual vulnerable functions across 75 open-source Java projects.
   - Built from previous works (e.g., Transfer [24]), PoC reproduction, and manual verification.

3. **Cross-project Vulnerability Dataset (DCV)**:
   - Data on downstream projects and their dependencies, drawn from CPVD [53], with 62 CVEs, 61 upstream vulnerable OSS, and 9,829 downstream instances.
   - Used to evaluate the reduction of false positives in SCA tools.

---

## 3. What LLM models were used?

The main Large Language Model used is:

- **ChatGPT (gpt-3.5-turbo-0613)** from OpenAI.
  - Used as the foundation for the in-context learning extractor for entity extraction.
  - All LLM queries in the experiments were conducted using this model.

Additionally, for embedding and similarity:
- **angle-llama-7b-nli-20231027**, an STS (semantic textual similarity)-fine-tuned model based on Llama-2-7b-hf.
  - Used to create query/document embeddings and for semantic similarity during function retrieval.

---

## 4. What are the prompting strategies discussed in this paper?

Two major prompting strategies for entity extraction from CVE descriptions:

1. **Direct Prompting (LLM without ICL)**:
   - Supply the model with an explicit instruction and a bare CVE description.
   - Example: “Please extract the Root Cause and the Attack Vector for me.”

2. **ICL-Enhanced Prompting (LLM with in-context learning using Demonstrations)**:
   - In addition to instruction and the target CVE, also **supply several demonstration examples** (chosen for their structural pattern similarity, not just semantic similarity).
   - Each demonstration provides a CVE description along with labeled attack vector and root cause entities.
   - Demonstrations are **retrieved via custom pattern embeddings** that abstract out anchor entities (vendor/product/version, CWE-type, component, attacker) and focus on sentence structure to find the most pattern-similar examples.

---

## 5. What specific prompts were used?

Two main prompt formats (corresponding to the two approaches above):

**a) Direct Prompt (as used in baseline):**
```
Please extract the Root Cause and the Attack Vector for me.
[CVE Description]
```

**b) ICL-Enhanced Prompt (main VFFinder approach):**
```
Instruction:
I want you to act like an expert in vulnerability report analysis. I will give you a description. Your task is to extract the attack vector and root cause from it.
Attack vector is the method of triggering the vulnerability. If you cannot find the corresponding answer, just answer None.

Demonstration cases:
Description: OpenRefine before 3.2 beta allows directory traversal via a relative pathname in a ZIP archive
Answer: {"attack vector": "relative pathname in a ZIP archive", "root cause": "None"}

[More demonstration cases with varied, similar patterns]

CVE Description:
[Target CVE description here]
```

- For each LLM query, 3 demonstration examples are prepended, sorted by structural pattern similarity (not raw semantics).
- There is also a heuristic process for abstracting and embedding sentence patterns for demonstration retrieval.

---

## 6. Where are the prompt strategies explained?

- **Section 3.2: In-Context Learning** – High-level introduction to ICL.
- **Section 3.3: Motivating Examples (and Figure 2, Table 1, Table 2)** – Comparison and analysis of direct prompt vs. pattern-based ICL.
- **Section 4.1: ICL-Enhanced Extractor** – Full details of the demonstration selection, the construction of the prompt, anchor entity abstraction, parsing, pattern retrieval, and prompt format (Figure 5).

---

## 7. What are the results of the prompting strategies? How successful or not were they?

Evaluation (Table 1, “Effectiveness of Vulnerability Entities Extraction”):

- **JSI (Jaccard Similarity Index) and Coverage Rate:**
  - **Baseline (CaVAE):** JSI ~0.04, Coverage ~0.07
  - **Direct LLM Prompting:** JSI 0.35, Coverage 0.41
  - **Semantic ICL (S-ICL):** JSI 0.54, Coverage 0.56
  - **Pattern ICL (P-ICL, the proposed method):** JSI 0.58, Coverage 0.59

**Conclusion:**
- ICL-enhanced prompts **substantially outperform direct LLM extraction** and unsupervised methods (CaVAE).
- Structure/pattern-based demonstration selection (P-ICL) beats semantic-based demonstration selection (S-ICL).
- The extraction is more accurate (higher coverage) and less redundant (shorter output) when pattern-based ICL is used.

---

## 8. Why did one strategy give better results than another?

- **LLMs alone (direct prompts)** struggle to accurately and concisely extract the relevant entities amidst natural language noise.
- **Unsupervised methods (like CaVAE)** rely on fixed noun phrase patterns and can miss critical attributes that don’t match their assumptions.

**In-context learning (ICL) with demonstration examples:**
- **Improves LLM performance** by giving the model concrete examples, especially those with *similar syntactic and semantic patterns*.
- **Pattern-based demonstration selection** enables the LLM to learn from cases where entity boundaries and cues are similar, aiding accurate extraction in a new, but structurally related, target description.
- **Semantic similarity retrieval** (S-ICL) is less effective since it can present demonstrations with similar topics but different structures, which does not help the LLM as much in recognizing the precise entity boundaries.

**Therefore:**  
ICL using pattern-similar demonstrations (P-ICL) gives the LLM “templates” for what and where to extract, making extraction more consistent and focused.

---

## 9. All specifics of the researcher's prompt strategies

### a) **Prompt Construction**

- **Instruction**: Always gives context (“extract attack vector and root cause; if not found, answer None”).
- **Demonstrations**: For each input, *select 3 closest pattern-matched examples* from a manual case pool (labeled with attack vector and root cause).
  - Demonstrations are chosen not just by embedding similarity but via a custom pattern embedding process:
    - Recognize anchor entities: vendor/product/version (VPV), vulnerability type (VT), component, attacker.
    - Abstract the CVE description, replacing anchors with type labels, parse into a syntactic tree, and collapse subtrees to make a pattern vector.
    - Compute cosine similarities between pattern vectors to select the most structurally similar demonstration cases.
- **Prompt Format**:  
  - [Instruction]
  - [3 Demonstration Descriptions + their labeled extracted entities]
  - [Target CVE Description]
- **LLM Settings**: ChatGPT gpt-3.5-turbo-0613, temperature 0 for reproducibility. The ground-truth case is removed from the demonstration pool for each query to avoid leakage.
- **Sentence-level Extraction**: The extractor is run on each sentence for multi-sentence CVEs.

### b) **Entity Coverage and Output**

- Extracted entities are tokenized, stemmed, and redundancy/overlap checked.
- Output includes both the required fields (“attack vector” and “root cause”) with “None” if something is not present (as instructed by the prompt).

### c) **Impact of Prompt Components**

- The ablation study (Figure 7) shows that removing ICL-enhanced extraction decreases Top-1 accuracy by 10.8%, and larger K by up to 13%.
- Compared to other components (like class matching), the ICL-enhanced extractor is among the most impactful parts.

---

### **Summary Table: Prompt Related Results**

| **Method**      | **Coverage (AV+RC)** | **JSI** | **Notes**                       |
|-----------------|----------------------|---------|----------------------------------|
| CaVAE (baseline)| 0.07                 | 0.04    | Unsupervised, poor performance   |
| Direct LLM      | 0.41                 | 0.35    | No examples, mid performance     |
| S-ICL           | 0.56                 | 0.54    | Semantic similarity examples     |
| P-ICL (VFFinder)| **0.59**             | **0.58**| Pattern example, best performance|

---

## Brief Answers per Question

1. **Point/Unique**: To use LLMs (with ICL) to accurately localize vulnerable functions from CVE descriptions + code. Unique in combining LLM/ICL and IR for this task.
2. **Datasets**: Human-labeled A&R for entity extraction, 77 CVEs with function-level ground-truth, large CPVD dataset for downstream analysis.
3. **LLMs**: ChatGPT (gpt-3.5-turbo-0613), and angle-llama-7b-nli for embedding/similarity.
4. **Prompt Strategies**: Direct prompt vs. ICL-enhanced prompt with demonstration selection—pattern-based is best.
5. **Specific Prompts**: Instruction + Top-3 pattern-matched demonstration cases + target CVE. Full format shown above.
6. **Prompt Explanation**: Explained in Section 3.2, 3.3, 4.1, with examples in Figures 1–5.
7. **Prompting Results**: Pattern ICL yields highest accuracy and lowest redundancy. JSI jumps from 0.35 (direct) to 0.58.
8. **Why pattern ICL is best**: It gives LLM relevant structural examples, helping it find correct entity boundaries.
9. **Prompt Specifics**: Anchor/entity recognition, pattern abstraction, demonstration retrieval via pattern vectors, instruction template, 3 pattern-matched examples, “None” policy, sentence-level extraction.

---

Let me know if you want more details on any specific section, including code examples, diagrams, or further breakdowns of a particular component or experiment!
