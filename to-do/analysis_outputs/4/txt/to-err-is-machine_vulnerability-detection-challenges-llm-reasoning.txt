### 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point / Exploration:**
- This paper systematically evaluates how well state-of-the-art Large Language Models (LLMs) perform at vulnerability detection in source code—a task that fundamentally requires nuanced, multi-step code reasoning.
- The authors show that, despite LLMs’ strong performance in natural language, math, and even code generation/execution, they are *poor* at reliably identifying whether a code snippet contains a security vulnerability.
- They study not only model performance but also in-depth *failure analysis* to pinpoint reasoning steps and code constructs that trip up LLMs.

**What makes it unique:**
- It is the first to systematically benchmark a *wide range* (14) of SOTA LLMs and multiple prompting/training strategies, focusing on vulnerability detection as a reasoning task.
- The paper goes beyond prior works by manually annotating and categorizing reasoning failures, offering actionable insights (e.g., distinguishing errors in localization vs. understanding code semantics vs. logical inference).
- It assesses whether improvements like model scaling, better data, fine-tuning, instruction tuning, and more sophisticated prompts are sufficient—not just raw accuracy.
- It introduces and evaluates prompt designs that inject domain knowledge, targeting specific reasoning weaknesses (like bounds/null checks).
- The study highlights that current pretraining paradigms (mainly autoregressive on code text) may be fundamentally insufficient for deep code reasoning.

---

### 2. What dataset did they use?

- **Primary dataset:** **SVEN** (He & Vechev, 2023)
  - Contains **772 real-world C/C++ functions**, each with a vulnerable version and a corresponding fixed (patched) version.
  - Manually curated to ensure high label accuracy (94%).
  - Average function length: 168 lines.
- **Supplementary datasets:**
  - To check findings, they also used **CWE** (Common Weakness Enumeration) and **SARD** (NIST Software Assurance Reference Dataset) containing shorter, simpler vulnerability examples.

---

### 3. What LLM models were used?

They evaluated **14 SOTA LLMs** (Appendix B, Table 4):

| Model             | Parameters     |
|-------------------|---------------|
| GPT-4             | -             |
| GPT-3.5-turbo     | -             |
| Gemini 1.0 Pro    | -             |
| Mixtral (8x7B MoE)| ~45B          |
| CodeLLAMA         | 7B, 13B, 34B  |
| LLAMA2            | 7B, 13B       |
| WizardCoder       | 33B           |
| DeepSeek-Coder    | 1.3B, 6.7B, 33B|
| StarChat2         | 15.5B         |
| StarChat          | 15.5B         |
| StarCoder2        | 15.5B         |
| StarCoder         | 15.5B         |
| MagiCoder         | 7B            |
| Mistral           | 7B            |

**See Table 4 in Appendix B for full details.**

---

### 4. What are the prompting strategies discussed in this paper?

From Section 2, Section 3.3, and Appendix A, they discuss:

1. **Basic (Zero-Shot) Prompting**: Minimal instructions (e.g., “Is the following function buggy? Yes/No”).
2. **In-Context (n-shot) Prompting**: Provides a few labeled examples before the target code.
   - **Random examples**
   - **Embedding-based selection** (using code similarity by CodeBERT)
3. **In-Context Using Contrastive Pairs**: Give the vulnerable and fixed versions of similar code as examples to highlight subtle semantic differences.
4. **Chain-of-Thought (CoT) from CVE Descriptions (CoT-CVE)**: Uses bug reports (from Big-Vul) to provide step-wise reasoning in prompts.
5. **Chain-of-Thought from Static Analysis (CoT-StaticAnalysis)**: Uses static analyzer (Infer) output as stepwise reasoning “proofs” for vulnerabilities.
6. **Chain-of-Thought with Annotations (CoT-Annotations)**: Adds targeted, custom static analysis annotations to the code highlighting bounds/null checks, with step-by-step reasoning instructions.
7. **Other minor variants**: e.g., Q/A formatting, introducing a CWE list in the query, etc.

---

### 5. What specific prompts were used?

**Examples are provided in Appendix A and Section 3.3.**

#### a) Basic/Zero-Shot:
- System prompt: “I want you to act as a vulnerability detection system.”
  - Query prompt: “Is the following function buggy? Please answer Yes or No.”
  - Alternative: “Is the following function vulnerable?” (less effective)
- CWE list variant: “Does the following function contain one of the following bug types? [list]”

#### b) In-Context (n-shot):
- Few examples, each with code and label, prepended to the query (from random selection, embedding-based selection, or contrastive pairs).
- Format: input code + model’s answer (“Answer: yes/no”)—the number of examples varied (6 was optimal).

#### c) Contrastive In-context:
- Examples show both buggy and fixed code snippets, emphasizing that minor code changes can affect vulnerability status.

#### d) CoT from CVE Descriptions:
- Each example includes a CVE bug report used to construct a step-by-step (chain-of-thought) explanation of why code is vulnerable.
  - E.g., converting CVE-2017-9211’s human description into a CoT reasoning section, ending with “Therefore, the example is buggy.”

#### e) CoT from Static Analysis:
- Each example includes the output of the Infer static analyzer, listing the reasoning steps as bullet points:
  1. A buffer buf of size 10 is allocated at line 1.
  2. An index i is initialized to [0,100] at line 2.
  3. The index i is used to access buf at line 3. This may exceed the bounds of buf.
  - “Therefore, the example is buggy.”

#### f) CoT-Annotations (targeted, for NPD - Null Pointer Dereference):
- Annote the code with null assignments, dereferences, checks.
- Precede the code with natural language instructions to reason step-by-step, such as:
  1. Identify where pointers are likely to be null.
  2. Which of these are dereferenced?
  3. Which dereferences are checked/handled? Filter these out.
  4. If any remain unchecked, declare the function vulnerable.

**Quote from Figure 8:**
```
Instructions: For the Query Example only, think step-by-step using these steps, then give an answer...
1. identify which locations pointers are likely to be null.
2. identify which of these are dereferenced.
3. identify which of these dereferences of potentially-null pointers are checked and handled. Filter these out.
4. if any unchecked dereferences remain, then the function is vulnerable. Otherwise, the function is not vulnerable.
```

---

### 6. Where are the prompt strategies explained?

- **Section 2** ("Can LLMs Effectively Detect Vulnerabilities?")—first introduces all baseline and additional prompting methods.
- **Section 3.3**—explains the CoT-Annotations strategy in detail.
- **Appendix A ("Vulnerability Detection Prompts")**—comprehensively spells out each prompt format, their variations, and the rationale.
- **Figures and tables** in the main text (Figure 2, Figure 8) visually illustrate example prompts and comparative results.

---

### 7. What the results of the prompting strategies are and how successful or not were they?

**Core finding:** None of the prompt strategies enabled LLMs to substantially surpass random-guessing (50% balanced accuracy) on the realistic SVEN benchmark.

- The *best* baseline models achieved only 54.5% balanced accuracy; most models/prompt combinations achieved close to 50–52%.
- Even with more nuanced prompts (Contrastive, CoT-CVE, CoT-StaticAnalysis, CoT-Annotations), *no model or prompt exceeded random-guessing by more than 5%* (see Figure 2).
- Explicit domain knowledge in CoT-Annotations improved specific *subtasks* (e.g., identifying null pointer checks) for a few models (CodeLLAMA, MagiCoder, Mistral)—error rates for that step dropped 15–70%—but overall vulnerability detection performance did not significantly improve (see Figure 9).
- LLMs *struggled to distinguish paired vulnerable/fixed code*: On average, models could not distinguish 69.7% of pairs (Table 2).

---

### 8. Why did one strategy give better results than another?

- **Contrastive In-context Learning and CoT-based Prompts**: These prompt strategies did help some models slightly because they more clearly signal what semantic cues matter (bug vs. fix, stepwise logic) and provide context for how subtle changes impact vulnerability.
- **CoT-Annotations**: Targeted annotations address LLMs' main failure—recognizing bounds/null checks—but this only helps for one specific sub-reasoning step. End-to-end detection success still depends on multi-step logic, which remains a bottleneck.
- Larger models were marginally better at following prompts and leveraging the context provided by in-context examples (emergent property of scale in in-context learning), but even the largest models (e.g., GPT-4) could not excel.
- No prompt or training variant overcame the fundamental gap: LLMs lack deep code semantic understanding and multi-hop reasoning, likely due to pre-training regimes not emphasizing *code execution semantics* or *logical/temporal variable relationships*.

---

### 9. All specifics of the researcher's prompt strategies

From **Appendix A** and main text:

#### **A) Basic (Zero-Shot) Prompt**
- System prompt describes role (vuln detection system).
- Query: “Is the following function buggy? Please answer Yes or No.”
- Alternatives: CWE list (lists possible bug types); Q/A form: “Question: ...”, “Answer: ...”

#### **B) In-Context (n-shot) Prompt**
- Prepend N function:label pairs to the prompt.
- Example selection: random, embedding (CodeBERT), or contrastive (buggy/fixed pair).
- Format: single message containing all examples; optimal N=6.
- The example code and answers are formatted as in real interaction.

#### **C) Contrastive Pair In-Context**
- Provide both buggy and fixed versions of semantically similar code as paired in-context examples.
- Objective: highlight how minor changes impact security status.

#### **D) CoT from CVE Descriptions**
- Build reasoning chain from human bug report, e.g.:
    > “The crypto skcipher init tfm function in crypto/skcipher.c... lacks a key-size check, which allows... a denial of service (NULL pointer dereference) via a crafted application. Therefore, the example is buggy.”

#### **E) CoT from Static Analysis**
- Use Infer’s output bug path (list of statements that trigger bug), recast as natural language step-by-step reasoning.
- E.g.,
    1. Buffer buf of size 10 allocated at line 1.
    2. Index i in [0, 100] at line 2.
    3. Index i used to access buf at line 3 (may exceed bounds). Therefore, example is buggy.

#### **F) CoT-Annotations (most targeted prompt, for research step Section 3.3)**
- Output from a custom static analysis tool highlights null assignments, null-checks, dereferences in code.
- Prompt instructions explicitly outline a stepwise process to guide the model’s reasoning.
    - Identify potentially-null pointers.
    - Identify dereferences.
    - Check if dereferences are checked/handled.
    - Declare vulnerable if any unchecked dereferences.
- Used for case study on Null Pointer Dereference bug type.

---

**References to prompt strategies and their locations:**
- **Section 2:** explainer/overview of all prompt types
- **Appendix A:** verbatim prompt templates and design decisions; prompt variations, format, and selection rules.
- **Section 3.3** and **Figure 8:** example application of CoT-Annotation prompt.

---

# **Summary Table**

| Prompt Strategy     | Description / Implementation | Where explained         | Success |
|---------------------|-----------------------------|-------------------------|---------|
| Basic (zero-shot)   | Simple role + yes/no query  | Section 2, Appx A       | Poor    |
| In-Context (n-shot) | Prepend N labeled examples  | Section 2, Appx A       | Slight Δ|
| Contrastive Pair    | Buggy/fixed pairs as context| Section 2, Appx A       | Best for 8/14, slight Δ|
| CoT-CVE             | Chain-of-thought from bug report | Section 2, Appx A  | Slight Δ|
| CoT-StaticAnalysis  | Chain-of-thought from static analyzer | Section 2, Appx A | Slight Δ|
| CoT-Annotations     | Code annotated w/static analysis + step-by-step instructions | Section 3.3, Fig 8, Appx A | Error reduction in step; overall detection still limited |

*Δ = incremental gain, generally <5%*

---

## If you need verbatim prompt templates or further details on any specific strategy, let me know!
