### 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point/Goal:**  
The paper investigates how different prompt engineering strategies can enhance the performance of large language models (specifically ChatGPT-4) in automatic software vulnerability detection, a critical task for software security.

**What it explores:**  
- The impact of various prompt designs (prompt engineering) on ChatGPT's ability to detect vulnerabilities in source code.
- Whether adding structural and sequential auxiliary information (like API call sequences and dataflow graphs) to prompts helps ChatGPT detect vulnerabilities better.
- The utility of chain-of-thought (multi-stage, stepwise) prompting for this task.

**What makes it unique:**  
- First comprehensive study of prompt engineering for LLM-based vulnerability detection—prior work gave ChatGPT naive, simple questions (e.g., "Is this buggy?") without designing prompts tailored for software vulnerability detection.
- Explores the addition of traditional static analysis concepts (API call sequences, dataflow graphs) directly into LLM prompts.
- Conducts a systematic comparison with prior methods (machine learning, deep learning approaches).
- Tests strategies such as multi-round (chain-of-thought) prompting and prompt ordering.

---

### 2. What dataset did they use?

They use **two datasets** (Section 3):
- **Java Dataset:**  
  - Collected from SARD (Software Assurance Reference Dataset).
  - Contains both real-world vulnerable samples and synthetic test cases, labeled as good (non-vulnerable), bad (vulnerable), or mixed.
  - After preprocessing: 1,171 vulnerable functions and 917 non-vulnerable functions, focused on the top 50 vulnerability types, total 2,088 samples.

- **C/C++ Dataset:**  
  - Collected from NVD (National Vulnerability Database), using the benchmark from prior work [31].
  - Focused on files with either a CVE (Common Vulnerability and Exposure) or its patch.
  - After preprocessing: 1,015 vulnerable and 922 non-vulnerable functions, total 1,937 samples.

**Auxiliary Information (for prompts):**
- API call sequences (extracted using tree-sitter and traversal rules).
- Data flow graphs (DFG), extracted following GraphCodeBERT's method.

---

### 3. What LLM models were used?

Only a single model was used:
- **ChatGPT-4** (the current latest version at the time of experiments; in the paper, "ChatGPT" always refers to version 4).

No experiments were run with other LLMs. (Other models are discussed in related work/background.)

---

### 4. What are the prompting strategies discussed in this paper?

The strategies examined include:

1. **Basic Prompting**: Simple question asking whether a piece of code is buggy or correct, requesting a Yes/No answer.
2. **Role-based Prompting**: Same as the basic, but with instructions specifying ChatGPT’s role, e.g., “I want you to act as a vulnerability detection system.”
3. **Reverse-Question Prompt**: Asks the opposite (e.g., "Is the code correct?") to detect prediction biases.
4. **Prompting with Auxiliary Information**:
   - **API Call Sequences**: Embedding information about the sequence of API calls in the prompt.
   - **Dataflow Graphs (DFG)**: Embedding dataflow information as additional text in the prompt.
5. **Chain-of-Thought Prompting**:
   - Multi-step prompts: First, ask ChatGPT to summarize or state the intent of the code; next, given the summary, ask if the code is buggy.
6. **Prompt Order/Positioning**:
   - Varying the order of code and auxiliary information within the prompt (e.g., putting API call sequence before or after the code).
7. **Prompts with Code Summaries**:
   - Inserting LLM-generated code summaries as context before asking for vulnerability detection.

---

### 5. What specific prompts were used? (with examples)

#### Main Prompt Variations (Section 4; Table 2)

- **Basic Prompt (Pb):**  
  Is the following program buggy? Please answer Yes or No. [CODE]

- **Role-based Basic Prompt (Pr-b):**  
  I want you to act as a vulnerability detection system. My first request is "Is the following program buggy?" Please answer Yes or No. [CODE]

- **Role-based Reverse-Question Prompt (Pr-r-b):**  
  I want you to act as a vulnerability detection system. My first request is "Is the following program correct?" Please answer Yes or No. [CODE]

- **Prompt with Dataflow (Pr-b-d):**  
  I want you to act as a vulnerability detection system. I will provide you with the original program and the data flow information, and you will act upon them. Is the following program buggy? [CODE].[DF description]

- **Prompt with API Sequence (Pr-a-b):**  
  I want you to act as a vulnerability detection system. I will provide you with the original program and the API call sequence, and you will act upon them. [API description]. Is the following program buggy? [CODE]

- **Chain-of-thought Prompt:**
  - Step 1 (P1): Please describe the intent of the given code. [CODE]
  - Step 2 (P2, r-b): I want you to act as a vulnerability detection system. Is the above program buggy? Only answer Yes or No.

- **Prompt templates for [API description]**:  
  The program first calls a1, then calls a2, ..., and finally calls an.

- **Prompt templates for [DF description]**:  
  The data value of the variable vi at the pi-th token comes from/is computed by the variable vj at the pj-th token.

- **Position variations:**  
  - [POS1].Pb.[POS2]  
  - Pr.[POS1].Pb.[POS2]  
  [API description] or [DF description] can be placed in POS1 (before code) or POS2 (after code).

---

### 6. Where are the prompt strategies explained?

- **Section 4, "Prompt Design"**: Detailed explanation of all prompting strategies and the motivation for each.
- **Section 5, "Experimental Results"**: Explanation of how each prompt is used and compared.
- **Prompt templates for auxiliary information are in Section 4.2.2.**
- **Position experiments are in Section 5.5; order and configurations are tabled in Table 5.**
- **Chain-of-thought prompts are described in Section 4.3.**
- **Prompts with code summaries are discussed in Section 5.4.2 (Table 4).**

---

### 7. What are the results of the prompting strategies and how successful were they?

**Highlights (Section 5.2, Table 2; Section 5.3; Section 5.4, Table 3):**
- **ChatGPT with Prompt Engineering > Baselines**: ChatGPT (with prompts) outperforms classic ML (CFGNN, Bugram) on Java and has much higher recall for vulnerabilities.
- **Role specification helps** (Pr-b vs. Pb): Small improvements, more on Java.
- **Auxiliary info** (API calls, DFG):  
  - Adds a slight improvement on Java; helps with precision for non-vulnerable code, mixed on vulnerabilities.
  - API call sequence is more helpful on Java; DFG helps a little on C/C++ (but not by a large margin).
- **Chain-of-thought prompting**:  
  - On Java: No consistent gain, sometimes lost accuracy.
  - On C/C++: Marked improvement in accuracy!
- **Prompt content/order matters**:  
  - API calls before the code is generally better. DFG after the code is better.
- **Code summaries**: Helped accuracy on C/C++, but hurt Java performance.

**Key quantified outcomes:**
- Java (best case, API-augmented prompt): ~0.75 accuracy overall (vs. CFGNN/Bugram ~0.43).
- C/C++ (best case, chain-of-thought): ~0.74 accuracy (vs. CFGNN/Bugram ~0.48).
- ChatGPT is especially strong at spotting vulnerabilities (high recall), but default prompts tend to overpredict vulnerability ("buggy" bias).
- Explaining detected vulnerabilities: ~64% of explanations for Java and 52% for C/C++ rated highly, but about half lacked correct vulnerability reasoning.

---

### 8. Why did one strategy give better results than another?

**Why strategies vary:**
- **Auxiliary info lets LLMs "see" code relationships** (APIs/dataflow) that are not captured in surface code, aiding in understanding complex vulnerabilities.
- **API calls are more explicit in Java code behavior**, which helps ChatGPT spot bugs related to misuse or sequence errors.
- **Dataflow graphs help with C/C++**, where vulnerabilities often depend on value propagation not obvious from syntax alone.
- **Prompt order influences saliency:** Placing API information before the code likely primes the LLM to attend to behaviors within the code relevant to vulnerability context. Placing DFG after code helps the LLM map inference onto the observed code flow.
- **Role-prompts focus the model**: Telling the model to act as "a vulnerability detection system" reduces irrelevant responses and primes for the right answer type.
- **Chain-of-thought works in C/C++** because stepwise reasoning clarifies code intent before checking for bugs; in Java, perhaps code and vulnerabilities are more directly signaled, so summarization does not help.

---

### 9. All specifics of the researcher's prompt strategies

Here's a summarized "dictionary" of all prompt strategies:

- **Pb (Basic):**
  ```
  Is the following program buggy? Please answer Yes or No.
  [CODE]
  ```
- **Pr-b (Role-based):**
  ```
  I want you to act as a vulnerability detection system.
  My first request is "Is the following program buggy?"
  Please answer Yes or No.
  [CODE]
  ```
- **Pr-r-b (Role-based, reverse):**
  ```
  I want you to act as a vulnerability detection system.
  My first request is "Is the following program correct?"
  Please answer Yes or No.
  [CODE]
  ```
- **Pr-b-d (Role-based, DFG after code):**
  ```
  I want you to act as a vulnerability detection system.
  I will provide you with the original program and the data flow information, and you will act upon them.
  Is the following program buggy?
  [CODE]
  [DF description]
  ```
- **Pr-a-b (Role-based, API seq before code):**
  ```
  I want you to act as a vulnerability detection system.
  I will provide you with the original program and the API call sequence, and you will act upon them.
  [API description]
  Is the following program buggy?
  [CODE]
  ```
- **Pr-a-b-d (Role-based, API before, DFG after):**
  ```
  I want you to act as a vulnerability detection system.
  I will provide you with the original program, API call sequence, and data flow information, and you will act upon them.
  [API description]
  [CODE]
  [DF description]
  ```
- **API Description (template):**
  ```
  The program first calls <API-1>, then calls <API-2>, ..., and finally calls <API-n>.
  ```
- **DFG Description (template):**
  ```
  The data value of the variable <v_i> at the <p_i>th token comes from/is computed by the variable <v_j> at the <p_j>th token.
  ```
- **Chain-of-Thought Prompting:**
  - Step 1:
    ```
    Please describe the intent of the given code.
    [CODE]
    ```
  - Step 2:
    ```
    I want you to act as a vulnerability detection system. Is the above program buggy? Only answer Yes or No.
    ```
  - OR, include auxiliary info in Step 2:
    ```
    I want you to act as a vulnerability detection system. Is the above code buggy? Only answer Yes or No.
    Here is its API call sequence/data flow information that you may use: [API description]/[DF description]
    ```
- **Prompt position/order experiments:**
  - API info before/after code ([POS1]/[POS2]).
  - E.g. [API description]. [CODE]. OR [CODE]. [API description].

- **Prompts with summary:**
  - Add code summary (from LLM or preprocessing) as part of prompt, either before or after the code.

---

**Summary Table of key prompt composition (from Table 2 & 5):**
| Prompt                 | Order in prompt                                   | Description                           |
|------------------------|---------------------------------------------------|---------------------------------------|
| Pb                     | [CODE]                                            | Basic                                 |
| Pr-b                   | [Role] [CODE]                                     | Role-based                            |
| Pr-a-b                 | [Role] [API desc] [CODE]                          | Role + API before code                |
| Pr-b-d                 | [Role] [CODE] [DF desc]                           | Role + DFG after code                 |
| Pb-a                   | [API desc] [CODE]                                 | API before code (no role)             |
| Pa-b                   | [CODE] [API desc]                                 | API after code (no role)              |
| Pr-b-a                 | [Role] [CODE] [API desc]                          | Role + API after code                 |
| Pr-a-b-d               | [Role] [API desc] [CODE] [DF desc]                | Role + API before, DFG after          |
| Chain-of-Thought       | Step1: intent Q, Step2: vulnerability detection Q | Multi-turn, decomposition             |

---

**For more details or for code snippets of prompt generation and more nuanced variations, check Section 4, Section 5, and their tables (Tables 2, 3, 4, 5).**

---

Please let me know if you need any further breakdown, code examples, or more granularity on any of the points above!
