## 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point of the Paper:**
- The paper challenges the prevailing view that Large Language Models (LLMs) require few-shot (with examples) prompting to demonstrate strong multi-step reasoning.
- It shows that LLMs can perform strong "zero-shot reasoning" (no in-context training examples) when prompted with a simple phrase like **"Let's think step by step."**
- The authors introduce "Zero-shot-CoT" (Zero-shot Chain of Thought), which is a single fixed prompt that elicits multi-step, chain-of-thought reasoning from LLMs across a wide variety of tasks—without any few-shot examples.

**Uniqueness:**
- The simplicity and universality of the approach: a single phrase ("Let's think step by step") can drastically boost performance across unrelated complex reasoning tasks.
- Contrasts with previous approaches that required crafting task-specific few-shot prompts, or custom templates, or fine-tuning.
- Establishes a new, stronger zero-shot baseline for reasoning tasks.
- It demonstrates the existence of broad, untapped zero-shot cognitive abilities in existing LLMs.

---

## 2. What dataset did they use?

**Benchmark datasets** were selected to cover a broad variety of reasoning tasks:

### 1. **Arithmetic Reasoning:**
   - [SingleEq](https://aclanthology.org/Q15-1042/)
   - [AddSub](https://aclanthology.org/D14-1058/)
   - [MultiArith](https://aclanthology.org/D15-1202/)
   - [AQUA-RAT](https://aclanthology.org/P17-1015/)
   - [GSM8K](https://arxiv.org/abs/2110.14168)
   - [SVAMP](https://aclanthology.org/2021.naacl-main.168/)
### 2. **Commonsense Reasoning:**
   - [CommonsenseQA](https://aclanthology.org/N19-1421/)
   - [StrategyQA](https://aclanthology.org/2021.tacl-1.21/)
### 3. **Symbolic Reasoning:**
   - LastLetterConcatenation (constructed per protocol from [Wei et al., 2022])
   - CoinFlip (constructed as above)
### 4. **Other Logical Reasoning:**
   - DateUnderstanding (from BIG-bench)
   - TrackingShuffledObjects (from BIG-bench)

**See Table 7 in the Appendix and Section 4 for dataset details and answer formats.**

---

## 3. What LLM models were used?

A **wide variety of LLMs** (both proprietary and open-source), specifically:

- **Original GPT-3** (ada, babbage, curie, davinci; 0.3B to 175B parameters)
- **InstructGPT-3** (text-ada-001, text-babbage-001, text-curie-001, text-davinci-001, text-davinci-002)
- **PaLM** (Pathways Language Model): 8B, 62B, 540B parameters
- **Others:** GPT-2 (1.5B), GPT-Neo (2.7B), GPT-J (6B), T0 (11B), OPT (13B)

**Default** and primary model in most experiments: **InstructGPT-3 text-davinci-002**.

---

## 4. What are the prompting strategies discussed in this paper?

Strategies compared:

1. **Zero-shot**
   - Standard zero-shot prompting: just present the question and an answer marker (e.g., "The answer is ...").
2. **Zero-shot-CoT (Chain-of-Thought, the paper's proposal)**
   - Add a generic step-by-step reasoning trigger phrase (e.g., "Let's think step by step.") before the answer.
   - No task-specific samples, no hand-crafted examples required—single universal prompt across all tasks.
3. **Few-shot**
   - Classic few-shot in-context learning: provide several task-specific Q&A examples before the target question.
4. **Few-shot-CoT**
   - In-context learning with chain-of-thought: each example has a step-by-step reasoning trace, carefully crafted per task.
5. **Zero-plus-Few-shot-CoT**
   - Both few-shot-CoT exemplars and the Zero-shot-CoT trigger in the answer part.
---
Other prompt variants and ablations (Section 4.1 and Table 4) were also tested to examine robustness: different wording, misleading or irrelevant triggers, etc.

---

## 5. What specific prompts were used?

### The main Zero-shot-CoT "reasoning trigger" is:
- **"Let's think step by step."**

Other variants (see Table 4):
- "First,"
- "Let's think about this logically."
- "Let's solve this problem by splitting it into steps."
- "Let's be realistic and think step by step."
- "Let's think like a detective step by step."
- "Let’s think"

**Other (for comparison):**
- Misleading/irrelevant prompts such as "Don't think. Just feel.", "By the way, I found a good restaurant nearby.", "Abrakadabra!"

**See Table 4 and Table 13 (Appendix B) for full examples.**

#### For extracting the final answer (Step 2 in the two-step process):
- For math: "Therefore, the answer (arabic numerals) is"
- For multiple choice: "Therefore, among A through E, the answer is"
- For Yes/No: "Therefore, the answer (Yes or No) is"

**Full list in Table 10 (Appendix A.5).**

---

## 6. Where are the prompt strategies explained?

- The core **prompting strategy is explained in Section 3 ("Zero-shot Chain of Thought")**.
- Two-stage prompting process: See Section 3.1 ("Two-stage prompting") and Figure 2.
- Prompt variants are discussed in **Section 4.1 (see Table 4)** and further analyzed in **Appendix B (Table 13)**
- Detailed answer extraction prompts are in **Appendix A.5 (Tables 9 & 10)**.

---

## 7. What are the results of the prompting strategies and how successful or not were they?

### **Headline Results**
- **Zero-shot-CoT ("Let's think step by step.") drastically outperforms plain zero-shot prompting** on challenging reasoning tasks (especially multi-step arithmetic and symbolic problems).
- Example (Table 1):
  - **MultiArith:** Zero-shot: 17.7% → Zero-shot-CoT: 78.7%
  - **GSM8K:** Zero-shot: 10.4% → Zero-shot-CoT: 40.7%
- Also **significant improvements** for other tasks and models, like PaLM-540B (GSM8K: Zero-shot 12.5% → Zero-shot-CoT 43.0%).
- For simple arithmetic (not multi-step), gains are less significant (sometimes similar or slightly lower), indicating CoT is especially helpful for complex reasoning.
- For commonsense reasoning (CommonsenseQA), gains are limited or even absent, but qualitative analysis (Table 3) shows that Zero-shot-CoT reasoning traces are often plausible.

**In summary:** Zero-shot-CoT closes a large part of the gap to Few-shot-CoT, with no need for laborious example crafting.

- **See Table 2 for comparison across all prompting strategies.**

---

## 8. Why did one strategy give better results than another?

- **Zero-shot-CoT outperforms standard zero-shot because** its step-by-step reasoning cue ("Let's think step by step") encourages the LLM to explicitly unfold and articulate multi-step reasoning, making it less likely to shortcut or skip important deduction steps.
- Multi-step reasoning is not naturally elicited by the plain prompt; adding the chain-of-thought trigger leads the model to internally simulate more "deliberate" problem-solving.
- **Few-shot-CoT is even stronger,** as it not only provides the step-by-step reasoning signal, but also demonstrates the answer format and style, which the model can directly mimic.
- However, Few-shot-CoT is sensitive to the content and domain of exemplars; if examples are mismatched or in different formats, performance drops (see Table 5).
- **Zero-shot-CoT is more robust and general:** just a single phrase, broadly applicable.

---

## 9. All specifics of the researchers' prompt strategies:

### **A. The Two-Stage Prompting Pipeline (Section 3.1, Figure 2):**

**Step 1: Reasoning Extraction**
- Prompt: Question + "Let's think step by step."
  - E.g.:  
    Q: [Task-specific question]  
    A: Let's think step by step.
- Model generates **a reasoning trace** (multiple step explanation).

**Step 2: Answer Extraction**
- Feed: [The original prompt] + [Generated reasoning trace] + [answer trigger specific to task]
  - E.g.:  
    Q: ...  
    A: Let's think step by step. ... [reasoning trace]  
    Therefore, the answer (arabic numerals) is
- The model then outputs a final answer (parsed appropriately as number, letter, Yes/No, etc.—see Table 11 for cleansing methods).

### **B. Prompt Wording Variations (Robustness)**
- Explored many variations in the trigger phrase. (Table 4)
- Only "instructive" prompts work well (those that encourage stepwise reasoning).
- Misleading/irrelevant phrases can degrade or even worsen performance compared to plain zero-shot.

### **C. Answer Extraction Variants**
- Used answer triggers matched to task format: "Therefore, the answer is..." (see Table 9 & Table 10 for Zero-shot and Zero-shot-CoT answer extraction triggers).
- The appropriateness of prompt wording for answer extraction matters for optimal performance, but not as much as the reasoning trigger.

### **D. Model and Dataset Scope**
- Tested both with InstructGPT-3 & PaLM on all datasets; also included experiments with smaller and open models to test scaling law effects.
- Found that CoT benefits increase with model size (Figure 3).

---

## **Quick Reference Table – Prompting Strategies & Results**

| Setting                | Reasoning Prompt                | Examples Provided | Answer Extraction Prompt                  | MultiArith (%) | GSM8K (%) |
|------------------------|---------------------------------|-------------------|-------------------------------------------|---------------|-----------|
| Zero-shot              | *none* (just Q, marker)         | None              | "The answer is"                           | 17.7          | 10.4      |
| Zero-shot-CoT (Ours)   | "Let's think step by step."     | None              | "Therefore, the answer (arabic numerals) is" | 78.7        | 40.7      |
| Few-shot               | *none*                          | 8 per task        | Typically formatted as per example        | 33.8          | 15.6      |
| Few-shot-CoT           | Stepwise explanations per ex.   | 8 per task        | Step-by-step samples in context           | 93.0          | 48.7      |
| Zero+Few-shot-CoT      | "Let's think step by step." + ex| 8 per task        | Step-by-step samples + trigger            | 92.8          | 51.5      |

(See Table 2 for source)

---

### **In Summary:**
- Prompt: "Let's think step by step." (and similar instructive variants)
- Two steps: elicit reasoning, then specific answer.
- Universality: same prompt works across all tested tasks, no customization needed.
- Results: massive gains for challenging, multi-step reasoning tasks (especially in arithmetic/symbolic reasoning).
- Main insight: LLMs have untapped zero-shot reasoning powers—if you ask the right way!

---

**Let me know if you want sample prompts, code, or more detailed analyses on a particular experiment or dataset.**
