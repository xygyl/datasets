### 1. What is the point of this paper? What is it trying to explore? What makes it unique?
**Point & Exploration:**  
The paper aims to *comprehensively compare* traditional Static Application Security Testing (SAST) tools and Large Language Models (LLMs) for detecting software vulnerabilities at the *repository level* (repo-level) across Java, C, and Python projects. It explores their strengths and weaknesses, and whether combining them can yield better outcomes.

**What makes it unique:**
- **First comprehensive comparison** between SAST tools and LLMs for repo-level detection (as opposed to the usual function-level detection).
    > "To the best of our knowledge, we are the first to comprehensively compare the SAST tools with LLMs in three popular programming languages, Java, C, and Python."
- **Introduction of the repo-level vulnerability detection task** for LLMs, which is more aligned with real-world needs and SAST tool usage.
- **New and curated datasets** for Python, and for full repositories, rather than only individual functions.
    > "We have constructed a new repo-level Python vulnerability dataset and curated Java and C vulnerability datasets..."
- **Evaluate four diverse prompting/adaptation strategies** for LLMs and ensemble methods.
- **Practical insight:** Which method or combination is best depends on language, and how ensembles can mitigate shortcomings of each approach.

---

### 2. What dataset did they use?

**Datasets:**  
- **Three repo-level datasets:** one each for Java, C, and Python.
    - **Java:** Derived from [Li et al. 2023], curated to have real-world CVE-labeled repositories.
    - **C:** Derived from [Lipp et al. 2022], as above.
    - **Python:** Newly constructed by scraping NVD (National Vulnerability Database) for vulnerabilities in Python GitHub repositories.

- **Dataset stats (see Table 1):**
    - Java: 50 projects, 87 CVEs, ~4 million functions, 0.008% vulnerability ratio
    - C: 6 projects, 85 CVEs, ~1.18 million functions, 0.009% vulnerability ratio
    - Python: 78 projects, 98 CVEs, ~312,000 functions, 0.18% vulnerability ratio

- **All datasets provide**: For each CVE—CVE ID, CWE type, full source code, and function-level vulnerability localization.

---

### 3. What LLM models were used?
**12 Open-Source LLMs:**
- **Lightweight (<1B params; "fine-tuned"):**
    - CodeBERT (0.13B, Encoder)
    - GraphCodeBERT (0.13B, Encoder)
    - CodeT5 (0.22B, Encoder-Decoder)
    - UniXcoder (0.13B, Encoder-Decoder)
- **Large/Recent Code Models (mostly decoder):**
    - StarCoder (7B)
    - CodeLlama (7B)
    - DeepSeek-Coder (6.7B)
    - StarCoder2 (7B)
    - CodeQwen (7B)
    - Llama3 (8B)
- **Large/Recent General LLMs:**
    - Mistral (7B)
    - Phi3 (3.8B)

_(Release dates & details in Table 2)_

---

### 4. What are the prompting strategies discussed in this paper?
**Four adaptation/prompting strategies:**
- **Zero-shot prompting:** Give only an instruction and the code snippet, no examples.
- **Chain-of-thought (CoT) prompting:** Add “Let’s think step by step” for improved reasoning.
- **Few-shot prompting:** Provide a couple of labeled examples (1 vulnerable, 1 clean) as part of the prompt.
- **Fine-tuning:** Full fine-tuning for small models; parameter-efficient fine-tuning (LoRA) for large LLMs.

All of these are *applied for repo-level vulnerability detection*.

---

### 5. What specific prompts were used?
**Prompt format (Section 2.5.1):**
- The prompt for the task consists of three parts, sequentially:
    1. **Task Description:**  
        - "If the following code snippet has any vulnerabilities, output Yes; otherwise, output No."
    2. **Formatted Input:**  
        - The code is enclosed by two markers "//CodeStart" and "//CodeEnd".
    3. **Prediction Marker:**  
        - "//Detection", indicating where the LLM should provide its output.

**CoT Style:** Adds “Let’s think step by step” after the task description.

**Few-shot:** Adds two examples (input code + ground truth label for 1 vulnerable and 1 clean function) after the task description and before the target code snippet.

---

### 6. Where are the prompt strategies explained?
- **Main explanation:**  
    - **Section 2.5.1 (Prompt-based Methods)** – This describes prompt structure, zero/few-shot, and chain-of-thought strategies in detail.
    - **Section 3.3 (Implementation Details)** – Explains how prompts were concretely constructed from the training set for few-shot.

---

### 7. What are the results of the prompting strategies? How successful or not were they?

**Key Results:**
- **Fine-tuning leads to best results**—for both small and large models.
     > “Among different adaptations for large LLMs (≥1B), fine-tuning achieves the highest detection ratios with the fewest marked vulnerable functions.”
- **Zero-shot, Chain-of-Thought & Few-shot methods:**
    - Detect more vulnerabilities than SAST, **but at a high cost of false positives** ("marked" function ratio often 50%—70%+).
    - Few-shot often performs worse than fine-tuning, but sometimes better than Zero-shot/CoT depending on the model.
    - CoT sometimes provides higher detection ratio, but increases marked function ratio.

- **SAST tools:** Low detection rate but very low false positive/marked ratio.

- **LLMs:**  
    - Can reach up to 100% detection (for some languages/settings), but often mark 20–70%+ of functions, i.e., many false positives.
    - Combining LLMs reduces marked ratio at cost of some recall.

**Table 3** gives comprehensive breakdowns for all models, strategies, and languages.

**Summary Table (e.g. Java):**
- (Best, Fine-tuned Llama3): 87.5% det. (S2), 21.8% marked
- (Best, Zero-shot): up to 50% det. (S2), 64.6% marked (Mistral)
- (SAST best): e.g. Horusec: 0% det. (S2), 1.7% marked

---

### 8. Why did one strategy give better results than another?
- **Fine-tuning outperforms prompt-based methods** because it lets the model explicitly adapt to domain-specific vulnerabilities, learn the codebase features, and encode fine-grained distinctions between vulnerable/clean code. Prompts only tap into general pre-trained knowledge, which is more generic and less contextualized to the domain, leading to higher over-prediction and false positives.

    > "Fine-tuned large LLMs outperform prompted large LLMs. ... Fine-tuned lightweight LLMs detect more vulnerabilities but also mark more functions..."  
    > "We observed that, compared to fine-tuning, LLMs do not perform effectively under zero-shot, chain-of-thought, and few-shot prompting. This suggests that memorization may not be a significant factor; if the models had already been pre-trained on our evaluation datasets, they would have achieved significantly better performance with the prompts, even without fine-tuning on our training sets."

- **Zero-shot:**  
    Does not benefit from domain adaptation, so it tends to be conservative/high-marking.

- **CoT & Few-shot:**  
    These can offer improved reasoning or task alignment, but are limited by token context, prompt length, and model capacity, and don’t "internalize" dataset features as in fine-tuning.

- **Large models (vs small):**  
    Can memorize or generalize better; parameter-efficient fine-tuning helps them adapt to new tasks cost-effectively.

---

### 9. All specifics of the researcher's prompt strategies.

**Prompting Details (from Section 2.5.1 and Implementation Section):**
- **Prompt structure (used in all prompt-based methods):**
    - Task Description (as above)
    - Formatted Input: `//CodeStart` <code> `//CodeEnd`
    - Prediction Marker: `//Detection`

- **Chain-of-Thought (CoT):**  
    - Add "Let’s think step by step" immediately after the task description.

- **Few-shot:**  
    - After task description (and before the input code), add two full function code examples, one labeled "Yes" (vulnerable), one "No" (non-vulnerable).
    - Examples are *randomly sampled* from the training set.

- **Fine-tuning:**
    - For small models: Full model parameter fine-tuning.
    - For large models: LoRA parameter-efficient fine-tuning (r=8, α=16, 20 epochs, batch size 8), using balanced datasets (equal vulnerable/non-vulnerable functions, created by subsampling).

---

**In summary:**

- The paper's uniqueness lies in the scope (repo-level), language diversity, comprehensive comparison, and rigorous methodology, and introduces a practical task for future research.
- It uses new and community datasets, and evaluates 12 open-source LLMs of diverse architectures and sizes.
- Prompting strategies tried are zero-shot, chain-of-thought, few-shot, and fine-tuning, all with a fixed prompt structure and carefully described implementation specifics.

Let me know if you would like the discussion of prompt specifics illustrated as a sample input/output!
