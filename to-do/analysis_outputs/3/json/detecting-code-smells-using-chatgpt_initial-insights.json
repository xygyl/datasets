{
  "filename": "detecting-code-smells-using-chatgpt_initial-insights.pdf",
  "analysis": "Certainly! Here’s a detailed breakdown of the paper “Detecting Code Smells using ChatGPT: Initial Insights” based directly on your questions and the provided text.\n\n---\n\n## 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point & Exploration:**\n- The paper investigates the use of ChatGPT (specifically GPT-3.5 Turbo) to automatically detect code smells in Java projects. \n- Code smells are patterns in code that signal problems in design or implementation (“bad smells”), such as Blob, Data Class, Feature Envy, and Long Method.\n- The research compares how well ChatGPT can identify code smells under two different prompting strategies: a generic prompt (doesn't specify which smells to look for) and a specific (detailed) prompt (explicitly lists the smells).\n\n**Unique Aspects:**\n- This is one of the first studies to systematically evaluate LLMs for *code smell detection* (as opposed to more common tasks like code generation, summarization, or bug fixing).\n- It analyzes how prompt engineering—varying in specificity—impacts LLM performance for this software engineering task.\n- It employs a curated, industry-relevant dataset with severity annotations, providing a nuanced view on model effectiveness.\n\n---\n\n## 2. What dataset did they use?\n\n**Dataset:**\n- The study uses the **MLCQ dataset** ([9] in References).\n  - Contains 14,739 labeled instances of four code smells: Blob, Data Class, Feature Envy, Long Method.\n  - Each instance is annotated with a severity level (None, Minor, Major, Critical).\n  - For this study, only instances with severity Minor, Major, or Critical are used (None are discarded), totaling 3,291 instances: 974 Blob, 1,057 Data Class, 454 Feature Envy, 806 Long Method.\n\n---\n\n## 3. What LLM models were used?\n\n- **ChatGPT version 3.5-Turbo** (GPT-3.5-Turbo) was used in all experiments.\n\n---\n\n## 4. What are the prompting strategies discussed in this paper?\n\nTwo main prompting strategies:\n\n### 1. **Generic Prompt (\"Prompt #1\")**\n- Asks ChatGPT to check if the provided Java code contains any code smells without specifying which types.\n- Simulates a base scenario to see if the model can detect different types of smells *autonomously* (i.e., open-ended detection).\n\n### 2. **Detailed/Specific Prompt (\"Prompt #2\")**\n- Lists explicitly the target code smells (Blob, Data Class, Feature Envy, Long Method).\n- Asks ChatGPT to identify which of these occur in the code, but not to describe them.\n- This tests if explicit task instructions improve detection accuracy.\n\n---\n\n## 5. Where are the prompt strategies explained?\n\n- **Section 2.2 Prompts Definition** (and Figures 1 and 2 in the paper) detail the prompts.\n- Also summarised in **Section 3 Research Questions** and **Section 4 Results**.\n- Full texts of both prompt templates are given (verbatim) in the paper.\n\n---\n\n## 6. What are the results of the prompting strategies and how successful or not were they?\n\n**Summary: Prompt #2 (detailed) outperforms Prompt #1 (generic).**\n\n### Prompt #1 (Generic)\n- Generally poor performance.\n- Best F-measure: Long Method (0.41)\n- For Blob, ChatGPT failed to correctly identify any instance.\n- F-measure for Data Class as low as 0.11; Feature Envy 0.23.\n\n### Prompt #2 (Detailed)\n- Modest but significant improvement, especially for Data Class (F-measure jumps to 0.59, a 0.48 increase!).\n- For Blob, some improvement (F-measure 0.19).\n- Improvement is much less pronounced for Feature Envy; in some cases, performance slightly dropped.\n- For overall performance, the statistical test (McNemar’s) showed that ChatGPT is **2.54 times more likely to give a correct answer with Prompt #2**; absolute success rate increased by **16%**.\n\n### By Severity\n- ChatGPT is best at detecting *Critical* smells (F-measure up to 0.52 with Prompt #2) and worst at Minor.\n- Data Class and Long Method smells are detected more accurately than Blob or Feature Envy.\n\n---\n\n## 7. Why did one strategy give better results than another?\n\n- **Providing explicit, targeted information directs the model’s attention to relevant features in the code**, overcoming its general uncertainty in the open-ended scenario.\n- Prompt #2 \"focuses\" ChatGPT on concrete categories, increasing recall and precision, particularly for code smells that manifest via clear textual/code patterns (as in Data Class).\n- When unspecified (Prompt #1), ChatGPT tends to generalize and may overlook subtler or less prototypical cases.\n- Some smells, like Feature Envy, remain challenging regardless of prompt, likely due to their context-dependency or complexity (needing inter-class information not provided in isolated code chunks).\n- The biggest benefit is for \"simpler\" smells or those that can be recognized via prominent features (like fields in Data Classes).\n\n---\n\n## 8. All specifics of the researcher's prompt strategies\n\n**Prompt #1: Generic Prompt (see Figure 1)**\n- Instruction: \"I need to check if the Java code below contains code smells (aka bad smells). Could you please identify which smells occur in the following code? However, do not describe the smells, just list them.\"\n- Answer syntax: “YES I found bad smells. The bad smells are: (list, e.g., 1.Long method, 2.Feature envy…)” or “NO, I did not find any bad smell.”\n- [Followed by] The Java source code with the smell.\n\n**Purpose:** Test if ChatGPT can autonomously identify any of the four code smells without being told which ones to look for.\n\n---\n\n**Prompt #2: Detailed Prompt (see Figure 2)**\n- Instruction: \"The list below presents common code smells (aka bad smells). I need to check if the Java code provided at the end of the input contains at least one of them. \\n *Blob \\n *DataClass \\n *FeatureEnvy \\n *LongMethod\"\n- Then the same answer instructions as above.\n- [Followed by] The Java source code.\n\n**Purpose:** See if explicitly mentioning the target smells improves detection performance.\n\n---\n\n**Prompt Development:**  \n- Both prompts were iteratively refined after pilot tests to ensure clarity and consistency.\n- The answer format was constrained for easier automatic parsing of results.\n\n**Processing:**  \n- ChatGPT answers were parsed to extract identified smells; if output deviated from the expected format, manual correction happened (~27 instances).\n- Both prompts were sent to ChatGPT via automated Python scripts.\n\n---\n\n## In summary/at a glance:\n\n**- The paper tests ChatGPT (GPT-3.5-Turbo) for automatically detecting code smells in Java, focusing on how prompt specificity affects performance.**\n**- Dataset: MLCQ, four smells (Blob, Data Class, Feature Envy, Long Method), 3 severity levels, 3,291 instances.**\n**- Two prompt strategies: generic (no smell names) and detailed (lists the smells).**\n**- ChatGPT is much more accurate (up to 2.54x) with detailed prompts, especially for Data Class smells.**\n**- Smell/Severity matters: best performance for critical Data Class and Long Method, poorest for Feature Envy and Blob regardless of strategy.**\n**- Prompt specificity helps, but not a panacea for more complex/conceptual smells.**\n**- Prompts are fully laid out in Section 2.2 with all required formatting.**\n\n---\n\n**If you want further breakdowns by table or metric, or want the literal text of the prompts as used in the study, let me know!**"
}