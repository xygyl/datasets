{
  "filename": "prompt-enhanced-software-vulnerability-detection-using-chatgpt.pdf",
  "analysis": "Certainly! Here is a structured answer to your questions, based directly on the paper text:\n\n---\n\n### 1. What is the point of this paper? What is it trying to explore? What makes it unique?\n\n**Point and Purpose:**  \nThe paper investigates whether and how Large Language Models (LLMs), specifically ChatGPT-4, can be enhanced to automatically detect software vulnerabilities. It aims to go beyond previous, naive uses of LLMs (simple Q&A) by introducing carefully constructed prompt engineering—including auxiliary (structural/sequential) information and chain-of-thought reasoning—to see if such designs improve detection performance.\n\n**Uniqueness:**  \n- It is the first work to systematically design and compare various prompt strategies tailored for vulnerability detection tasks, rather than generic prompting.\n- It directly integrates classic auxiliary code information (API call sequences and Data Flow Graphs, or DFGs) as context in prompts.\n- The study also leverages ChatGPT’s multi-round/dialogue (\"chain-of-thought\") abilities to potentially mimic the stepwise reasoning humans use to verify software vulnerabilities.\n\n---\n\n### 2. What dataset did they use?\n\nThey use two datasets:\n- **Java dataset:** Collected from the Software Assurance Reference Dataset (SARD), published by NIST. Contains 1,171 vulnerable and 917 non-vulnerable Java functions, focusing on the top-50 vulnerability types.\n- **C/C++ dataset:** Derived from the National Vulnerability Database (NVD), focusing on .cor.cpp files associated with vulnerabilities. Contains 1,015 vulnerable and 922 non-vulnerable functions.\n\n---\n\n### 3. What LLM models were used?\n\n- **ChatGPT-4**: The main focus; experiments and all prompt engineering strategies use ChatGPT-4.\n- **Baselines for comparison:** CFGNN (a graph neural network model for bugs) and Bugram (an n-gram language model for code).\n  - But *ChatGPT-4 is the only LLM model* tested in prompt engineering and main experiments.\n\n---\n\n### 4. What are the prompting strategies discussed in this paper?\n\nSeveral prompting strategies are explored:\n- **Basic Prompting**: A straightforward question—\"Is the following program buggy? Yes/No\".\n- **Role-based Prompting:** Asks ChatGPT to act as a \"vulnerability detection system\" (adds system role-context).\n- **Reverse-question Prompting:** Swaps the word \"buggy\" for \"correct\" to test for keyword bias in answers.\n- **Auxiliary Information Prompting:** Adds code-level info to the prompt:\n    - **API Call Sequences**: A sequence of external calls made by code.\n    - **Data Flow Graphs (DFG):** Paths of variable value assignment and use.\n- **Chain-of-Thought Prompting:** Multi-round (multi-step) dialogue in which the model first summarizes the code's intent/functionality, then is asked if the code is buggy, possibly with auxiliary information.\n- **Prompt Positioning:** Testing whether auxiliary info works better before or after the code body in the prompt.\n- **Combining Code Summaries:** Incorporating LLM-generated natural-language summaries of code as context for the vulnerability detection question.\n\n---\n\n### 5. Where are the prompt strategies explained?\n\n- **Section 4 (Prompt Design):** Main explanations; see subsections:\n    - 4.1: Basic Prompting\n    - 4.2: Prompting with Auxiliary Information (including API call, DFG; 4.2.1, 4.2.2)\n    - 4.3: Chain-of-Thought Prompting\n- **Section 5.5 (Impact of Position):** Details on placing auxiliary info before/after code in the prompt.\n\nPrompt templates are explicitly listed (pseudocode), showing what is put in each prompt.\n\n---\n\n### 6. What are the results of the prompting strategies and how successful or not were they?\n\n**Main Results:**\n- **ChatGPT-4 outperforms classic baselines** (CFGNN, Bugram) on both Java and C/C++ datasets when using tailored prompts.\n- **Basic Prompting**: Gives high recall on vulnerable samples (ChatGPT tends to predict \"vulnerable\" often), but is biased by prompt keywords.\n- **Role-Based Prompting**: Marginal improvement over basic; more so in Java than C/C++.\n- **Auxiliary Information (API call/DFG):**\n    - Improves accuracy in some cases, especially when incorporating API calls for Java.\n    - The effectiveness is programming-language-specific: API call info helps Java, DFG helps C/C++.\n    - Order matters: API calls work best before code; DFG works best after code in the prompt.\n- **Chain-of-Thought Prompting**:\n    - Greatly improves detection for C/C++, but can harm performance on Java.\n    - Confirms that stepwise reasoning is useful, but language-specific.\n- **Code summaries**: Sometimes help (C/C++), sometimes not (Java).\n- **Vulnerability-type Results**:\n    - Certain boundary/grammar/logical vulnerabilities (e.g., off-by-one, logic bombs, always-false-expressions) are detected with near-100% accuracy.\n    - Others (e.g., missing salt, weak hash) are poorly detected; these types require more contextual or domain-specific knowledge.\n\n---\n\n### 7. Why did one strategy give better results than another?\n\n- **API Call auxiliary info:** Java code often relies on clear library/API patterns, so providing that info aligns with how Java vulnerabilities often occur and are fixed. In C/C++, vulnerabilities are more about data/state manipulation (buffer overflows, etc.), so DFG is more useful.\n- **Prompt positioning:** Placing info in the spot that best primes the LLM’s attention helps—leading context is often more strongly attended to.\n- **Chain-of-Thought:** Helps C/C++ analysis (which is more \"reasoning-based\" for vulnerabilities), but for Java, the extra step may introduce confusion or irrelevant distractions.\n- **Role Prompting:** Frames the task more explicitly for the LLM, directing its reply style (especially for languages/contexts where the model is less sure).\n\n---\n\n### 8. All specifics of the researchers’ prompt strategies:\n\n#### Basic Prompt:\n- \"**Is the following program buggy? Please answer Yes or No. [CODE]**\"\n\n#### Role-based Basic Prompt:\n- \"**I want you to act as a vulnerability detection system. My first request is 'Is the following program buggy?' Please answer Yes or No. [CODE]**\"\n\n#### Reverse-question Prompt (Test for Keyword Bias):\n- **\"Is the following program correct?\"** (substitutes \"buggy\" with \"correct\")\n\n#### Prompting with Auxiliary Information:\n- **API Call Sequence Prompt:**\n    - Format: \"**I want you to act as a vulnerability detection system. I will provide you with the original program and the API call sequence, and you will act upon them. [APIdescription]. Is the following program buggy? [CODE]**\"\n    - [APIdescription]: Natural language list of API/function calls extracted from code.\n- **Data Flow Graph Prompt:**\n    - Format: \"**I want you to act as a vulnerability detection system. I will provide you with the original program and the data flow information, and you will act upon them. Is the following program buggy? [CODE]. [DFdescription]**\"\n    - [DFdescription]: Natural language explanation of variable assignments/flows.\n\n#### Chain-of-Thought Prompting:\n- **Step 1:** \"**Please describe the intent of the given code. [CODE]**\"\n- **Step 2:** \"**I want you to act as a vulnerability detection system. Is the above program buggy? Please answer Yes or No.**\"\n- *Also tested with API/DFG auxiliary info added in Step 2*.\n\n#### Prompt Positioning:\n- [APIdescription] and [DFdescription] tested both before/after [CODE] in the prompt.\n\n#### Prompt Naming:\n- P_b: Basic\n- P_r-b: Role-based\n- P_r-r-b: Reverse-question role-based\n- P_r-a-b: Role-based, API before code\n- P_r-b-d: Role-based, DFG after code\n- ... and their ordering variants (see Table 5 in text)\n\n#### Example APIdescription Construction:\n- \"**The program first calls KerberosPrincipal.new, then calls String.toCharArray, then calls KerberosKey.new, then calls KerberosKey.toString, and finally calls IO.writeLine.**\"\n\n#### Example DFdescription Construction:\n- \"**The data value of the variable data at the 17th token comes from the variable data at the 11th token or the variable data at the 14th token...etc.**\"\n\n#### Human Evaluation and Further Testing:\n- Generated answers to prompts were rated for quality by human annotators for code explanation and vulnerability explanation tasks.\n\n---\n\nIf you need the tables extracted or more example prompts, I can supply that too! Let me know if you want those."
}