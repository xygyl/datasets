{
  "filename": "interleaving-static-analysis-and-llm-prompting.pdf",
  "analysis": "Certainly! Here is a detailed answer to each of your questions about the paper \"Interleaving Static Analysis and LLM Prompting\":\n\n---\n\n**1. What is the point of this paper? What is it trying to explore? What makes it unique?**\n\n**Point:** The paper introduces a new approach that combines static program analysis with Large Language Model (LLM) prompting, interleaving calls between a static analyzer (EESI) and an LLM during the process of inferring *error specifications* in C system code. Error specifications define what values functions return when errors occur, which is important for program understanding and bug detection.\n\n**Exploration:** It explores how program analysis can be improved by using LLMs to provide information when static analysis alone is insufficient, especially for scenarios involving third-party functions or incomplete program facts.\n\n**Uniqueness:** The main novelty is the *interleaving*—static analysis and LLM querying aren't simply run separately then combined; rather, intermediate results from analysis are used to construct LLM prompts and vice versa, creating a feedback loop. The LLM is only called when the static analyzer would otherwise fail or be imprecise. Unlike prior work, LLM results are fed back into the analyzer, improving its further analysis.\n\n---\n\n**2. What dataset did they use?**\n\nThey used six real-world open source C programs that cover a range of error-handling idioms:\n- **Apache HTTPD**\n- **LittleFS**\n- **MbedTLS**\n- **Netdata**\n- **Pidgin OTRv4**\n- **zlib**\n\nDetails like lines of code and domain knowledge provided are shown in Tables 1 and 2 of the paper. Initial domain knowledge (such as initial error specifications for certain functions, error codes, success codes, and error-only functions) is also supplied per benchmark.\n\n---\n\n**3. What LLM models were used?**\n\n- The experiments used **OpenAI's GPT-4** as the LLM queried in their workflow ([Section 5, Implementation Details] in the paper).\n\n---\n\n**4. What are the prompting strategies discussed in this paper?**\n\nThere are two main prompting strategies:\n  - **queryLLMThirdParty**: Used when dealing with *third-party functions* (whose code is not accessible to static analysis). The prompt includes all currently learned error specifications in the program as context, and asks the LLM about the specific third-party function.\n  - **queryLLMAnalysis**: Used when static analysis for a function is *inconclusive* (result is ⊥, unknown). The prompt includes the current error specifications of *called functions* (within the analyzed function), plus other context, and asks the LLM to infer the error specification for this particular function.\n\nThe prompts themselves are carefully constructed and include:\n  - **CommonContext**: Explanation of the error specification inference problem, the abstract domain used by EESI, idioms/rules about error codes, and chain-of-thought examples to guide the LLM.\n  - **FunctionContext**: Relevant error specifications from the call graph or program facts (either all known error specs for third-party, or only those called inside the function for regular analysis).\n  - **Question**: Either the function name (third-party) or the function's source code (analysis), and a request for the error specification.\n\nAdditionally, chain-of-thought prompting, self-consistency (multiple runs), and example formatting are employed to stabilize LLM output.\n\n---\n\n**5. Where are the prompt strategies explained?**\n\nPrompt construction is detailed in **Section 4.1 (Building Prompts)** of the paper. The distinction between `queryLLMThirdParty` and `queryLLMAnalysis`, and their use in the proposed algorithm, is explained in **Section 4.2 (Error Specification Inference)** and in the pseudocode (Algorithm 1).\n\n---\n\n**6. What are the results of the prompting strategies and how successful or not were they?**\n\nSee Table 4 and Figure 4.\n\n- **queryLLMThirdParty** increased recall by about 29% on average over baseline EESI, especially benefiting benchmarks heavily reliant on third-party code (e.g., Netdata, Pidgin OTRv4), with similar precision to EESI.\n- **queryLLMAnalysis** gave a much higher recall increase (average +59.88%) and recall (70.26%) over EESI, with a modest drop or maintenance in precision.\n- **Combined/Interleaving approach** (using both strategies) achieved the best results:\n    - **Recall**: rose from EESI's 52.55% to 77.83% (average)\n    - **F1-score**: improved from 0.612 to 0.804\n    - **Precision**: remained high (slight decrease from 86.67% to 85.12%)\n\nSuccess varied across benchmarks, most notably improving error specification inference in projects where static analysis had limited knowledge (e.g., due to third-party code).\n\n---\n\n**7. Why did one strategy give better results than another?**\n\n- **queryLLMThirdParty** helps mostly when missing knowledge is due to third-party or library functions, since only the names and previously inferred error specs can be used. It cannot reason about internal function logic, so the gain is limited to inferring specs for \"black box\" APIs.\n- **queryLLMAnalysis** is more impactful because it enables the LLM to directly analyze the source code of functions where static analysis failed, and it supplies context/specs for any called functions. The LLM can thus reason over code and context, supplying previously missing insights that static analysis struggled with.\n- **Combined interleaving** works best—using the right method at the right time and feeding LLM results back into static analysis, so earlier discoveries enhance later ones.\n\n---\n\n**8. All specifics of the researcher's prompt strategies.**\n\n**Prompt Structure for Each Query (see Section 4.1):**\n- **CommonContext** (for all prompts):\n    - General explanation of error specification inference\n    - Description of the \"abstract domain\" (the mathematical model EESI uses)\n    - List of current error codes, success codes, error-only functions (domain knowledge)\n    - List of idioms/rules about how errors/specs generally work (from domain knowledge)\n    - Multiple chain-of-thought examples: function definitions, their error specifications, and an explanation for clarity. This both demonstrates the expected answer format and provides LLM with examples to reason by analogy.\n\n- **FunctionContext**:\n    - For **queryLLMThirdParty**: include all known error specifications from the program facts, as context, to help the LLM infer the likely error spec for a given third-party function name.\n    - For **queryLLMAnalysis**: include only the error specs of functions called within the function under analysis, to help with local reasoning.\n\n- **Question**:\n    - For **queryLLMThirdParty**: \"What is the error specification for function XYZ?\"\n    - For **queryLLMAnalysis**: Provide full function source code, and \"What is the error specification for this function?\"\n\n**Additional details and stabilization strategies:**\n- LLM is called *only if* static analysis is inconclusive (⊥) or the function is third-party.\n- Every LLM response is parsed to check format; if the output doesn't fit expectations, it is discarded (marked as ⊥/unknown).\n- To reduce hallucinations:\n    - They query the LLM multiple times and use self-consistency to select the most common result.\n    - Ensure that LLM’s output matches the chain-of-thought reasoning it provides.\n    - Don't accept error specs that (a) include known success values or (b) are improper supersets of the function’s return range, to avoid semantic contradictions.\n- Outputs from LLM are fed back as new program facts for static analysis to use in subsequent inference; this creates an iterative improvement.\n\n**Algorithmic Workflow:**\n- Bottom-up over the program's call graph.\n- For each function:\n    - If third-party: queryLLMThirdParty\n    - Else: Run EESI analysis; if inconclusive, queryLLMAnalysis\n- Update learned error specs after each function.\n\nThese strategies allow the approach to leverage the best of both worlds: static analysis's confidence in some scenarios, and LLM's flexible reasoning in the presence of incomplete information.\n\n---\n\n**Summary Table**\n\n|                    | EESI Only         | LLM ThirdParty      | LLM Analysis           | Combined Interleaved Approach   |\n|--------------------|-------------------|---------------------|------------------------|---------------------------------|\n| Precision (avg)    | 86.7%             | ~85%                | ~84.7%                 | 85.1%                           |\n| Recall (avg)       | 52.5%             | 62.2%               | 70.3%                  | 77.8%                           |\n| F1-score (avg)     | 0.612             | 0.68                | 0.74                   | 0.804                           |\n\nThe combined, interleaved approach is most successful, balancing high recall and high precision, and thus the best F1, for error specification inference in C code.\n\n---\n\nIf you want expanded details on any subpoint, just ask!"
}