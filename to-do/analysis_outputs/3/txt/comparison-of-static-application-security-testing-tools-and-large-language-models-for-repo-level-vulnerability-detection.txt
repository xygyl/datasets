### 1. What is the point of this paper? What is it trying to explore? What makes it unique?

**Point & Exploration:**
- The paper aims to **comprehensively compare Static Application Security Testing (SAST) tools and Large Language Models (LLMs)** for software vulnerability detection at the repository (repo) level.
- It focuses on **repo-level vulnerability detection** (i.e., identifying vulnerable functions anywhere in an entire code repository), rather than just at function-level granularity.
- The study evaluates **15 SAST tools and 12 LLMs** across three programming languages: Java, C, and Python, using newly curated datasets of real-world vulnerabilities.

**Uniqueness:**
- **First to directly compare SAST tools with LLMs** on repo-level vulnerability detection for multiple, real-world datasets in three languages (Java, C, Python), including constructing a new Python repo-level vulnerability dataset.
- First to **apply and systematically evaluate multiple LLM prompting/adaptation strategies** (zero-shot, chain-of-thought, few-shot, fine-tuned, PEFT) in this context.
- The study develops a **benchmark and evaluation protocol** for realistic, practical repo-level vulnerability detection, as SAST tools work at this scale.
- It examines the **synergy of ensemble approaches** (combining SAST tools or LLMs) to mitigate the weaknesses of each family of tools.

**Source:** see Abstract, “Contributions”, and “Introduction” sections, and Section 2 Study Design.

---

### 2. What dataset did they use?

**Datasets Used:**
- **Java Vulnerability Dataset:** Sourced and curated from the dataset of Li et al. [49], extended by manually linking CVEs/project/version info to real GitHub repos and extracting function-level code snippets. 50 projects, 87 CVEs, nearly 4 million functions (~0.008% vulnerable).
- **C Vulnerability Dataset:** Sourced and curated from Lipp et al. [51] with similar process; 6 projects, 85 CVEs, over 1.1 million functions (~0.009% vulnerable).
- **Python Vulnerability Dataset:** Newly constructed for this work; collected all NVD JSON entries (2002–2023), filtered to GitHub Python projects with accessible source, verified fixing commits, labeled vulnerable functions by lines changed in fixes. 78 projects, 98 CVEs, >300K functions (~0.18% vulnerable).

**Source:** See "2.2 Vulnerability Datasets," Table 1, and details under PythonVulnerabilityDataset.

---

### 3. What LLM models were used?

**LLMs Studied (see Table 2):**
- **Lightweight (<1B params):** CodeBERT (0.13B), GraphCodeBERT (0.13B), CodeT5 (0.22B), UniXcoder (0.13B)
- **Large (≥1B params):**
    - **Code models:** StarCoder (7B), DeepSeek-Coder (6.7B), CodeLlama (7B), StarCoder2 (7B), CodeQwen (7B)
    - **General models:** Mistral (7B), Llama3 (8B), Phi3 (3.8B)

**All are open-source.** (Commercial LLMs like OpenAI models were evaluated in a small-scale baseline, see Section 5.1, but not in the main experiments.)

**Source:** Table 2, Section "2.4 Studied LLMs."

---

### 4. What are the prompting strategies discussed in this paper?

**Prompting/LLM Adaptation Strategies:**
1. **Zero-shot Prompting:** Directly ask the model if a given code snippet is vulnerable, using a task description and code delimiters. No labeled data required.
2. **Chain-of-Thought (CoT) Prompting:** Add “Let’s think step by step” to the prompt to encourage reasoning.
3. **Few-shot Prompting:** Provide a few labeled examples (one vulnerable, one clean) before the query.
4. **Fine-tuning:** For smaller LLMs, standard full-parameter fine-tuning; for large models, parameter-efficient fine-tuning via LoRA.
5. **PEFT (Parameter-Efficient Fine-Tuning):** Used LoRA for models >1B parameters.
6. **Ensemble/voting strategies** in combining LLMs: threshold-based agreement (e.g., mark as vulnerable if >50%, >2/3, or >80% of LLMs agree).

**Source:** Section 2.5, Table 2, "Prompt-based Methods", and Ensemble discussion in 4.3.

---

### 5. Where are the prompt strategies explained?

**Locations in the Text:**
- **Prompting methods:** Section 2.5.1 “Prompt-based Methods”. Detailed explanations start under "We studied three popular prompt-based methods..."
    - Concrete formats for zero-shot, CoT, and few-shot.
- **Fine-tuning (including PEFT):** Section 2.5.2.
- Used in experiments as detailed in 3.3 “Implementation Details” for how data and prompts were formatted and how models were trained/evaluated.

---

### 6. What are the results of the prompting strategies and how successful or not were they?

**Overall Findings:**
- **Fine-tuning (especially for large models):** Achieved the best balance in high vulnerability detection **and** lower marked function ratio ("false positive" proxy). Example: in Java, fine-tuned large LLMs detect 73.4% (S1) and 51.6% (S2) of vulnerabilities, marking only 19% of functions as vulnerable.
- **Prompt-based strategies (zero-shot, CoT, few-shot):**
   - Generally, **lower detection rates** and **higher marked function ratios** than fine-tuned models.
   - Large variance by model and prompt; sometimes high detection, but always at the price of even more false positives (marked function ratio often 40–77%).
- **SAST vs LLMs:**
   - SAST: low detection rates, low false positives.
   - LLMs (with any prompt strategy): high detection rates, high false positives.
- **Ensembling improves trade-offs:** Combining LLMs using majority/vote schemes dramatically reduces marked function ratio, at some cost to detection rates.

**Specifics:**
- Table 3: Rows beginning "Zero-shot", "Few-shot", "CoT" show per-model results across three benchmarks.
- Figure 3 & 5-7: Visualize trade-offs for each method per language.

---

### 7. Why did one strategy give better results than another?

**Explanation (from results and discussion):**
- **Fine-tuning adapts the LLM to the specific vulnerability detection task,** letting it learn domain/class boundaries more precisely. This leads to higher sensitivity for vulnerabilities *and* greater discrimination, thus fewer false positives than mere prompting.
    - For large models, PEFT (LoRA) allows such task adaptation efficiently.
- **Prompt-based methods** rely on whatever the model already "knows": these work best if the pretraining distribution closely matches the eval set/task. However, they often "over-predict" (high marked function ratio) because they lack task-specific calibration and err on the side of marking many functions as dangerous.
- **Chain-of-thought can sometimes improve reasoning**, but wasn't a clear winner.
- **Few-shot may help slightly for some models,** but gains are less than full fine-tuning.
- **Class imbalance matters:** Without fine-tuning and explicit negative examples, models tend to label too many functions as suspicious due to the extreme rarity of actual vulnerabilities.

**Source:** Section 4.1.3 “Comparisons Among LLMs” and discussion in 5.2.

---

### 8. All specifics of the researcher's prompt strategies.

**Prompt Format Used for All LLMs:**
- **Task Description:** “If the following code snippet has any vulnerabilities, output Yes; otherwise, output No;”
- **Formatted Input:** Code is enclosed between two markers “//CodeStart” and “//CodeEnd”
- **Prediction Marker:** “//Detection” indicates where the model should output its answer.

**Prompting Strategies:**
- **Zero-shot:** Provide the prompt and the code—no training, no examples.
- **Chain-of-thought:** **Add “Let's think step by step.”** after the task description, to stimulate more elaborate output and potential reasoning.
- **Few-shot:** After the standard prompt, include *two example* code snippets (one labeled vulnerable, one clean), then the query.
    - These are picked randomly from the (training) data.
    - Intended to ‘calibrate’ response style and format.
- **Ensembling (in Section 4.3):** When combining LLMs, label a function as vulnerable if
    - C1: More than 50% of fine-tuned LLMs agree
    - C2: More than 2/3 agree
    - C3: More than 80% agree

**Implementation Details:**
- **All LLMs (regardless of size) were prompted at function granularity** (repositories split to functions due to input length limits).
- **Fine-tuning:** Used a balanced set (equal positives/negatives, randomly sampled from training data), 20 epochs, batch size 8, best snapshot by validation. For large models, LoRA (r=8, alpha=16) was used for parameter-efficient adaptation.
- **Evaluation:** Vulnerability considered detected if at least one vulnerable function in the repo is predicted (“Scenario 1”), or all are detected (“Scenario 2”). “Marked function ratio” used as false positive proxy.

**Sources:** Section 2.5.1 for prompting details, 2.5.2 for fine-tuning/PEFT, 3.3–3.4 for implementation, ensemble details in 4.3 and corresponding Figures.

---

**If you need a sample prompt verbatim as used with an LLM in this study, here is a template:**

```
If the following code snippet has any vulnerabilities, output Yes; otherwise, output No.
//CodeStart
[PASTE FUNCTION SOURCE CODE HERE]
//CodeEnd
//Detection
```
- For CoT: After the first line, add “Let's think step by step.”
- For few-shot: Add two [//CodeStart ... //CodeEnd //Detection: Yes/No] examples (one for each class) before the test query.

---

### **Summary Table**

| Strategy                | Description                                                                           | How Used in Study                                            | Effectiveness                                                      |
|-------------------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------------------------------------|
| Zero-shot               | Prompt includes task description, code, & marker. No examples/training.               | All LLMs, all languages                                      | Medium-high detection, high false positives                        |
| Chain-of-thought (CoT)  | As zero-shot, but adds "Let's think step by step."                                    | All LLMs, all languages                                      | Small improvements in some cases, still high false positives        |
| Few-shot                | Adds 2 labeled examples (one positive, one negative) before query.                    | All LLMs, all languages                                      | Marginal improvement; not as strong as fine-tuning                  |
| Full Fine-tuning        | Task-specific parameter update (all weights) on balanced data.                        | Lightweight LLMs                                             | Best overall: high detection, moderate false positives              |
| PEFT (LoRA)             | Parameter-efficient fine-tuning on large LLMs.                                        | Large LLMs                                                   | Nearly as good as full FT; best trade-off for large models          |
| Ensemble (Voting)       | Combines predictions from multiple LLMs: function marked if enough models agree.      | All fine-tuned LLMs. Different thresholds (majority, 2/3, 80%)| Dramatic reduction in false positives at some cost to sensitivity.  |

---

If you want a question-by-question mapping to paper locations for reference or need further code/prompt details, let me know!
