## 1. What is the point of this paper? What is it trying to explore? What makes it unique?
**Point & Exploration:**
- The paper rigorously evaluates ChatGPT's capability to assist in six real-world tasks along the end-to-end vulnerability management process within software engineering.
- The main research questions (RQ1–RQ3) are (Section 1):
  - RQ1: Can ChatGPT perform as well as state-of-the-art (SOTA) approaches on vulnerability management tasks?
  - RQ2: How do prompt engineering methods impact ChatGPT’s performance?
  - RQ3: What are promising future directions to further improve ChatGPT’s performance on these tasks?
- It uniquely benchmarks ChatGPT both against specialized SOTA approaches for each task and across different prompt engineering strategies and LLM versions (gpt-3.5, gpt-4), systematically highlighting strengths and weaknesses.

**Uniqueness:**
- It’s the first large-scale, comprehensive, end-to-end study of ChatGPT for multiple steps in practical vulnerability management (not just code understanding/repair, but including bug summary, severity, patch correctness, etc.).
- It examines not just models and datasets, but also how prompting strategies (manual, heuristic, with/without expertise, and in-context demonstrations) impact effectiveness, providing “guidance for prompt design” as a scientific contribution.
- The study also explores when and why LLMs like ChatGPT fail, and what kind of prompt engineering can close the gap with traditional specialized tools.

## 2. What dataset did they use?
- The evaluation uses datasets drawn from the SOTA approaches for **each of the six evaluation tasks** (see Table 1 and §3.2), strictly splitting training and test sets per each task paper's rules to ensure fairness.
- **Aggregate Summary:** The combined dataset has **70,346 samples** and **~19.4M tokens** across six tasks: bug report summarization, security bug report identification, vulnerability severity evaluation, vulnerability repair, patch correctness assessment, and stable patch classification.
- Each dataset's original source is listed in §2.1 and Table 1, e.g., iTAPE [18] for bug summaries (33k samples), DKG [57] for bug report identification (~23k), DiffCVSS [48] for severity (1.6k), etc.
- Some data (such as for vulnerability repair) are hand-crafted CVEs plus real-world bugs as released in previous work [37].

## 3. What LLM models were used?
- **OpenAI GPT-3.5-turbo (gpt-3.5-turbo-0301)**
  - Used for initial prompt testing, probe-test pipeline, and most practical evaluations because of cost and speed.
- **OpenAI GPT-4 (gpt-4-0314)**
  - Used for main large-scale test set evaluations for final reporting.
- Both evaluated via OpenAI’s API (see §3.1, footnote 1).
- For the vulnerability repair task, compared to group of LLMs collectively termed 'LLMset' in [37], which includes Codex models (by OpenAI) and AI21 Jurassic-1 models.

## 4. What are the prompting strategies discussed in this paper?
**Prompt Strategies (see Table 2, §2.2, §3.3):**
Six main templates, each capturing a different way to structure the query/prompt for ChatGPT:

- **0-shot**: Only the task description and input, no demonstrations.
- **1-shot**: Task description, followed by a single example before the input.
- **Few-shot**: Task description + several (e.g., four) randomly-selected demonstration examples before the input.
- **General-Info**: Uses system instructions to assign ChatGPT a task-related role, includes reinforcement, task confirmation, positive feedback, and "let’s think step-by-step" skill.
- **Expertise**: Builds on General-info, but also explicitly adds expert, domain-specific knowledge to the prompt, e.g., about what counts as a security bug.
- **Self-Heuristic**: Guides ChatGPT to extract 'expertise' or rules from demonstrations itself via meta-prompting, then uses this self-generated knowledge in the formal task prompt.

These strategies are systematically compared on each task.

## 5. Where are the prompt strategies explained?
- **Table 2** (Prompt Strategy Templates): Overview, names, and descriptions of each prompt type.
- **Section 3.3 (Prompt Design and Implementation)**: Detailed methodology for each template, how demonstration examples are selected, distinction between general-info and expertise, and the process of producing the self-heuristic prompt.
- **Table 12 & Table 13** (Appendix A): Show example prompt constructions and the "skills" used (e.g., role, reinforce, zero-CoT).
- **Examples**: Figure 3 demonstrates the expert prompt; Table 13 in the Appendix provides template-specific instantiations.

## 6. What are the results of the prompting strategies, and how successful or not were they?
**Summary of Results by Task and Prompt (see Section 4):**

- **Bug Report Summarization:**  
  - Even 0-shot ChatGPT outperforms SOTA iTAPE (Table 3).
  - More complex prompts (general-info, expertise, self-heuristic) did *not* always help: simply asking for a summary (0/few-shot) was best.
- **Security Bug Report Identification:**  
  - Expertise prompt significantly improves performance (precision/recall/F1) over basic prompting.
  - Recall benefits from 1/few-shot, but precision can decrease (Table 4).
  - gpt-4 further improves recall, but sometimes lowers precision (more aggressive at flagging security bugs).
- **Vulnerability Severity Evaluation:**
  - Performance is poor in 0/1/few-shot modes, only strong when using self-heuristic (where ChatGPT is itself prompted to summarize "rules" for severity based on examples) (Table 5).
  - gpt-4 + self-heuristic achieves scores close to SOTA.
- **Vulnerability Repair:**  
  - Expertise prompt and gpt-4 model achieve highest proportion of correct repairs (Table 6, Table 7).
  - Providing more bug fixes as examples (few-shot) is not that helpful.
- **Patch Correctness Assessment:**  
  - Self-heuristic and expertise prompts improve -recall and overall F1, especially in gpt-4.
  - Too much info (e.g., overloading with patch description and code) can *confuse* the model, which begins to answer whether patch matches description rather than whether it is correct. (Tables 8–10)
- **Stable Patch Classification:**  
  - Expertise prompt outperforms 0/1/few-shot in accuracy and recall.
  - gpt-4 outperforms gpt-3.5 for this with the same prompt, but still a bit behind SOTA PatchNet.

**Success/Failure:**
- In straightforward NLP-like tasks (e.g., summarization), LLMs excel on 0/regular shot prompts.
- In highly domain-specific or logic-heavy tasks (severity, repair, patch correctness), **prompts with explicit expertise (manual or self-heuristic) are much more successful**.
- Random demos for few-shot are not reliably helpful in complex tasks—ChatGPT needs explicit domain logic.
- Overloading the prompt with too much or irrelevant info can backfire.

## 7. Why did one strategy give better results than another?
**Discussion of Why (see Section 4, summaries of each task):**
- **For simple NLP-style tasks, like summarization (Section 4.1):**
  - ChatGPT and similar models are trained heavily on such tasks, so even simple prompts (0/few-shot) are best. Adding more info doesn’t help and can distract the model.
- **For complex software engineering/security tasks:**
  - The models lack the latent domain knowledge/rules, so explicit *expertise* or *self-derived heuristics* (i.e. asking ChatGPT to extract rules from examples) are essential. Prompts that supply this info help the model reason effectively.
  - Random in-context demos (few-shot) are less effective when there's hidden domain structure or the logic is subtle.
- **Too much or non-task-specific information:**
  - Causes confusion or “misuse”—LLMs focus on irrelevant info or focus on correlating patches with descriptions rather than correctness (shown in patch correctness assessment, Section 4.5).
- **Model differences:**
  - GPT-4 generally outperforms GPT-3.5, especially in complex tasks.
  - However, GPT-4 can become 'overaggressive' on recall (e.g., marking too many bug reports as security-relevant), lowering precision.

## 8. All specifics of the researchers’ prompt strategies.
**Prompt Construction Details (Section 3.3, Table 2, Appendix Table 13):**
- **0-shot:** Simple instruction: “Summarize the bug report: <bug report>”
- **1-shot / Few-shot:** Task description + 1 or several demonstrations, each demonstration consisting of a similar sample input and output (labelled). All demonstration examples are randomly drawn from the *training set*.
- **General-info:**  
  - *System*: Assigns ChatGPT a role and offers reinforcement/instructional guiding.
  - *User/Assistant*: Includes a “confirmation dialog” (“I will give you a bug report… got it?”), positive feedback, and encourages step-by-step thinking ("Let's think step by step").
- **Expertise:**  
  - Everything in general-info, but adds explicit domain knowledge, typically a short list, direct feature, formal rule, or criteria relating to the exact label (e.g., “Bug reports about memory leaks or null pointers are always security-relevant”).
  - Expertise content is sourced from authoritative documents/literature/official documentation; see Table 14, Appendix.
- **Self-heuristic:**  
  - First, give ChatGPT a batch of labelled training samples and ask it to generalize/extract “rules” or summaries that characterize the desired output for the label/class.
  - Then, inject that summary (generated by ChatGPT) into the actual task prompt—effectively letting the model use its own “knowledge distillation” as guidance.
- For each template, complete prompt texts and instantiation are shown in Table 13 of the Appendix.
- *Prompt 'skills'* (Table 12):  
  - *role*: Assignment of identity/expertise role.
  - *reinforce*: Emphatic repetition/reminding of the task.
  - *taskconfirmation*: Explicit confirmation (“yes, I understand, Frederick…”).
  - *positivefeedback*: “Great, let’s begin then”.
  - *zero-CoT*: “Let’s think step by step”.
  - *right*: “Let’s reach the right conclusion”.

- **Implementation details:**
  - All prompt crafting and selection is done only on training/probe-test data, never touching the test set.
  - Final large-scale evaluations use the prompt identified as “best” on the probe-test dataset for each task.

**Summary Table of Prompt Usage:**
| Task                      | Best Prompt                 |
|---------------------------|-----------------------------|
| Bug Report Summarization  | Few-shot (simple, 0-shot good) |
| Security Bug Identification | Expertise (explicit instruction) |
| Vulnerability Severity    | Self-heuristic (model-generated rules) |
| Vulnerability Repair      | Expertise (clear task description, error context) |
| Patch Correctness         | Self-heuristic and code-only (no overloading)     |
| Stable Patch Classification | Expertise |

## References (in-paper)
- See Table 1, Table 2, Table 3–11 (results), §3.3 (Prompt Design and Implementation), §4 (Results); Appendix Tables 12–14 and Figure 3, 4, 6.

---

If you need **real examples of prompt text**, just say which task you are interested in, and I’ll provide or format the exact prompt used as shown in Appendix Table 13!
