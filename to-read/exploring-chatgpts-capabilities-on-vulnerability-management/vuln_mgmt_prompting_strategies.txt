Prompting Strategies in "Exploring ChatGPT’s Capabilities on Vulnerability Management"

1. 0-shot Prompting (Section 2.2, Table 2)
   - Defined: In-context learning without examples; directly describe the task and input.
   - Template:
     USER <task description> <input>

2. 1-shot Prompting (Section 2.2, Table 2)
   - Defined: Provide one demonstration example before the query. 
   - Template:
     USER <task description> <demonstration example> <input>

3. Few-shot Prompting (Section 2.2, Table 2)
   - Defined: Provide multiple (four) demonstration examples before the query. 
   - Template:
     USER <task description>
     <demonstration example 1>
     <demonstration example 2>
     <demonstration example 3>
     <demonstration example 4>
     <input>

4. General-info Prompting (Section 2.2, Table 2)
   - Defined: Integrate a role assignment with zero-shot Chain-of-Thought (CoT) instructions via system messages and reinforced dialogue. 
   - Skills (Table 12): role, reinforce, task confirmation, positive feedback, zero-CoT, right. 
   - Template:
     SYSTEM <role> <task description> <reinforce>
     USER <task description> <task confirmation>
     ASSISTANT <task confirmation>
     USER <positive feedback> <input> <zero-CoT> <right>

5. Expertise Prompting (Section 2.2, Table 2)
   - Defined: Extend general-info by adding manually summarized domain expertise into the system and user messages. 
   - Expertise examples (Table 14): task-specific rules (e.g., memory leak seen as security bug) 
   - Template:
     SYSTEM <role> <task description> <expertise> <reinforce>
     USER <expertise> <task description> <task confirmation>
     ASSISTANT <task confirmation>
     USER <positive feedback> <input> <zero-CoT> <right>

6. Self-heuristic Prompting (Section 3.3, Figure 6)
   - Defined: Use ChatGPT itself to summarize domain knowledge from demonstration examples, then integrate that into the prompt.
   - Process: Extract expertise via ChatGPT on 100 samples, then include the generated summary as <knowledge>.
   - Template:
     SYSTEM <role> <task description> <reinforce>
     USER <knowledge> <task description> <task confirmation>
     ASSISTANT <task confirmation>
     USER <positive feedback> <input> <zero-CoT> <right>


Results of Prompting Strategies:

1. Bug Report Summarization:
- Baseline (iTAPE): ROUGE-1 F1 = 31.36
- ChatGPT gpt-3.5 0-shot: ROUGE-1 F1 = 34.33, ROUGE-2 F1 = 11.05, ROUGE-L F1 = 27.95
- gpt-3.5 few-shot: ROUGE-1 F1 = 37.30, ROUGE-2 F1 = 13.99, ROUGE-L F1 = 31.52
- gpt-4 few-shot: ROUGE-1 F1 = 40.38, ROUGE-2 F1 = 15.86, ROUGE-L F1 = 34.30
- gpt-4 few-shot on test: ROUGE-1 F1 = 39.17, demonstrating a 7.81-point improvement over baseline.

2. Security Bug Report Identification:
- Baselines (DKG): Recall = 0.70, Precision = 0.74, F1 = 0.71
- gpt-3.5 0-shot: Recall = 0.35, Precision = 0.21, F1 = 0.27
- gpt-3.5 few-shot: Recall = 0.88, Precision = 0.21, F1 = 0.34
- gpt-3.5 expertise: Recall = 0.71, Precision = 0.57, F1 = 0.63
- gpt-4 expertise (test): Recall = 0.68, Precision = 0.53, F1 = 0.57

3. Vulnerability Severity Evaluation:
- Baseline (DiffCVSS): average Recall ≈ 0.92, Precision ≈ 0.93 across CVSS metrics
- gpt-3.5 self-heuristic: average Recall ≈ 0.88, Precision ≈ 0.55
- gpt-4 self-heuristic: average Recall ≈ 0.92, Precision ≈ 0.74
- Performance within 3.5 points recall and 11.2 points precision of baseline

4. Vulnerability Repair:
- Baselines: ExtractFix fixed 10/12 CVEs; LLMset expertise fixed 8/12 (17.6% valid rate)
- gpt-3.5 expertise (probe): valid repair rate = 72.6%, fixed 7/12
- gpt-4 expertise (test): valid repair rate = 92.8%, fixed 10/12
- gpt-4 outperforms LLMset and matches ExtractFix

5. Patch Correctness Assessment:
- Baseline A (Quatrain): F1 = 0.504, Accuracy = 0.775
- Baseline B (Invalidator): F1 = 0.675, Accuracy = 0.813
- Baseline C (Panther): F1 = 0.773, Accuracy = 0.745
- gpt-4 self-heuristic (code-only): F1 = 0.632, Accuracy = 0.816 on Quatrain
- gpt-4 self-heuristic (desc-code): F1 = 0.517, Accuracy = 0.700
- gpt-4 self-heuristic (full): F1 = 0.727, Accuracy = 0.849 (exceeds Invalidator)
- gpt-4 self-heuristic: F1 = 0.825, Accuracy = 0.813 (comparable to Panther)

6. Stable Patch Classification:
- Baseline (PatchNet): Accuracy = 0.862, Precision = 0.839, Recall = 0.907, F1 = 0.871
- gpt-3.5 expertise: Accuracy = 0.762, Precision = 0.761, Recall = 0.837, F1 = 0.798
- gpt-3.5 self-heuristic: Accuracy = 0.646, Precision = 0.631, Recall = 0.884, F1 = 0.737
- gpt-4 expertise (test): Accuracy = 0.733, Precision = 0.679, Recall = 0.950, F1 = 0.792
- gpt-4 achieves 4.7% higher recall than baseline but 15% lower accuracy and precision on average
