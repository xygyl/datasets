Prompting Strategies in “To Err is Machine: Vulnerability Detection Challenges LLM Reasoning”

Location: Section 2 (Main Prompts) and Appendix A (Prompt Designs) 

1. Basic Zero-Shot Prompting
   - System Prompt: “I want you to act as a vulnerability detection system.”
   - User Query: “Is the following function buggy? Please answer Yes or No.”
   - Variations:
     • “Does the following function contain one of the following bug types? [list of CWEs]”
     • Q/A formatting with “Question:” and “Answer:”.
   
2. In-Context (n-shot) Prompting
   - Provide example input–output pairs before the query to condition the model.
   - Example Selection:
     a. Random examples
     b. Retrieval-augmented generation (RAG) using CodeBERT embeddings
     c. Contrastive pairs: vulnerable and fixed versions in the same prompt
   - Formatting: Append all examples in one assistant message (performed best).
   - Optimal number of examples: 6.
   
3. Contrastive In-Context Prompting
   - Use pairs of vulnerable (pre-fix) and fixed (post-fix) code versions as in-context examples to highlight fine-grained bug differences.
   
4. Chain-of-Thought from CVE Descriptions (CoT-CVE)
   - Use natural-language descriptions from CVE bug reports (Big‑Vul dataset) as reasoning examples.
   - For vulnerable examples: adapt CVE description and append “Therefore, the example is buggy.”
   - Non-vulnerable examples: use default simple responses.
  
5. Chain-of-Thought from Static Analysis (CoT-StaticAnalysis)
   - Use bug-triggering paths from the Infer static analyzer (D2A dataset) as step-by-step reasoning.
   - Convert buggy paths into natural-language steps (e.g., buffer allocation, index usage).
   - Append “Therefore, the example is buggy.” for vulnerable examples.

Results of Prompting Strategies:

Overall Performance (Balanced Accuracy):
- Baseline Zero-Shot and In-Context prompting across 14 models achieved only 50–55% Balanced Accuracy, barely above random guessing.
- Optimal In-Context (6-shot) yielded up to 54.5% Balanced Accuracy (StarCoder2), a gain of <5% over baseline.
- Contrastive prompting improved Balanced Accuracy for 8 out of 14 models by 1–4%, but no model exceeded 60% Balanced Accuracy.
- Chain-of-Thought from CVE Descriptions and Static Analysis (CoT-CVE, CoT-StaticAnalysis) provided marginal gains (<3%), still within 50–55% range.

Error Analysis with and without Domain Knowledge:
- Manual review of 300 responses showed 44% of answers were error-free; 56% contained localization, semantic understanding, or logical inference errors.
- CoT-Annotations (annotated null checks and paths) reduced bounds/NULL check errors by 15–70% for some models, but overall vulnerability detection Balanced Accuracy remained unchanged (~50–55%).

Scaling and Training Variations:
- Increasing model size, training data volume, instruction/adapter fine-tuning, and adding domain knowledge did not yield significant improvements; all prompting strategies remained within a few percentage points of random.
