Prompting Strategies in LLift (Enhancing Static Analysis for Practical Bug Detection)

1. Three-Step Workflow Decomposition (Section 4.1):
   - Decompose the analysis into three sequential prompts:
     • Φ1: Identify initializers for suspicious variables.
     • Φ2: Extract post-constraints for those initializers.
     • Φ3: Analyze initializer behavior under the post-constraints.
   - Case-specific prompt templates use placeholders for function names, variables, and constraints (e.g., "looking at this function [libcfs_ip_str2addr], what are possible initializers for variables <a, b, c, d>?"). 

2. D#1: Post-Constraint Guided Path Analysis (Section 4.3):
   - Teach LLMs to extract and apply post-constraints via few-shot in-context examples of common patterns (Type A: conditional checks before use; Type B: failure checks). 
   - Prompt includes illustrative examples and rules, enabling the LLM to prune infeasible paths and summarize initialization status under qualified constraints. 

3. D#2: Progressive Prompt (Section 4.4):
   - Encourage the LLM to request missing function definitions when uncertain by returning a specific JSON request format:
     [{"type":"function_def", "name":"<function_name>"}]
   - LLift automatically supplies the requested code snippets on demand, avoiding context overflow and improving accuracy. 

4. D#3: Task Decomposition (Section 4.5.1):
   - Break down each stage (Φ1–Φ3) into multi-turn conversations, allowing manageable sub-tasks and clearer context per prompt.
   - Ensures each prompt remains focused and within token limits. 

5. D#4: Self-Validation (Section 4.5.2):
   - After initial response, prompt the LLM to review its output against a set of predefined rules (if-then statements) and correct mistakes.
   - Emphasize “may_init” as a safe default when uncertain to avoid false negatives. 

6. Additional Strategies (Section 4.5.3):
   a. Chain-of-Thought Prompting:
      - Append “think step by step” to every prompt to elicit intermediate reasoning and improve decision quality. 
   b. Source Code Analysis:
      - Provide only relevant function source code on demand instead of full IR, leveraging semantic cues while respecting token limits. 

Results of Prompting Strategies:

RQ1 – Performance on UBITect Undecided Cases:
- Simple Prompt: Precision 50% (5/10), Recall 100% on 300 sampled cases.
- Extended (1,000 cases): 26 positives, 13 true positives; four confirmed bugs in Linux kernel.

RQ2 – Contributions of Design Components (Cmp-40 dataset):
- Simple Prompt: Precision 12%, Recall 15%, F1 0.13
- + Post-Constraint Guided Path Analysis (PGA): Precision 26%, Recall 38%, F1 0.31
- + Progressive Prompt (PP): Precision 21%, Recall 46%, F1 0.29
- + Self-Validation (SV): Precision 33%, Recall 85%, F1 0.48
- + Task Decomposition (TD): Precision 55%, Recall 46%, F1 0.50
- Final (PGA + PP + TD + SV): Precision 87%, Recall 100%, F1 0.93

RQ3 – Model Versatility:
- GPT-4: Detected all 9 tested real bugs (100% recall).
- GPT-3.5: 89% recall.
- Claude 2: 67% recall.
- Bard: 67% recall.
- Progressive Prompt and Task Decomposition less effective on GPT-3.5 and Bard; best results achieved using only the PGA strategy.

RQ4 – Generality on Other Projects:
- Nginx (11 false alarms by UBITect): LLift correctly filtered all 11 (100% accuracy).
- EDK II CryptoPkg: 13 potential bugs from UBITect; LLift correctly classified 12 and had one false positive due to missing struct definitions.

Overall, each prompt design component in LLift contributed to substantial improvements in precision, recall, and F1, demonstrating robust bug detection performance across models and codebases.
