Prompting Strategies in "Large Language Models Are Zero-Shot Reasoners"

Location: Section 3 "Zero-shot Chain of Thought" and Table 4

1. Zero-shot-CoT
   - A single, task-agnostic prompt template that elicits multi-step reasoning without examples.
   - Reasoning Prompt:
       Q: <input question>
       A: Let's think step by step.
   - Answer Prompt:
       Q: <input question>
       A: Let's think step by step.
       <model-generated reasoning>
       A: Therefore, the answer is
   - Answer triggers vary by task:
       * "Therefore, the answer (arabic numerals) is" for numeric outputs.
       * "Therefore, among A through E, the answer is" for multiple-choice.

2. Trigger Phrase Variants (Table 4)
   - Instructive phrases tested include:
       * "Let's think step by step."
       * "First, ..."
       * "Let's think about this logically."
       * "Let's solve this problem by splitting it into steps."
       * "Let's be realistic and think step by step."
       * and others encouraging stepwise reasoning.
   - Templates that did not encourage reasoning (misleading or irrelevant) failed to improve accuracy.

3. Baseline Prompting Methods
   - Zero-shot: ask "Q: <question>\nA: The answer is" without reasoning.
   - Few-shot: standard in-context learning with 2–8 examples.
   - Few-shot-CoT: few-shot with step-by-step reasoning examples in the demonstrations.

4. Implementation Details
   - Models evaluated: InstructGPT (text-ada/curie/davinci), original GPT-3 (ada, babbage, curie, davinci), PaLM (8B, 62B, 540B), GPT-Neo, GPT-J, T0, OPT.
   - Decoding: greedy decoding for deterministic outputs.
   - Answer extraction: parsing the first matching format in the model’s output.

These strategies demonstrate that adding a simple reasoning trigger can transform an LLM into a strong zero-shot reasoner across arithmetic, commonsense, symbolic, and other logical tasks.


Results of Prompting Strategies:

Zero-shot vs. Zero-shot-CoT:
- SingleEq: 74.6% → 78.0%
- AddSub: 72.2% → 69.6%
- MultiArith: 17.7% → 78.7%
- GSM8K: 10.4% → 40.7%
- AQUA-RAT: 22.4% → 33.5%
- SVAMP: 58.8% → 62.1%

Other Reasoning Tasks:
- CommonsenseQA: 68.8% → 64.6% (no improvement)
- StrategyQA: 12.7% → 54.8%
- Date Understanding: 49.3% → 67.5%
- Tracking Shuffled Objects: 31.3% → 52.4%
- Last Letter Concatenation: 0.2% → 57.6%
- Coin Flip: 12.8% → 91.4%

Few-shot-CoT Comparison (MultiArith / GSM8K):
- Zero-shot: 17.7% / 10.4%
- Few-shot (2 examples): 33.7% / 15.6%
- Few-shot-CoT (2 examples): 84.8% / 41.3%
- Few-shot-CoT (8 examples): 93.0% / 48.7%
- Zero-shot-CoT: 78.7% / 40.7%

Model Scaling (MultiArith):
- GPT-3 series: performance flat without CoT; jumps to 78.7%–79.3% with Zero-shot-CoT as model size increases.
- PaLM 540B: Zero-shot 25.5% → Zero-shot-CoT 66.1% → +Self-Consistency 89.0%.

Trigger Phrase Robustness:
- “Let’s think step by step.”: 78.7%
- “First, …”: 77.3%
- “Let’s think about this logically.”: 74.5%
- Other instructive variants: 70.3%–72.2%
- Misleading/irrelevant templates stalled at 13.1%–18.8%.

Overall Impact:
- Zero-shot-CoT boosts accuracy by ~3–61 pp over standard zero-shot on multi-step tasks.
- Outperforms zero-shot and basic few-shot, approaching few-shot-CoT performance without examples.
