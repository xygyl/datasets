Prompting Strategies in “LLMs Cannot Reliably Identify and Reason About Security Vulnerabilities”

Location: Section 3.3 – Prompt Templates 

SecLLMHolmes explores four prompting techniques:
1. Zero-Shot Task-Oriented (ZS-TO)
2. Zero-Shot Role-Oriented (ZS-RO)
3. Few-Shot Task-Oriented (FS-TO)
4. Few-Shot Role-Oriented (FS-RO)

Prompts are divided into three categories (Table 3):
– Standard (S): Direct answer to the problem.
– Step-by-Step (R): Chain-of-thought reasoning.
– Definition-based (D): Includes CWE definition.

Prompt templates (Table 3) :

S1 ZS-TO:
  Code snippet with question about a specific CWE (e.g., out-of-bound write).

S2 ZS-RO:
  Same as S1, but assigns the LLM the role of a “helpful assistant.”

S3 ZS-RO:
  Similar to S1, with the LLM acting as a “security expert.”

S4 ZS-RO:
  Defines the LLM as a “security expert” analyzing a specific vulnerability, without the question.

S5 FS-TO:
  Includes a vulnerable example, its patch, and standard reasoning from the same CWE.

S6 FS-RO:
  Like S4, but adds a vulnerable example, its patch, and standard reasoning.

R1 ZS-TO:
  Starts with “Let’s think step by step” to encourage chain-of-thought reasoning.

R2 ZS-RO:
  LLM plays a security expert role, following a multi-step chain-of-thought approach.

R3 ZS-TO:
  Multi-round conversation: progressive sub-component analysis mirroring human experts.

R4 FS-RO:
  Builds on S6 with step-by-step reasoning developed by the authors.

R5 FS-RO:
  Like R2, but includes few-shot examples (same CWE) with step-by-step reasoning.

R6 FS-TO:
  Similar to R5, but without assigning a role in the system prompt.

D1 ZS-TO:
  Adds the CWE definition to the prompt before asking the question.

D2 ZS-RO:
  LLM is a security expert; CWE definition included in system prompt.

D3 FS-RO:
  Builds on S6 with the CWE definition in the system prompt.

D4 FS-RO:
  Like R4, plus the CWE definition in the system prompt.

D5 FS-TO:
  Similar to D4, but without assigning a role in the system prompt.


Results of Prompting Strategies:

Experiments over 48 hand-crafted vulnerability scenarios using all 17 prompt templates showed that performance depended heavily on both model and prompt type:

- GPT-4 achieved the highest overall accuracy of 89.5%, with its best results coming from few-shot role-oriented and step-by-step chain-of-thought prompts.  
- GPT-3.5 and codechat-bison also benefited significantly from chain-of-thought prompting, improving both detection accuracy and reasoning consistency.  
- chat-bison (PaLM2) performed best when given an explicit security-expert role in zero-shot prompts, highlighting the impact of role assignment.  
- codellama34b excelled with simple standard prompts (S1), achieving its top accuracy with minimal guidance.  
- Adding vulnerability definitions in prompts boosted accuracy for GPT-4 and codellama34b, though gains were inconsistent across other models.  
- Correct reasoning rates generally tracked accuracy, but all models occasionally produced correct answers with flawed reasoning or incorrect answers with plausible reasoning, underscoring challenges in trustworthy explanation.  

Overall, no single prompting strategy dominated across all LLMs; tailoring prompt structure and content to each model is crucial for optimal vulnerability detection.
